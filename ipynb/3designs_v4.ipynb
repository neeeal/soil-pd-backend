{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Flatten, LeakyReLU, ReLU, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, concatenate, Activation\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.backend import clear_session\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop , SGD\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "# from tensorflow.keras.regularizers import L2\n",
    "# from tensorflow.keras import metrics\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from shutil import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 198 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('.\\\\final_data.csv',  names=[\"path\",\"value\"])\n",
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    )\n",
    "\n",
    "data_generator = generator.flow_from_dataframe(\n",
    "    df, \n",
    "    x_col=\"path\", \n",
    "    y_col=\"value\", \n",
    "    class_mode='raw', \n",
    "    batch_size=198,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = []\n",
    "Ys = []\n",
    "iterations = 4\n",
    "for i in range(iterations):\n",
    "    x,y = next(data_generator)\n",
    "    Xs.extend([np.array(value).astype(int) for value in x])\n",
    "    Ys.extend([np.array(value.replace(\"'\",\"\")[1:-1].split(', ')).astype(float) for value in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXPH = np.max(np.array(Ys)[:,1])\n",
    "MINPH = np.min(np.array(Ys)[:,1])\n",
    "\n",
    "MAXMOISTURE = np.max(np.array(Ys)[:,0])\n",
    "MINMOISTURE = np.min(np.array(Ys)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(image):\n",
    "    image = image.astype(np.uint8)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    binr = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    binr = np.invert(binr)\n",
    "\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    mask = cv2.erode(binr, kernel, iterations=3)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def process_image(image):\n",
    "    rgb_image = image.astype(np.uint8)\n",
    "    \n",
    "    mask = get_mask(image)\n",
    "\n",
    "    rgb_planes = cv2.split(rgb_image)\n",
    "    result_planes = []\n",
    "    for plane in rgb_planes:\n",
    "        processed_image = cv2.medianBlur(plane, 3)\n",
    "        processed_image = cv2.bitwise_and(processed_image, processed_image, mask=mask)\n",
    "        result_planes.append(processed_image)\n",
    "    result = cv2.merge(result_planes)\n",
    "\n",
    "    return result\n",
    "\n",
    "def process_label(image, label, maxPh, minPh, maxMoisture, minMoisture):\n",
    "    mask = get_mask(np.array(image))\n",
    "    shape = mask.shape \n",
    "    array = np.ones(shape=(shape[0], shape[1], 1))\n",
    "\n",
    "    moisture = cv2.bitwise_and(array, array, mask=mask)\n",
    "    ph = cv2.bitwise_and(array, array, mask=mask)\n",
    "\n",
    "    moisture_value = (label[0] - minMoisture) / (maxMoisture - minMoisture)\n",
    "    ph_value = (label[1] - minPh) / (maxPh - minPh)\n",
    "    moisture[moisture > 0] = moisture_value\n",
    "    ph[ph > 0] = ph_value\n",
    "\n",
    "    output = np.stack([moisture, ph], axis=-1)  \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_labels(mask, label):\n",
    "    channel_0 = cv2.bitwise_and(label[:,:,0], label[:,:,0], mask=mask)\n",
    "    channel_1 = cv2.bitwise_and(label[:,:,1], label[:,:,1], mask=mask)\n",
    "    return cv2.merge([channel_0, channel_1])\n",
    "\n",
    "def unprocess_label(label, maxPh, minPh, maxMoisture, minMoisture):\n",
    "    moisture = np.array(label[0::2], dtype=float) * (float(maxMoisture) - float(minMoisture)) + float(minMoisture)\n",
    "    ph = np.array(label[1::2], dtype=float) * (float(maxPh) - float(minPh)) + float(minPh)\n",
    "    output = [np.mean(moisture), np.mean(ph)]\n",
    "    return output\n",
    "\n",
    "def unprocess_label_wmask(image,label):\n",
    "    unnormalized = np.array(image*255)\n",
    "    MASK = np.array(get_mask(unnormalized))\n",
    "    mask_label = mask_labels(MASK, np.array(label))\n",
    "    results = unprocess_label(label, MAXPH, MINPH, MAXMOISTURE, MINMOISTURE)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(truths, predictions):\n",
    "    true_moisture, true_ph = unprocess_label(truths,MAXPH, MINPH, MAXMOISTURE, MINMOISTURE)\n",
    "    predicted_moisture, predicted_ph = unprocess_label(predictions,MAXPH, MINPH, MAXMOISTURE, MINMOISTURE)\n",
    "    accuracy_moisture = (1 - abs(true_moisture - predicted_moisture) / true_moisture) * 100\n",
    "    accuracy_ph = (1 - abs(true_ph - predicted_ph) / true_ph) * 100\n",
    "    return [accuracy_moisture,accuracy_ph]\n",
    "\n",
    "def get_model_accuracy(model, x_values, y_values, copy=[]):\n",
    "    predictions = model.predict(x_values)\n",
    "    samples = []\n",
    "    for i in copy:\n",
    "        samples.extend(i)\n",
    "    total_acc = []\n",
    "    for i,pred in enumerate(predictions):\n",
    "        unnormalized = np.array(samples[i]*255)\n",
    "        MASK = np.array(get_mask(unnormalized))\n",
    "        mask_truth = mask_labels(MASK, np.array(y_values[i]))\n",
    "        mask_predictions = mask_labels(MASK, np.array(predictions[i]))\n",
    "        total_acc.append(get_accuracy(mask_truth, mask_predictions))\n",
    "    return np.mean(total_acc[:][0]),np.mean(total_acc[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def process_label(image, label, maxPh, minPh, maxMoisture, minMoisture):\n",
    "#     # mask = get_mask(np.array(image))\n",
    "#     # shape = mask.shape \n",
    "#     # array = np.ones(shape=(shape[0], shape[1], 1))\n",
    "\n",
    "#     # moisture = cv2.bitwise_and(array, array, mask=mask)\n",
    "#     # ph = cv2.bitwise_and(array, array, mask=mask)\n",
    "\n",
    "#     moisture_value = (label[0] - minMoisture) / (maxMoisture - minMoisture)\n",
    "#     ph_value = (label[1] - minPh) / (maxPh - minPh)\n",
    "#     # moisture[moisture > 0] = moisture_value\n",
    "#     # ph[ph > 0] = ph_value\n",
    "\n",
    "#     output = np.stack([moisture_value, ph_value], axis=-1)  \n",
    "\n",
    "#     return output\n",
    "\n",
    "# def process_label(image, label, maxPh, minPh, maxMoisture, minMoisture):\n",
    "#     mask = get_mask(np.array(image))\n",
    "#     shape = mask.shape \n",
    "#     array = np.ones(shape=(shape[0], shape[1], 1))\n",
    "\n",
    "#     moisture = cv2.bitwise_and(array, array, mask=mask)\n",
    "#     ph = cv2.bitwise_and(array, array, mask=mask)\n",
    "\n",
    "#     # moisture_value = (label[0] - minMoisture) / (maxMoisture - minMoisture)\n",
    "#     # ph_value = (label[1] - minPh) / (maxPh - minPh)\n",
    "#     moisture_value = label[0] * 100\n",
    "#     ph_value = label[1]\n",
    "#     moisture[moisture > 0] = moisture_value\n",
    "#     ph[ph > 0] = ph_value\n",
    "\n",
    "#     output = np.stack([moisture, ph], axis=-1)  \n",
    "\n",
    "#     return output\n",
    "\n",
    "\n",
    "# def process_label(image, label, maxPh, minPh, maxMoisture, minMoisture):\n",
    "#     mask = get_mask(np.array(image))\n",
    "#     shape = mask.shape \n",
    "#     array = np.ones(shape=(shape[0], shape[1], 1))\n",
    "\n",
    "#     moisture = cv2.bitwise_and(array, array, mask=mask)\n",
    "#     ph = cv2.bitwise_and(array, array, mask=mask)\n",
    "\n",
    "#     # moisture_value = (label[0] - minMoisture) / (maxMoisture - minMoisture)\n",
    "#     # ph_value = (label[1] - minPh) / (maxPh - minPh)\n",
    "#     moisture_value = label[0] * 100\n",
    "#     ph_value = label[1]\n",
    "#     moisture[moisture > 0] = moisture_value\n",
    "#     ph[ph > 0] = ph_value\n",
    "\n",
    "#     output = np.stack([moisture, ph], axis=-1)  \n",
    "\n",
    "#     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n"
     ]
    }
   ],
   "source": [
    "Y_values = []\n",
    "X_values = []\n",
    "for i, x in enumerate(Xs):\n",
    "    Y_values.append(process_label(Xs[i], Ys[i], MAXPH, MINPH, MAXMOISTURE, MINMOISTURE))\n",
    "    X_values.append(process_image(Xs[i]))\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_values,Y_values , \n",
    "                                   test_size=0.20,  \n",
    "                                   shuffle=True) \n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "        width_shift_range = 0.5,\n",
    "        height_shift_range = 0.5, \n",
    "        zoom_range = 0.5,\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True,\n",
    "        brightness_range = (0.7,1.0),\n",
    "        rotation_range = 45,\n",
    "    ).flow(x=np.array(X_train), y=y_train, batch_size=16) \n",
    "\n",
    "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    "    ).flow(x=np.array(X_test), y=y_test, batch_size=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "633"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.66666667 0.10185185]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for i in next(train_generator)[1][0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = \"path\"\n",
    "y_col = \"pH\"\n",
    "batch_size = 16\n",
    "epochs = 1024\n",
    "lr = 1e-5\n",
    "image_size = (IMAGE_SIZE[0],IMAGE_SIZE[1])\n",
    "channels = 3\n",
    "shuffle = True\n",
    "class_mode =\"raw\"\n",
    "color_mode = \"rgb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Designs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of Designs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Dropout, Concatenate, Flatten, Dense, Reshape\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "# def convolution_block(inputs, num_filters, kernel_size=3, padding=\"same\", use_bias=False):\n",
    "#     x = Conv2D(num_filters, kernel_size=kernel_size, padding=padding, use_bias=use_bias, kernel_initializer=HeNormal())(inputs)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "#     return x\n",
    "\n",
    "# def upsample_block(inputs, skip_features, num_filters):\n",
    "#     x = UpSampling2D((2, 2))(inputs)\n",
    "#     x = Conv2D(num_filters, (2, 2), padding=\"same\")(x)\n",
    "#     x = Concatenate()([x, skip_features])\n",
    "#     x = convolution_block(x, num_filters)\n",
    "#     return x\n",
    "\n",
    "# def dense_upsampling_block(inputs, skip_features, num_filters):\n",
    "#     x = UpSampling2D((2, 2))(inputs)\n",
    "#     x = Conv2D(num_filters, (2, 2), padding=\"same\")(x)\n",
    "#     x = Concatenate()([x, skip_features])\n",
    "#     x = convolution_block(x, num_filters)\n",
    "#     return x\n",
    "\n",
    "# def unet_plus_plus_model(hp,input_shape=(64,64,3)):\n",
    "#     inputs = Input(input_shape)\n",
    "\n",
    "#     resnet50 = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "#     resnet50.trainable = True\n",
    "\n",
    "#     # Encoder\n",
    "#     s1 = resnet50.get_layer(\"conv1_relu\").output\n",
    "#     s2 = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "#     s3 = resnet50.get_layer(\"conv3_block4_out\").output\n",
    "#     s4 = resnet50.get_layer(\"conv4_block6_out\").output\n",
    "\n",
    "#     b1 = resnet50.get_layer(\"conv5_block3_out\").output\n",
    "#     # Flatten the bottleneck output\n",
    "#     x = Flatten()(b1);UNITS = hp.Choice('units',values = [256,512,1024,2048]);DROPOUT = hp.Float('dropout',min_value=0.0, max_value=0.5, step=0.1)\n",
    "    \n",
    "#     BATCHNORM = hp.Boolean('batchnorm',default=False)\n",
    "#     # Dense layers between encoder and decoder\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     # Reshape back to spatial dimensions for the decoder\n",
    "#     SIZE = input_shape[0] // 32  # Assuming input size is a multiple of 32\n",
    "#     if BATCHNORM:\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Dense(SIZE * SIZE * 2048, activation='relu')(x)\n",
    "#     x = Reshape((SIZE, SIZE, 2048))(x)\n",
    "\n",
    "#     # Nested U-Net\n",
    "#     d4_2 = dense_upsampling_block(x, s4, 512)\n",
    "#     d3_2 = dense_upsampling_block(d4_2, s3, 256)\n",
    "#     d2_2 = dense_upsampling_block(d3_2, s2, 128)\n",
    "#     d1_2 = dense_upsampling_block(d2_2, s1, 64)\n",
    "\n",
    "#     d4_1 = upsample_block(x, s4, 512)\n",
    "#     d3_1 = upsample_block(d4_1, s3, 256)\n",
    "#     d2_1 = upsample_block(d3_1, s2, 128)\n",
    "#     d1_1 = upsample_block(d2_1, s1, 64)\n",
    "\n",
    "#     outputs = UpSampling2D()(d1_1)\n",
    "#     outputs = Conv2D(2, (1, 1), padding=\"same\", activation=\"sigmoid\")(outputs)\n",
    "#     model = tf.keras.Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "    \n",
    "#     # Hyperparameter choice for optimizer\n",
    "#     optimizer_choice = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    \n",
    "#     # Select optimizer\n",
    "#     if optimizer_choice == 'adam':\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "#     elif optimizer_choice == 'sgd':\n",
    "#         optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "#     elif optimizer_choice == 'rmsprop':\n",
    "#         optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    \n",
    "#     model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "#     return model\n",
    "\n",
    "# # # Define input shape and build model\n",
    "# # input_shape = (256, 256, 3)\n",
    "# # model = build_resnet50_unetpp(input_shape)\n",
    "# # model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# # # Print model summary\n",
    "# # model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining the Convolutional Block\n",
    "# def conv_block(inputs, num_filters):\n",
    "# \t# Applying the sequence of Convolutional, Batch Normalization\n",
    "# \t# and Activation Layers to the input tensor\n",
    "# \tx = Sequential([\n",
    "# \t\t# Convolutional Layer\n",
    "# \t\tConv2D(num_filters, 1, padding='same'),\n",
    "# \t\t# Batch Normalization Layer\n",
    "# \t\tBatchNormalization(),\n",
    "# \t\t# Activation Layer\n",
    "# \t\tReLU(),\n",
    "# \t\t# Convolutional Layer\n",
    "# \t\tConv2D(num_filters, 1, padding='same'),\n",
    "# \t\t# Batch Normalization Layer\n",
    "# \t\tBatchNormalization(),\n",
    "# \t\t# Activation Layer\n",
    "# \t\tReLU()\n",
    "# \t])(inputs)\n",
    "\n",
    "# \t# Returning the output of the Convolutional Block\n",
    "# \treturn x\n",
    "# def dense_block(units, dropout_rate):\n",
    "#     return Sequential([\n",
    "#         Dense(units, activation='relu'),\n",
    "#     ])\n",
    "# # Defining the Unet++ Model\n",
    "# def unet_plus_plus_model(hp):\n",
    "# \tinputs = Input(shape=image_size+(3,))\n",
    "# \thp_filters = hp.Choice('filters',values = [16,32,64])\n",
    "# \t# Encoding Path\n",
    "# \tx_00 = conv_block(inputs, hp_filters)\n",
    "# \tx_10 = conv_block(MaxPooling2D()(x_00), hp_filters*2)\n",
    "# \tx_20 = conv_block(MaxPooling2D()(x_10), hp_filters*4)\n",
    "# \tx_30 = conv_block(MaxPooling2D()(x_20), hp_filters*8)\n",
    "# \tx_40 = conv_block(MaxPooling2D()(x_30), hp_filters*16)\n",
    "\t\n",
    "# \thp.Boolean(\"dropouts\", default=False)\n",
    "# \thp.Boolean(\"batch_normalization\", default=False)\n",
    "# \tflattened = Flatten()(x_40)\n",
    "# \tdense = dense_block(hp_filters*hp_filters, 0.2)(flattened)\n",
    "# \tif hp.Boolean(\"dropouts\"):\n",
    "# \t\tdense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "# \tif hp.Boolean(\"batch_normalization\"):\n",
    "# \t\tdense = tf.keras.layers.BatchNormalization()(dense)\n",
    "# \tdense = dense_block(hp_filters*hp_filters, 0.2)(dense)\n",
    "# \tif hp.Boolean(\"dropouts\"):\n",
    "# \t\tdense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "# \tif hp.Boolean(\"batch_normalization\"):\n",
    "# \t\tdense = tf.keras.layers.BatchNormalization()(dense)\n",
    "# \tdense = dense_block(hp_filters*hp_filters, 0.2)(dense);hp.Boolean(\"4th dense\", default=False);\n",
    "# \tif hp.Boolean(\"4th dense\"):\n",
    "# \t\tdense = dense_block(hp_filters*hp_filters, 0.2)(dense)\n",
    "# \t\tif hp.Boolean(\"dropouts\"):\n",
    "# \t\t\tdense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "# \t\tif hp.Boolean(\"batch_normalization\"):\n",
    "# \t\t\tdense = tf.keras.layers.BatchNormalization()(dense)\n",
    "# \t# dense = dense_block(4096, 0.2)(dense)\n",
    "# \t# dense = dense_block(4096, 0.2)(dense)\n",
    "# \treshaped = tf.keras.layers.Reshape((x_40.shape[1], x_40.shape[1], (hp_filters*hp_filters)//(x_40.shape[1]*x_40.shape[1])))(dense)  # Reshape to reintroduce spatial dimensions\n",
    "\n",
    "\n",
    "# \t# Nested Decoding Path\n",
    "# \tx_01 = conv_block(concatenate(\n",
    "# \t\t[x_00, UpSampling2D()(x_10)]), hp_filters)\n",
    "# \tx_11 = conv_block(concatenate(\n",
    "# \t\t[x_10, UpSampling2D()(x_20)]), hp_filters*2)\n",
    "# \tx_21 = conv_block(concatenate(\n",
    "# \t\t[x_20, UpSampling2D()(x_30)]), hp_filters*4)\n",
    "# \tx_31 = conv_block(concatenate(\n",
    "# \t\t[x_30, UpSampling2D()(reshaped)]), hp_filters*8)\n",
    "\n",
    "# \tx_02 = conv_block(concatenate(\n",
    "# \t\t[x_00, x_01, UpSampling2D()(x_11)]), hp_filters)\n",
    "# \tx_12 = conv_block(concatenate(\n",
    "# \t\t[x_10, x_11, UpSampling2D()(x_21)]), hp_filters*2)\n",
    "# \tx_22 = conv_block(concatenate(\n",
    "# \t\t[x_20, x_21, UpSampling2D()(x_31)]), hp_filters*4)\n",
    "\n",
    "# \tx_03 = conv_block(concatenate(\n",
    "# \t\t[x_00, x_01, x_02, UpSampling2D()(x_12)]), hp_filters)\n",
    "# \tx_13 = conv_block(concatenate(\n",
    "# \t\t[x_10, x_11, x_12, UpSampling2D()(x_22)]), hp_filters*2)\n",
    "\n",
    "# \tx_04 = conv_block(concatenate(\n",
    "# \t\t[x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), hp_filters)\n",
    "\t\n",
    "# \toutputs = tf.keras.layers.Conv2D(2, 1, activation='linear')(x_04);print(outputs.shape)\n",
    "\n",
    "# \t# Creating the model\n",
    "# \tmodel = tf.keras.Model(\n",
    "# \t\tinputs=inputs, outputs=outputs, name='Unet_plus_plus');lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5]);model.compile(optimizer= tf.keras.optimizers.Adam(lr=lr), loss= [\"mse\"], metrics=['mae'])\n",
    "# \t# Returning the model\n",
    "# \treturn model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Dropout, Concatenate, Flatten, Dense, Reshape\n",
    "# from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "# resnet50 = ResNet50(include_top=False, weights='imagenet')\n",
    "# print(resnet50.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the convolutional block\n",
    "# def conv_block(inputs, num_filters):\n",
    "#     x = Sequential([\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU(),\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU()\n",
    "#     ])(inputs)\n",
    "#     return x\n",
    "\n",
    "# # Define the dense block\n",
    "# def dense_block(units, dropout_rate):\n",
    "#     return Sequential([\n",
    "#         Dense(units, activation='relu'),\n",
    "#         Dropout(dropout_rate)\n",
    "#     ])\n",
    "\n",
    "# # Define the Unet++ model\n",
    "# def unet_plus_plus_model(hp, image_size=(224, 224)):\n",
    "#     inputs = Input(shape=image_size + (3,))\n",
    "\n",
    "#     # Load ResNet50 as encoder\n",
    "#     resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "#     encoder_layers = [resnet.get_layer(name).output for name in ['conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block6_out', 'conv5_block3_out']]\n",
    "    \n",
    "#     # Encoding path\n",
    "#     x_00 = encoder_layers[0]\n",
    "#     x_10 = encoder_layers[1]\n",
    "#     x_20 = encoder_layers[2]\n",
    "#     x_30 = encoder_layers[3]\n",
    "#     x_40 = encoder_layers[4]\n",
    "\n",
    "#     flattened = Flatten()(x_40)\n",
    "#     dense = dense_block(2048, 0.2)(flattened)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(2048, 0.2)(dense)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(2048, 0.2)(dense)\n",
    "#     if hp.Boolean(\"4th_dense\"):\n",
    "#         dense = dense_block(2048, 0.2)(dense)\n",
    "#         if hp.Boolean(\"dropouts\"):\n",
    "#             dense = Dropout(0.5)(dense)\n",
    "#         if hp.Boolean(\"batch_normalization\"):\n",
    "#             dense = BatchNormalization()(dense)\n",
    "\n",
    "#     reshaped = Reshape((x_40.shape[1], x_40.shape[2], 32))(dense)\n",
    "\n",
    "#     # Nested decoding path\n",
    "#     x_01 = conv_block(concatenate([x_00, UpSampling2D()(x_10)]), 64)\n",
    "#     x_11 = conv_block(concatenate([x_10, UpSampling2D()(x_20)]), 128)\n",
    "#     x_21 = conv_block(concatenate([x_20, UpSampling2D()(x_30)]), 256)\n",
    "#     x_31 = conv_block(concatenate([x_30, UpSampling2D()(reshaped)]), 512)\n",
    "\n",
    "#     x_02 = conv_block(concatenate([x_00, x_01, UpSampling2D()(x_11)]), 64)\n",
    "#     x_12 = conv_block(concatenate([x_10, x_11, UpSampling2D()(x_21)]), 128)\n",
    "#     x_22 = conv_block(concatenate([x_20, x_21, UpSampling2D()(x_31)]), 256)\n",
    "\n",
    "#     x_03 = conv_block(concatenate([x_00, x_01, x_02, UpSampling2D()(x_12)]), 64)\n",
    "#     x_13 = conv_block(concatenate([x_10, x_11, x_12, UpSampling2D()(x_22)]), 128)\n",
    "\n",
    "#     x_04 = conv_block(concatenate([x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), 64)\n",
    "\n",
    "#     outputs = Conv2D(2, 1, activation='sigmoid')(x_04)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, ReLU, Flatten, Dense, Dropout, concatenate, Reshape\n",
    "# import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "\n",
    "# # Define the convolutional block\n",
    "# def conv_block(inputs, num_filters):\n",
    "#     x = Sequential([\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU(),\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU()\n",
    "#     ])(inputs)\n",
    "#     return x\n",
    "\n",
    "# # Define the dense block\n",
    "# def dense_block(units, dropout_rate):\n",
    "#     return Sequential([\n",
    "#         Dense(units, activation='relu'),\n",
    "#         Dropout(dropout_rate)\n",
    "#     ])\n",
    "\n",
    "# # Define the Unet++ model\n",
    "# def unet_plus_plus_model(hp, image_size=(32, 32)):\n",
    "#     inputs = Input(shape=image_size + (3,))\n",
    "\n",
    "#     # Load ResNet50 as encoder\n",
    "#     resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "#     encoder_layers = [resnet.get_layer(name).output for name in ['conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block6_out', 'conv5_block3_out']]\n",
    "    \n",
    "#     # Encoding path\n",
    "#     x_00 = encoder_layers[0]\n",
    "#     x_10 = encoder_layers[1]\n",
    "#     x_20 = encoder_layers[2]\n",
    "#     x_30 = encoder_layers[3]\n",
    "#     x_40 = encoder_layers[4]\n",
    "#     print(x_40.shape)\n",
    "#     flattened = Flatten()(x_40)\n",
    "#     dense = dense_block(2048, 0.2)(flattened)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(2048, 0.2)(dense)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(2048, 0.2)(dense)\n",
    "#     if hp.Boolean(\"4th_dense\"):\n",
    "#         dense = dense_block(2048, 0.2)(dense)\n",
    "#         if hp.Boolean(\"dropouts\"):\n",
    "#             dense = Dropout(0.5)(dense)\n",
    "#         if hp.Boolean(\"batch_normalization\"):\n",
    "#             dense = BatchNormalization()(dense)\n",
    "#     reshaped = tf.keras.layers.Reshape((x_40.shape[1], x_40.shape[1], 2048))(dense)  # Reshape to reintroduce spatial dimensions\n",
    "\n",
    "#     # Nested decoding path\n",
    "#     x_01 = conv_block(concatenate([x_00, UpSampling2D()(x_10)]), 64)\n",
    "#     x_11 = conv_block(concatenate([x_10, UpSampling2D()(x_20)]), 128)\n",
    "#     x_21 = conv_block(concatenate([x_20, UpSampling2D()(x_30)]), 256)\n",
    "#     x_31 = conv_block(concatenate([x_30, UpSampling2D()(reshaped)]), 512)\n",
    "\n",
    "#     x_02 = conv_block(concatenate([x_00, x_01, UpSampling2D()(x_11)]), 64)\n",
    "#     x_12 = conv_block(concatenate([x_10, x_11, UpSampling2D()(x_21)]), 128)\n",
    "#     x_22 = conv_block(concatenate([x_20, x_21, UpSampling2D()(x_31)]), 256)\n",
    "\n",
    "#     x_03 = conv_block(concatenate([x_00, x_01, x_02, UpSampling2D()(x_12)]), 64)\n",
    "#     x_13 = conv_block(concatenate([x_10, x_11, x_12, UpSampling2D()(x_22)]), 128)\n",
    "\n",
    "#     x_04 = conv_block(concatenate([x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), 64)\n",
    "    \n",
    "#     x_final = UpSampling2D(size=(2, 2))(x_04)\n",
    "\n",
    "#     outputs = Conv2D(2, 1, activation='sigmoid')(x_final)\n",
    "#     print(outputs.shape)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the convolutional block\n",
    "# def conv_block(inputs, num_filters):\n",
    "#     x = Sequential([\n",
    "#         Conv2D(num_filters, 1, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU(),\n",
    "#         Conv2D(num_filters, 1, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU()\n",
    "#     ])(inputs)\n",
    "#     return x\n",
    "\n",
    "# # Define the dense block\n",
    "# def dense_block(units, dropout_rate):\n",
    "#     return Sequential([\n",
    "#         Dense(units, activation='relu')\n",
    "#     ])\n",
    "\n",
    "# # Define the Unet++ model\n",
    "# def unet_plus_plus_model(hp):\n",
    "#     inputs = Input(shape=(32, 32, 3))\n",
    "#     hp_filters = hp.Choice('filters', values=[16, 32, 64,])\n",
    "\n",
    "#     # Encoding Path\n",
    "#     x_00 = conv_block(inputs, hp_filters)\n",
    "#     x_10 = conv_block(MaxPooling2D()(x_00), hp_filters * 2)\n",
    "#     x_20 = conv_block(MaxPooling2D()(x_10), hp_filters * 4)\n",
    "#     x_30 = conv_block(MaxPooling2D()(x_20), hp_filters * 8)\n",
    "#     x_40 = conv_block(MaxPooling2D()(x_30), hp_filters * 16)\n",
    "\n",
    "#     # Dense layers and reshape\n",
    "#     flattened = Flatten()(x_40)\n",
    "#     dense = dense_block(hp_filters * hp_filters, 0.2)(flattened)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(hp_filters * hp_filters, 0.2)(dense)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(hp_filters * hp_filters, 0.2)(dense)\n",
    "#     if hp.Boolean(\"4th_dense\"):\n",
    "#         dense = dense_block(hp_filters * hp_filters, 0.2)(dense)\n",
    "#         if hp.Boolean(\"dropouts\"):\n",
    "#             dense = Dropout(0.5)(dense)\n",
    "#         if hp.Boolean(\"batch_normalization\"):\n",
    "#             dense = BatchNormalization()(dense)\n",
    "\n",
    "#     reshaped = tf.keras.layers.Reshape((x_40.shape[1], x_40.shape[1], (hp_filters * hp_filters) // (x_40.shape[1] * x_40.shape[1])))(dense)\n",
    "\n",
    "#     # Nested Decoding Path\n",
    "#     x_01 = conv_block(concatenate([x_00, UpSampling2D()(x_10)]), hp_filters)\n",
    "#     x_11 = conv_block(concatenate([x_10, UpSampling2D()(x_20)]), hp_filters * 2)\n",
    "#     x_21 = conv_block(concatenate([x_20, UpSampling2D()(x_30)]), hp_filters * 4)\n",
    "#     x_31 = conv_block(concatenate([x_30, UpSampling2D()(reshaped)]), hp_filters * 8)\n",
    "\n",
    "#     x_02 = conv_block(concatenate([x_00, x_01, UpSampling2D()(x_11)]), hp_filters)\n",
    "#     x_12 = conv_block(concatenate([x_10, x_11, UpSampling2D()(x_21)]), hp_filters * 2)\n",
    "#     x_22 = conv_block(concatenate([x_20, x_21, UpSampling2D()(x_31)]), hp_filters * 4)\n",
    "\n",
    "#     x_03 = conv_block(concatenate([x_00, x_01, x_02, UpSampling2D()(x_12)]), hp_filters)\n",
    "#     x_13 = conv_block(concatenate([x_10, x_11, x_12, UpSampling2D()(x_22)]), hp_filters * 2)\n",
    "\n",
    "#     x_04 = conv_block(concatenate([x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), hp_filters)\n",
    "\n",
    "#     # Flattening the final output\n",
    "#     flat_output = Flatten()(x_04)\n",
    "\n",
    "#     # Adding Dense layers\n",
    "#     dense_output = Dense(hp_filters * hp_filters, activation='relu')(flat_output)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense_output = Dropout(0.5)(dense_output)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense_output = BatchNormalization()(dense_output)\n",
    "\n",
    "#     # Final Dense layer with sigmoid activation for binary classification\n",
    "#     outputs = Dense(2, activation='sigmoid')(dense_output)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Flatten, Dense, Dropout, BatchNormalization, ReLU\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "# def conv_block(inputs, num_filters):\n",
    "#     x = Sequential([\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU(),\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU()\n",
    "#     ])(inputs)\n",
    "#     return x\n",
    "\n",
    "# def dense_block(units):\n",
    "#     return Sequential([\n",
    "#         Dense(units, activation='relu'),\n",
    "#         Dropout(0.5),\n",
    "#         BatchNormalization()\n",
    "#     ])\n",
    "\n",
    "# def unet_plus_plus_model(hp):\n",
    "#     inputs = Input(shape=(128, 128, 3))\n",
    "\n",
    "#     # Encoder with ResNet50V2\n",
    "#     base_model = ResNet50V2(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "#     base_model.trainable = False  # Freeze the model\n",
    "\n",
    "#     # Collect skip connections\n",
    "#     s1 = base_model.get_layer(\"conv2_block3_out\").output  # 128x128\n",
    "#     s2 = base_model.get_layer(\"conv3_block4_out\").output  # 64x64\n",
    "#     s3 = base_model.get_layer(\"conv4_block6_out\").output  # 32x32\n",
    "#     s4 = base_model.get_layer(\"post_relu\").output         # 16x16\n",
    "\n",
    "#     # Nested Decoding Path\n",
    "#     x_00 = s1\n",
    "#     x_10 = conv_block(s2, hp.Choice('filters_x10', values=[64, 128, 256]))\n",
    "#     x_20 = conv_block(s3, hp.Choice('filters_x20', values=[128, 256, 512]))\n",
    "#     x_30 = conv_block(s4, hp.Choice('filters_x30', values=[256, 512, 1024]))\n",
    "\n",
    "#     # Up-sampling and concatenation\n",
    "#     x_01 = conv_block(concatenate([x_00, UpSampling2D()(x_10)]), hp.Choice('filters_x01', values=[64, 128, 256]))\n",
    "#     x_11 = conv_block(concatenate([x_10, UpSampling2D()(x_20)]), hp.Choice('filters_x11', values=[128, 256, 512]))\n",
    "#     x_21 = conv_block(concatenate([x_20, UpSampling2D()(x_30)]), hp.Choice('filters_x21', values=[256, 512, 1024]))\n",
    "\n",
    "#     x_02 = conv_block(concatenate([x_00, x_01, UpSampling2D()(x_11)]), hp.Choice('filters_x02', values=[64, 128, 256]))\n",
    "#     x_12 = conv_block(concatenate([x_10, x_11, UpSampling2D()(x_21)]), hp.Choice('filters_x12', values=[128, 256, 512]))\n",
    "\n",
    "#     x_03 = conv_block(concatenate([x_00, x_01, x_02, UpSampling2D()(x_12)]), hp.Choice('filters_x03', values=[64, 128, 256]))\n",
    "\n",
    "#     # Final convolution\n",
    "#     outputs = Conv2D(2, 1, activation='sigmoid')(x_03)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name='ResNet50V2_Unet_plus_plus')\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Dropout, Concatenate, Flatten, Dense, Reshape\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "def convolution_block(inputs, num_filters, kernel_size=3, padding=\"same\", use_bias=False):\n",
    "    x = Conv2D(num_filters, kernel_size=kernel_size, padding=padding, use_bias=use_bias, kernel_initializer=HeNormal())(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def upsample_block(inputs, skip_features, num_filters):\n",
    "    x = UpSampling2D((2, 2))(inputs)\n",
    "    x = Conv2D(num_filters, (2, 2), padding=\"same\")(x)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = convolution_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def dense_upsampling_block(inputs, skip_features, num_filters):\n",
    "    x = UpSampling2D((2, 2))(inputs)\n",
    "    x = Conv2D(num_filters, (2, 2), padding=\"same\")(x)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = convolution_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def unet_plus_plus_model(hp, input_shape=(32, 32, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    resnet50 = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    resnet50.trainable = True\n",
    "\n",
    "    # Encoder\n",
    "    s1 = resnet50.get_layer(\"conv1_relu\").output\n",
    "    s2 = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "    s3 = resnet50.get_layer(\"conv3_block4_out\").output\n",
    "    s4 = resnet50.get_layer(\"conv4_block6_out\").output\n",
    "\n",
    "    b1 = resnet50.get_layer(\"conv5_block3_out\").output\n",
    "    # Flatten the bottleneck output\n",
    "    x = Flatten()(b1)\n",
    "    UNITS = hp.Choice('units', values=[256, 512, 1024, 2048], default=256)\n",
    "    DROPOUT = hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1, default=0.0)\n",
    "\n",
    "    BATCHNORM = hp.Boolean('batchnorm', default=False)\n",
    "    # Dense layers between encoder and decoder\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    # Reshape back to spatial dimensions for the decoder\n",
    "    SIZE = input_shape[0] // 32  # Assuming input size is a multiple of 32\n",
    "    if BATCHNORM:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Dense(SIZE * SIZE * 2048, activation='relu')(x)\n",
    "    x = Reshape((SIZE, SIZE, 2048))(x)\n",
    "\n",
    "    # Nested U-Net\n",
    "    d4_2 = dense_upsampling_block(x, s4, 512)\n",
    "    d3_2 = dense_upsampling_block(d4_2, s3, 256)\n",
    "    d2_2 = dense_upsampling_block(d3_2, s2, 128)\n",
    "    d1_2 = dense_upsampling_block(d2_2, s1, 64)\n",
    "\n",
    "    d4_1 = upsample_block(x, s4, 512)\n",
    "    d3_1 = upsample_block(d4_1, s3, 256)\n",
    "    d2_1 = upsample_block(d3_1, s2, 128)\n",
    "    d1_1 = upsample_block(d2_1, s1, 64)\n",
    "\n",
    "    outputs = UpSampling2D()(d1_1)\n",
    "    outputs = Conv2D(2, (1, 1), padding=\"same\", activation=\"linear\")(outputs)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['sgd', 'rmsprop', 'adam'], default='adam')\n",
    "    lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5], default=1e-3)\n",
    "\n",
    "    if optimizer_choice == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\design_a\\tuner0.json\n",
      "{'units': 2048, 'dropout': 0.30000000000000004, 'batchnorm': True, 'optimizer': 'adam', 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "tunerA = kt.BayesianOptimization(unet_plus_plus_model,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 30,\n",
    "                     project_name='design_a',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8)\n",
    "\n",
    "tunerA.search(train_generator, epochs=50, validation_data=val_generator, callbacks=[stop_early, reduce_lr])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsA=tunerA.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "print(best_hpsA.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hpsA.values['units']=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Unet_plus_plus\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 38, 38, 3)    0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 16, 16, 64)   9472        ['conv1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 16, 16, 64)   256         ['conv1_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 16, 16, 64)   0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 18, 18, 64)   0           ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 8, 8, 64)     0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 8, 8, 64)     4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 8, 8, 256)    16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 8, 8, 256)    0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 8, 8, 64)     16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 8, 8, 256)    0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 8, 8, 64)     16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 8, 8, 256)    0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 4, 4, 128)    32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 4, 4, 512)    131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 4, 4, 512)    0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 4, 4, 512)    0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 4, 4, 512)    0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 4, 4, 512)    0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 2, 2, 256)    131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 2, 2, 1024)   525312      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                                                  'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block1_out[0][0]',       \n",
      "                                                                  'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block2_out[0][0]',       \n",
      "                                                                  'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block3_out[0][0]',       \n",
      "                                                                  'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block4_out[0][0]',       \n",
      "                                                                  'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block5_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block5_out[0][0]',       \n",
      "                                                                  'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block6_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 1, 1, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 1, 1, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 1, 1, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 1, 1, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 2048)         0           ['conv5_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 2048)         4196352     ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 2048)         0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 2048)         4196352     ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 2048)         0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 2048)         4196352     ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 2048)         0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 2048)         4196352     ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 2048)        8192        ['dense_13[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 2048)         4196352     ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1, 1, 2048)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " up_sampling2d_22 (UpSampling2D  (None, 2, 2, 2048)  0           ['reshape_2[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 2, 2, 512)    4194816     ['up_sampling2d_22[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 2, 2, 1536)   0           ['conv2d_42[0][0]',              \n",
      "                                                                  'conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 2, 2, 512)    7077888     ['concatenate_20[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 2, 2, 512)   2048        ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 2, 2, 512)    0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_23 (UpSampling2D  (None, 4, 4, 512)   0           ['activation_20[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 4, 4, 256)    524544      ['up_sampling2d_23[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 4, 4, 768)    0           ['conv2d_44[0][0]',              \n",
      "                                                                  'conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 4, 4, 256)    1769472     ['concatenate_21[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 4, 4, 256)   1024        ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 4, 4, 256)    0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_24 (UpSampling2D  (None, 8, 8, 256)   0           ['activation_21[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 8, 8, 128)    131200      ['up_sampling2d_24[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 8, 8, 384)    0           ['conv2d_46[0][0]',              \n",
      "                                                                  'conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 8, 8, 128)    442368      ['concatenate_22[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 8, 8, 128)   512         ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_25 (UpSampling2D  (None, 16, 16, 128)  0          ['activation_22[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 16, 16, 64)   32832       ['up_sampling2d_25[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 16, 16, 128)  0           ['conv2d_48[0][0]',              \n",
      "                                                                  'conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 16, 16, 64)   73728       ['concatenate_23[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16, 16, 64)  256         ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_26 (UpSampling2D  (None, 32, 32, 64)  0           ['activation_23[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 32, 32, 2)    130         ['up_sampling2d_26[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 58,828,482\n",
      "Trainable params: 58,769,346\n",
      "Non-trainable params: 59,136\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the model with the best hp.\n",
    "modelA = unet_plus_plus_model(best_hpsA)\n",
    "modelA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n",
      "40/40 - 9s - loss: 0.3759 - mae: 0.3946 - val_loss: 64.8420 - val_mae: 7.0337 - lr: 0.0010 - 9s/epoch - 224ms/step\n",
      "Epoch 2/1024\n",
      "40/40 - 3s - loss: 0.0434 - mae: 0.1477 - val_loss: 3.1900 - val_mae: 1.4944 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 3/1024\n",
      "40/40 - 3s - loss: 0.0371 - mae: 0.1332 - val_loss: 1.2023 - val_mae: 0.9142 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 4/1024\n",
      "40/40 - 3s - loss: 0.0351 - mae: 0.1293 - val_loss: 0.2846 - val_mae: 0.4341 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 5/1024\n",
      "40/40 - 3s - loss: 0.0337 - mae: 0.1260 - val_loss: 0.1389 - val_mae: 0.2879 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 6/1024\n",
      "40/40 - 3s - loss: 0.0329 - mae: 0.1239 - val_loss: 0.1077 - val_mae: 0.2335 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 7/1024\n",
      "40/40 - 3s - loss: 0.0316 - mae: 0.1201 - val_loss: 0.0824 - val_mae: 0.1992 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 8/1024\n",
      "40/40 - 3s - loss: 0.0314 - mae: 0.1206 - val_loss: 0.0504 - val_mae: 0.1514 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 9/1024\n",
      "40/40 - 3s - loss: 0.0309 - mae: 0.1177 - val_loss: 0.0359 - val_mae: 0.1277 - lr: 0.0010 - 3s/epoch - 73ms/step\n",
      "Epoch 10/1024\n",
      "40/40 - 3s - loss: 0.0310 - mae: 0.1192 - val_loss: 0.0396 - val_mae: 0.1298 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 11/1024\n",
      "40/40 - 3s - loss: 0.0302 - mae: 0.1158 - val_loss: 0.0424 - val_mae: 0.1312 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 12/1024\n",
      "40/40 - 3s - loss: 0.0299 - mae: 0.1143 - val_loss: 0.0377 - val_mae: 0.1258 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 13/1024\n",
      "40/40 - 3s - loss: 0.0293 - mae: 0.1123 - val_loss: 0.0347 - val_mae: 0.1341 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 14/1024\n",
      "40/40 - 3s - loss: 0.0302 - mae: 0.1154 - val_loss: 0.0432 - val_mae: 0.1345 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 15/1024\n",
      "40/40 - 3s - loss: 0.0291 - mae: 0.1122 - val_loss: 0.0363 - val_mae: 0.1199 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 16/1024\n",
      "40/40 - 3s - loss: 0.0285 - mae: 0.1099 - val_loss: 0.0357 - val_mae: 0.1159 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 17/1024\n",
      "40/40 - 3s - loss: 0.0287 - mae: 0.1097 - val_loss: 0.0338 - val_mae: 0.1152 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 18/1024\n",
      "40/40 - 3s - loss: 0.0288 - mae: 0.1107 - val_loss: 0.0366 - val_mae: 0.1214 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 19/1024\n",
      "40/40 - 3s - loss: 0.0289 - mae: 0.1104 - val_loss: 0.0308 - val_mae: 0.1102 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 20/1024\n",
      "40/40 - 3s - loss: 0.0289 - mae: 0.1105 - val_loss: 0.0333 - val_mae: 0.1122 - lr: 0.0010 - 3s/epoch - 69ms/step\n",
      "Epoch 21/1024\n",
      "40/40 - 3s - loss: 0.0280 - mae: 0.1085 - val_loss: 0.0313 - val_mae: 0.1084 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 22/1024\n",
      "40/40 - 3s - loss: 0.0284 - mae: 0.1095 - val_loss: 0.0353 - val_mae: 0.1250 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 23/1024\n",
      "40/40 - 3s - loss: 0.0281 - mae: 0.1080 - val_loss: 0.0330 - val_mae: 0.1202 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 24/1024\n",
      "40/40 - 3s - loss: 0.0285 - mae: 0.1081 - val_loss: 0.0304 - val_mae: 0.1015 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 25/1024\n",
      "40/40 - 3s - loss: 0.0273 - mae: 0.1052 - val_loss: 0.0307 - val_mae: 0.1143 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 26/1024\n",
      "40/40 - 3s - loss: 0.0283 - mae: 0.1087 - val_loss: 0.0328 - val_mae: 0.1154 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 27/1024\n",
      "40/40 - 3s - loss: 0.0273 - mae: 0.1067 - val_loss: 0.0281 - val_mae: 0.1124 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 28/1024\n",
      "40/40 - 3s - loss: 0.0276 - mae: 0.1060 - val_loss: 0.0279 - val_mae: 0.1097 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 29/1024\n",
      "40/40 - 3s - loss: 0.0281 - mae: 0.1067 - val_loss: 0.0259 - val_mae: 0.0997 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 30/1024\n",
      "40/40 - 3s - loss: 0.0275 - mae: 0.1052 - val_loss: 0.0344 - val_mae: 0.1186 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 31/1024\n",
      "40/40 - 3s - loss: 0.0277 - mae: 0.1058 - val_loss: 0.0238 - val_mae: 0.0952 - lr: 0.0010 - 3s/epoch - 72ms/step\n",
      "Epoch 32/1024\n",
      "40/40 - 3s - loss: 0.0277 - mae: 0.1059 - val_loss: 0.0256 - val_mae: 0.1017 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 33/1024\n",
      "40/40 - 3s - loss: 0.0270 - mae: 0.1037 - val_loss: 0.0281 - val_mae: 0.1131 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 34/1024\n",
      "40/40 - 3s - loss: 0.0278 - mae: 0.1065 - val_loss: 0.0253 - val_mae: 0.1016 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 35/1024\n",
      "40/40 - 3s - loss: 0.0271 - mae: 0.1043 - val_loss: 0.0334 - val_mae: 0.1136 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 36/1024\n",
      "40/40 - 3s - loss: 0.0272 - mae: 0.1052 - val_loss: 0.0287 - val_mae: 0.1103 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 37/1024\n",
      "40/40 - 3s - loss: 0.0273 - mae: 0.1030 - val_loss: 0.0246 - val_mae: 0.0985 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 38/1024\n",
      "40/40 - 3s - loss: 0.0261 - mae: 0.0996 - val_loss: 0.0233 - val_mae: 0.0945 - lr: 1.0000e-04 - 3s/epoch - 72ms/step\n",
      "Epoch 39/1024\n",
      "40/40 - 3s - loss: 0.0260 - mae: 0.0989 - val_loss: 0.0241 - val_mae: 0.0955 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 40/1024\n",
      "40/40 - 3s - loss: 0.0259 - mae: 0.0987 - val_loss: 0.0244 - val_mae: 0.0946 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 41/1024\n",
      "40/40 - 3s - loss: 0.0261 - mae: 0.0990 - val_loss: 0.0229 - val_mae: 0.0915 - lr: 1.0000e-04 - 3s/epoch - 72ms/step\n",
      "Epoch 42/1024\n",
      "40/40 - 3s - loss: 0.0259 - mae: 0.0983 - val_loss: 0.0237 - val_mae: 0.0921 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 43/1024\n",
      "40/40 - 3s - loss: 0.0256 - mae: 0.0976 - val_loss: 0.0235 - val_mae: 0.0944 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 44/1024\n",
      "40/40 - 3s - loss: 0.0255 - mae: 0.0978 - val_loss: 0.0238 - val_mae: 0.0940 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 45/1024\n",
      "40/40 - 3s - loss: 0.0258 - mae: 0.0977 - val_loss: 0.0243 - val_mae: 0.0950 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 46/1024\n",
      "40/40 - 3s - loss: 0.0256 - mae: 0.0972 - val_loss: 0.0248 - val_mae: 0.0973 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 47/1024\n",
      "40/40 - 3s - loss: 0.0251 - mae: 0.0965 - val_loss: 0.0245 - val_mae: 0.0965 - lr: 1.0000e-05 - 3s/epoch - 68ms/step\n",
      "Epoch 48/1024\n",
      "40/40 - 3s - loss: 0.0256 - mae: 0.0973 - val_loss: 0.0239 - val_mae: 0.0951 - lr: 1.0000e-05 - 3s/epoch - 68ms/step\n",
      "Epoch 49/1024\n",
      "40/40 - 3s - loss: 0.0255 - mae: 0.0971 - val_loss: 0.0237 - val_mae: 0.0944 - lr: 1.0000e-05 - 3s/epoch - 68ms/step\n",
      "Epoch 50/1024\n",
      "40/40 - 3s - loss: 0.0257 - mae: 0.0973 - val_loss: 0.0238 - val_mae: 0.0947 - lr: 1.0000e-05 - 3s/epoch - 68ms/step\n",
      "Epoch 51/1024\n",
      "40/40 - 3s - loss: 0.0255 - mae: 0.0972 - val_loss: 0.0238 - val_mae: 0.0945 - lr: 1.0000e-05 - 3s/epoch - 68ms/step\n",
      "Epoch 52/1024\n",
      "40/40 - 3s - loss: 0.0253 - mae: 0.0968 - val_loss: 0.0237 - val_mae: 0.0942 - lr: 1.0000e-06 - 3s/epoch - 67ms/step\n",
      "Epoch 53/1024\n",
      "40/40 - 3s - loss: 0.0256 - mae: 0.0976 - val_loss: 0.0237 - val_mae: 0.0942 - lr: 1.0000e-06 - 3s/epoch - 68ms/step\n",
      "Epoch 54/1024\n",
      "40/40 - 3s - loss: 0.0254 - mae: 0.0971 - val_loss: 0.0237 - val_mae: 0.0942 - lr: 1.0000e-06 - 3s/epoch - 68ms/step\n",
      "Epoch 55/1024\n",
      "40/40 - 3s - loss: 0.0255 - mae: 0.0973 - val_loss: 0.0238 - val_mae: 0.0946 - lr: 1.0000e-06 - 3s/epoch - 68ms/step\n",
      "Epoch 56/1024\n",
      "40/40 - 3s - loss: 0.0252 - mae: 0.0967 - val_loss: 0.0238 - val_mae: 0.0947 - lr: 1.0000e-06 - 3s/epoch - 68ms/step\n",
      "Epoch 57/1024\n",
      "40/40 - 3s - loss: 0.0254 - mae: 0.0969 - val_loss: 0.0240 - val_mae: 0.0952 - lr: 1.0000e-07 - 3s/epoch - 71ms/step\n",
      "Training time: 164.94608640670776s\n"
     ]
    }
   ],
   "source": [
    "## DESIGN A\n",
    "import time \n",
    "start = time.time()\n",
    "historyA = modelA.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [ES, REDUCE_LR])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")\n",
    "\n",
    "# ## FUNCTIONALITY: INFERENCE TIME\n",
    "# modelC.evaluate(train_generator[1][0][0].reshape(1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare set of x values and y values\n",
    "X_values_1,y_values_1 = [],[]\n",
    "for i in range(40):\n",
    "    values = next(train_generator)\n",
    "    # for j in range(values[0].shape[0]):\n",
    "    X_values_1.append(values[0]) \n",
    "    y_values_1.append(values[1])\n",
    "\n",
    "## create X_values generator\n",
    "gen_X_values_1_1 = (x for x in X_values_1)\n",
    "gen_X_values_2_1 = (x for x in X_values_1)\n",
    "gen_X_values_3_1 = (x for x in X_values_1)\n",
    "y_values_1 = [y  for y_set in y_values_1 for y in y_set]\n",
    "\n",
    "## Prepare set of x values and y values\n",
    "X_values_2,y_values_2 = [],[]\n",
    "for i in range(10):\n",
    "    values = next(val_generator)\n",
    "    # for j in range(values[0].shape[0]):\n",
    "    X_values_2.append(values[0])\n",
    "    y_values_2.append(values[1])\n",
    "\n",
    "## create X_values generator\n",
    "gen_X_values_1_2 = (x for x in X_values_2)\n",
    "gen_X_values_2_2 = (x for x in X_values_2)\n",
    "gen_X_values_3_2 = (x for x in X_values_2)\n",
    "y_values_2 = [y  for y_set in y_values_2 for y in y_set]\n",
    "\n",
    "SAMPLE = next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 27ms/step\n",
      "Train Accuracy\n",
      "Moisture: 99.65, pH: 99.18\n",
      "10/10 [==============================] - 0s 23ms/step\n",
      "Test Accuracy\n",
      "Moisture: 98.93, pH: 99.85\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "TRUTH moisture: 0.13, ph: 3.74\n",
      "PREDICTIONS moisture: 0.13, ph: 3.75\n"
     ]
    }
   ],
   "source": [
    "## Prepare set of x values and y values\n",
    "\n",
    "modelA_train_accuracy = get_model_accuracy(modelA, gen_X_values_1_1, y_values_1, X_values_1)\n",
    "print(\"Train Accuracy\")\n",
    "print(\"Moisture: {:.2f}, pH: {:.2f}\".format(modelA_train_accuracy[0], modelA_train_accuracy[1]))\n",
    "\n",
    "modelA_test_accuracy = get_model_accuracy(modelA, gen_X_values_1_2, y_values_2, X_values_2)\n",
    "print(\"Test Accuracy\")\n",
    "print(\"Moisture: {:.2f}, pH: {:.2f}\".format(modelA_test_accuracy[0], modelA_test_accuracy[1]))\n",
    "\n",
    "predictions = modelA.predict(np.array([SAMPLE[0][0]]))\n",
    "truth = unprocess_label_wmask(SAMPLE[0][0],SAMPLE[1][0])\n",
    "pred = unprocess_label_wmask(SAMPLE[0][0],predictions[0])\n",
    "print(\"TRUTH moisture: {:.2f}, ph: {:.2f}\".format(truth[0],truth[1]))\n",
    "print(\"PREDICTIONS moisture: {:.2f}, ph: {:.2f}\".format(pred[0],pred[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelA.fit(train_generator, validation_data=val_generator, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#    Manual Training with y data alteration\n",
    "# '''\n",
    "# ## Variable declaration\n",
    "# epochs = 512\n",
    "# steps_per_epoch = 512 // batch_size + 1\n",
    "# min_lr = 1e-8\n",
    "# factor = 0.1\n",
    "# SCHEDULE_EPOCH = 50\n",
    "# STEPS = 50\n",
    "# MONITOR = \"loss\"\n",
    "# PATIENCE = 10\n",
    "# WEIGHTS = []\n",
    "\n",
    "# historyA = {\"history\": {\"loss\": [], \"mae\": [], \"val_loss\": [], \"val_mae\": []}}\n",
    "# for e in range(epochs):\n",
    "#     for i, (images, y_batch) in enumerate(train_generator):\n",
    "#         new_y_batch = []\n",
    "\n",
    "#         ## Train data y alteration\n",
    "#         for x, img in enumerate(images):\n",
    "#             array = np.ones(image_size + (3,))\n",
    "#             array *= img > 0\n",
    "#             array[array > 0] = y_batch[x]\n",
    "#             new_y_batch.append(array)\n",
    "#         new_y_batch = np.array(new_y_batch)\n",
    "#         loss = modelA.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "\n",
    "#         if i >= steps_per_epoch:  # manually detect the end of the epoch train set\n",
    "#             historyA[\"history\"][\"loss\"].append(loss[0])\n",
    "#             historyA[\"history\"][\"mae\"].append(loss[1])\n",
    "#             break\n",
    "\n",
    "#     for i, (images, y_batch) in enumerate(val_generator):\n",
    "#         new_y_batch = []\n",
    "\n",
    "#         ## Val data y alteration\n",
    "#         for x, img in enumerate(images):\n",
    "#             array = np.ones(image_size + (3,))\n",
    "#             array *= img > 0\n",
    "#             array[array > 0] = y_batch[x]\n",
    "#             new_y_batch.append(array)\n",
    "#         new_y_batch = np.array(new_y_batch)\n",
    "#         val = modelA.test_on_batch(images, new_y_batch)\n",
    "\n",
    "#         if i >= steps_per_epoch:  # manually detect the end of the epoch validation set\n",
    "#             historyA[\"history\"][\"val_loss\"].append(val[0])\n",
    "#             historyA[\"history\"][\"val_mae\"].append(val[1])\n",
    "#             break\n",
    "        \n",
    "#     ## LR Scheduler\n",
    "#     curr_lr = modelA.optimizer.learning_rate\n",
    "#     if e >= SCHEDULE_EPOCH and curr_lr > min_lr:\n",
    "#         K.set_value(modelA.optimizer.learning_rate, curr_lr * factor)\n",
    "#         curr_lr = modelA.optimizer.learning_rate\n",
    "#         SCHEDULE_EPOCH += STEPS\n",
    "\n",
    "#     # ## Early Stopping\n",
    "#     # if e > PATIENCE:\n",
    "#     #     PAST = historyA[\"history\"][MONITOR][-PATIENCE-1:-1]\n",
    "#     #     LATEST = historyA[\"history\"][MONITOR][-1]\n",
    "#     #     for i,P in enumerate(PAST):\n",
    "#     #         if LATEST > P:\n",
    "#     #             PATIENCE -= 1  ## decrement patience if previous monitor is worse than the current value\n",
    "\n",
    "#     # if PATIENCE <= 0:\n",
    "#     #     break  ## if no more patience left, early stop training\n",
    "#     # PATIENCE = 10\n",
    "\n",
    "\n",
    "#     print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}   CURR_LR {:.2E}\".format(\n",
    "#         e + 1, loss[0], loss[1], val[0], val[1], curr_lr.numpy()))\n",
    "#     train_generator.on_epoch_end()  # shuffles the data at the end of each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyA[\"history\"][\"loss\"], color='g',alpha=.5)\n",
    "# plt.plot(historyA[\"history\"][\"val_loss\"], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyA[\"history\"][\"mae\"], color='g',alpha=.5)\n",
    "# plt.plot(historyA[\"history\"][\"val_mae\"], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.suptitle(\"Model A: Nested U-Net\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(modelA(sample[0]))\n",
    "# print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validation\n",
    "# val_sample = next(val_generator)\n",
    "# val_result = modelA.predict(val_sample[0])\n",
    "\n",
    "# for x, i in enumerate(val_result):\n",
    "#     RGB = cv2.cvtColor(val_sample[0][x], cv2.COLOR_HSV2RGB)\n",
    "#     MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "#     ARRAY = np.array(val_result[x]).reshape(image_size) * MASK\n",
    "#     OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "#     print(\"Validation - Average Output:\", np.average(OUTPUT), \"Ground Truth:\", np.average(val_sample[1][x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelC.save('design_models/designA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelA = tf.keras.models.load_model('design_models/designA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shutil import rmtree\n",
    "# # removing directory \n",
    "# rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA.save('design_models/designA_v6.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Conv2DTranspose, Dropout, Flatten, Dense, Reshape\n",
    "# from tensorflow.keras import backend as K\n",
    "# import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "\n",
    "# def segnet(hp):\n",
    "#     hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "    \n",
    "#     # Encoding path\n",
    "#     inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    \n",
    "#     # Encoder\n",
    "#     x = Conv2D(hp_filters, (3, 3), padding='same', name='conv1')(inputs)\n",
    "#     x = BatchNormalization(name='bn1')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(hp_filters, (3, 3), padding='same', name='conv2')(x)\n",
    "#     x = BatchNormalization(name='bn2')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool1')(x)\n",
    "    \n",
    "#     x = Conv2D(hp_filters * 2, (3, 3), padding='same', name='conv3')(x)\n",
    "#     x = BatchNormalization(name='bn3')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(hp_filters * 2, (3, 3), padding='same', name='conv4')(x)\n",
    "#     x = BatchNormalization(name='bn4')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool2')(x)\n",
    "    \n",
    "#     x = Conv2D(hp_filters * 4, (3, 3), padding='same', name='conv5')(x)\n",
    "#     x = BatchNormalization(name='bn5')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(hp_filters * 4, (3, 3), padding='same', name='conv6')(x)\n",
    "#     x = BatchNormalization(name='bn6')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(hp_filters * 4, (3, 3), padding='same', name='conv7')(x)\n",
    "#     x = BatchNormalization(name='bn7')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool3')(x)\n",
    "    \n",
    "#     x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv8')(x)\n",
    "#     x = BatchNormalization(name='bn8')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv9')(x)\n",
    "#     x = BatchNormalization(name='bn9')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv10')(x)\n",
    "#     x = BatchNormalization(name='bn10')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool4')(x)\n",
    "    \n",
    "#     x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv11')(x)\n",
    "#     x = BatchNormalization(name='bn11')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv12')(x)\n",
    "#     x = BatchNormalization(name='bn12')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv13')(x)\n",
    "#     x = BatchNormalization(name='bn13')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     qwe = MaxPooling2D(name='pool5')(x)\n",
    "#     print(x.shape)\n",
    "#     x = Flatten()(x);UNITS = hp.Choice('units',values = [256,512,1024,2048], default=256);DROPOUT = hp.Float('dropout',min_value=0.0, max_value=0.5, step=0.1, default=0.0)\n",
    "    \n",
    "#     BATCHNORM = hp.Boolean('batchnorm',default=False)\n",
    "#     # Dense layers between encoder and decoder\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     # Reshape back to spatial dimensions for the decoder\n",
    "#     SIZE = IMAGE_SIZE[0] // 16  # Assuming input size is a multiple of 32\n",
    "#     if BATCHNORM:\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Dense(SIZE * SIZE * 512, activation='relu')(x)\n",
    "#     print((SIZE, SIZE, 512))\n",
    "#     x = Reshape((SIZE, SIZE, 512))(x)\n",
    "    \n",
    "#     # Decoding path\n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(hp_filters * 8, (3, 3), padding='same', name='deconv1')(x)\n",
    "#     x = BatchNormalization(name='bn14')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(hp_filters * 8, (3, 3), padding='same', name='deconv2')(x)\n",
    "#     x = BatchNormalization(name='bn15')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(hp_filters * 8, (3, 3), padding='same', name='deconv3')(x)\n",
    "#     x = BatchNormalization(name='bn16')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(hp_filters * 4, (3, 3), padding='same', name='deconv4')(x)\n",
    "#     x = BatchNormalization(name='bn17')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(hp_filters * 4, (3, 3), padding='same', name='deconv5')(x)\n",
    "#     x = BatchNormalization(name='bn18')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(hp_filters * 4, (3, 3), padding='same', name='deconv6')(x)\n",
    "#     x = BatchNormalization(name='bn19')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(hp_filters * 2, (3, 3), padding='same', name='deconv7')(x)\n",
    "#     x = BatchNormalization(name='bn20')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(hp_filters * 2, (3, 3), padding='same', name='deconv8')(x)\n",
    "#     x = BatchNormalization(name='bn21')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(hp_filters, (3, 3), padding='same', name='deconv9')(x)\n",
    "#     x = BatchNormalization(name='bn22')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(hp_filters, (3, 3), padding='same', name='deconv10')(x)\n",
    "#     x = BatchNormalization(name='bn23')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(hp_filters, (3, 3), padding='same', name='deconv11')(x)\n",
    "#     x = BatchNormalization(name='bn24')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(1, (3, 3), padding='same', name='deconv12')(x)\n",
    "#     x = BatchNormalization(name='bn25')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(1, (3, 3), padding='same', name='deconv13')(x)\n",
    "#     x = BatchNormalization(name='bn26')(x)\n",
    "    \n",
    "#     # Output layer with sigmoid activation for binary segmentation\n",
    "#     outputs = Conv2D(2, (1, 1), strides=2, activation='sigmoid')(x)\n",
    "#     print(outputs.shape)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=outputs);lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5]);model.compile(optimizer= tf.keras.optimizers.Adam(lr=lr), loss= [\"binary_crossentropy\"], metrics=['acc'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Concatenate\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# def segnet(hp, input_shape=(64, 64, 3), num_classes=2):\n",
    "#     # Load ResNet50 with pre-trained weights and without top layers\n",
    "#     resnet_base = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "#     # Get encoder output layers\n",
    "#     encoder_outputs = [\n",
    "#         resnet_base.get_layer(\"conv1_relu\").output,\n",
    "#         resnet_base.get_layer(\"conv2_block3_out\").output,\n",
    "#         resnet_base.get_layer(\"conv3_block4_out\").output,\n",
    "#         resnet_base.get_layer(\"conv4_block6_out\").output,\n",
    "#         resnet_base.get_layer(\"conv5_block3_out\").output\n",
    "#     ]\n",
    "\n",
    "#     # Decoder part of SegNet\n",
    "#     x = encoder_outputs[-1]\n",
    "#     for i in range(4, 0, -1):\n",
    "#         x = UpSampling2D()(x)\n",
    "#         x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Concatenate()([x, encoder_outputs[i - 1]])\n",
    "\n",
    "#     x = UpSampling2D()(x)\n",
    "#     # Output layer\n",
    "#     outputs = Conv2D(2, (1, 1), activation='softmax')(x)\n",
    "#     print(outputs.shape)\n",
    "#     # Create model\n",
    "#     model = Model(inputs=resnet_base.input, outputs=outputs)\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5]);model.compile(optimizer= tf.keras.optimizers.Adam(lr=lr), loss= [\"binary_crossentropy\"], metrics=['acc'])\n",
    "#     return model\n",
    "\n",
    "# # Usage\n",
    "# # model = segnet_with_resnet50()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Conv2DTranspose, Dropout, Flatten, Dense, Reshape\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras import backend as K\n",
    "# import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "\n",
    "# def segnet(hp):\n",
    "#     hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "    \n",
    "#     # Load ResNet50 with pre-trained weights\n",
    "#     resnet_base = ResNet50(include_top=False, weights='imagenet', input_shape=(64,64,3))\n",
    "#     resnet_base.trainable = False\n",
    "\n",
    "#     # Encoder part using ResNet50\n",
    "#     inputs = Input(shape=(64,64,3))\n",
    "#     encoder_outputs = [\n",
    "#         resnet_base.get_layer(\"conv1_relu\").output,\n",
    "#         resnet_base.get_layer(\"conv2_block3_out\").output,\n",
    "#         resnet_base.get_layer(\"conv3_block4_out\").output,\n",
    "#         resnet_base.get_layer(\"conv4_block6_out\").output,\n",
    "#         resnet_base.get_layer(\"conv5_block3_out\").output\n",
    "#     ]\n",
    "\n",
    "#     # Flatten the bottleneck output\n",
    "#     x = Flatten()(encoder_outputs[-1])\n",
    "\n",
    "#     # Dense layers between encoder and decoder\n",
    "#     x = Dense(2048, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(2048, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(2048, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(2048, activation='relu')(x)\n",
    "    \n",
    "#     print(encoder_outputs[-1].shape)\n",
    "    \n",
    "#     # Reshape back to spatial dimensions for the decoder\n",
    "#     SIZE = 2\n",
    "    \n",
    "#     x = Dense(SIZE * SIZE * 2048, activation='relu')(x)\n",
    "#     x = Reshape((SIZE, SIZE, 2048))(x)\n",
    "    \n",
    "#     # Decoder part\n",
    "#     for i in range(4, 0, -1):\n",
    "#         x = UpSampling2D()(x)\n",
    "#         x = Conv2D(hp_filters * 8, (3, 3), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "#         x = Conv2D(hp_filters * 8, (3, 3), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "#         x = Conv2D(hp_filters * 8, (3, 3), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "#         x = Concatenate()([x, encoder_outputs[i - 1]])\n",
    "\n",
    "#     # Output layer\n",
    "#     outputs = Conv2D(2, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "#     # Create and compile model\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "def segnet(hp, input_shape=(32, 32, 3), num_classes=2):\n",
    "    # Load the VGG16 model with batch normalization\n",
    "    vgg16_bn = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    vgg16_bn.trainable = True\n",
    "    \n",
    "    # Encoder\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Using the pretrained VGG16_bn layers\n",
    "    x = vgg16_bn.get_layer('block1_conv1')(inputs)\n",
    "    x = vgg16_bn.get_layer('block1_conv2')(x)\n",
    "    stage1 = x\n",
    "    x = vgg16_bn.get_layer('block1_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block2_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block2_conv2')(x)\n",
    "    stage2 = x\n",
    "    x = vgg16_bn.get_layer('block2_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block3_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block3_conv2')(x)\n",
    "    x = vgg16_bn.get_layer('block3_conv3')(x)\n",
    "    stage3 = x\n",
    "    x = vgg16_bn.get_layer('block3_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block4_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block4_conv2')(x)\n",
    "    x = vgg16_bn.get_layer('block4_conv3')(x)\n",
    "    stage4 = x\n",
    "    x = vgg16_bn.get_layer('block4_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block5_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block5_conv2')(x)\n",
    "    x = vgg16_bn.get_layer('block5_conv3')(x)\n",
    "    stage5 = x\n",
    "    x = vgg16_bn.get_layer('block5_pool')(x)\n",
    "\n",
    "    UNITS = hp.Choice('units',values = [256,512,1024,2048], default=256)\n",
    "    DROPOUT = hp.Float('dropout',min_value=0.0, max_value=0.5, step=0.1, default=0.0)\n",
    "    BATCHNORM = hp.Boolean('batchnorm',default=False)\n",
    "    \n",
    "    # Dense layers between encoder and decoder\n",
    "    x = layers.Flatten()(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    if BATCHNORM:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = layers.Dense(1*1*512, activation='relu')(x)  # Adjust to match the new dimensions\n",
    "    x = layers.Reshape((1, 1, 512))(x)  # Adjust to match the new dimensions\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    # Incorporate stage5\n",
    "    x = layers.Concatenate()([x, stage5])\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    # Incorporate stage4\n",
    "    x = layers.Concatenate()([x, stage4])\n",
    "    x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    # Incorporate stage3\n",
    "    x = layers.Concatenate()([x, stage3])\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    # Incorporate stage2\n",
    "    x = layers.Concatenate()([x, stage2])\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    # Incorporate stage1\n",
    "    x = layers.Concatenate()([x, stage1])\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(num_classes, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    outputs = Conv2D(2, (1, 1), padding=\"same\", activation=\"linear\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='SegNet')\n",
    "\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['sgd', 'rmsprop', 'adam'], default='adam')\n",
    "    lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5], default=1e-3)\n",
    "\n",
    "    if optimizer_choice == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=['mae'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\design_b\\tuner0.json\n",
      "{'units': 1024, 'dropout': 0.2, 'batchnorm': False, 'optimizer': 'rmsprop', 'learning_rate': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "tunerB = kt.BayesianOptimization(segnet,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 30,\n",
    "                     project_name='design_b',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8)\n",
    "\n",
    "\n",
    "tunerB.search(train_generator, epochs=50, validation_data=val_generator, callbacks=[stop_early, reduce_lr])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsB=tunerB.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "\n",
    "print(best_hpsB.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "# def segnet_resnet(input_shape, num_classes):\n",
    "#     # Define ResNet50 as the backbone\n",
    "#     backbone = ResNet50V2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "#     backbone.trainable = True\n",
    "    \n",
    "#     # # Encoder\n",
    "#     # x = backbone.get_layer(name=\"conv2_block2_out\").output\n",
    "#     # x = backbone.get_layer(name=\"conv3_block3_out\").output\n",
    "#     # x = backbone.get_layer(name=\"conv4_block4_out\").output\n",
    "#     # x = backbone.get_layer(name=\"conv5_block3_out\").output\n",
    "        \n",
    "#     # Encoder\n",
    "#     x = backbone.output\n",
    "#     denseOutputShape = x.shape\n",
    "    \n",
    "#     print(x.shape)\n",
    "    \n",
    "#     x = Flatten()(x);UNITS = 1024;DROPOUT = 0.4\n",
    "    \n",
    "#     BATCHNORM = False\n",
    "#     # Dense layers between encoder and decoder\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     # Reshape back to spatial dimensions for the decoder\n",
    "#     # SIZE = IMAGE_SIZE[0] // 32  # Assuming input size is a multiple of 32\n",
    "#     if BATCHNORM:\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Dense(denseOutputShape[1] * denseOutputShape[2] * denseOutputShape[3], activation='relu')(x)\n",
    "#     print((denseOutputShape[1], denseOutputShape[2], denseOutputShape[3]))\n",
    "#     x = Reshape((denseOutputShape[1], denseOutputShape[2], denseOutputShape[3]))(x)\n",
    "\n",
    "#     # Decoder\n",
    "#     decoder = layers.Conv2DTranspose(1024, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "#     decoder = layers.BatchNormalization()(decoder)\n",
    "#     decoder = layers.Activation('relu')(decoder)\n",
    "    \n",
    "#     decoder = layers.Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same')(decoder)\n",
    "#     decoder = layers.BatchNormalization()(decoder)\n",
    "#     decoder = layers.Activation('relu')(decoder)\n",
    "\n",
    "#     decoder = layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(decoder)\n",
    "#     decoder = layers.BatchNormalization()(decoder)\n",
    "#     decoder = layers.Activation('relu')(decoder)\n",
    "\n",
    "#     decoder = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(decoder)\n",
    "#     decoder = layers.BatchNormalization()(decoder)\n",
    "#     decoder = layers.Activation('relu')(decoder)\n",
    "\n",
    "#     decoder = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(decoder)\n",
    "#     decoder = layers.BatchNormalization()(decoder)\n",
    "#     decoder = layers.Activation('relu')(decoder)\n",
    "\n",
    "\n",
    "#     # Output layer\n",
    "#     output = layers.Conv2D(num_classes, (1, 1), activation='sigmoid', padding='same')(decoder)\n",
    "#     output = layers.Flatten()(output)\n",
    "#     output = layers.Dense(2, activation='sigmoid')(output)\n",
    "\n",
    "#     model = Model(inputs=backbone.input, outputs=output)\n",
    "    \n",
    "#     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "#     return model\n",
    "\n",
    "# # Example usage\n",
    "# input_shape = (64, 64, 3)  # Input shape of your images\n",
    "# num_classes = 2  # Number of segmentation classes\n",
    "# model = segnet_resnet(input_shape, num_classes)\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Conv2DTranspose, Dropout, Flatten, Dense, Reshape\n",
    "\n",
    "# def segnet():\n",
    "#     # Input layer\n",
    "#     inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    \n",
    "#     # Encoding path\n",
    "#     x = Conv2D(64, (3, 3), padding='same', name='conv1')(inputs)\n",
    "#     x = BatchNormalization(name='bn1')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(64, (3, 3), padding='same', name='conv2')(x)\n",
    "#     x = BatchNormalization(name='bn2')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool1')(x)\n",
    "    \n",
    "#     x = Conv2D(128, (3, 3), padding='same', name='conv3')(x)\n",
    "#     x = BatchNormalization(name='bn3')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(128, (3, 3), padding='same', name='conv4')(x)\n",
    "#     x = BatchNormalization(name='bn4')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool2')(x)\n",
    "    \n",
    "#     x = Conv2D(256, (3, 3), padding='same', name='conv5')(x)\n",
    "#     x = BatchNormalization(name='bn5')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(256, (3, 3), padding='same', name='conv6')(x)\n",
    "#     x = BatchNormalization(name='bn6')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(256, (3, 3), padding='same', name='conv7')(x)\n",
    "#     x = BatchNormalization(name='bn7')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool3')(x)\n",
    "    \n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv8')(x)\n",
    "#     x = BatchNormalization(name='bn8')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv9')(x)\n",
    "#     x = BatchNormalization(name='bn9')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv10')(x)\n",
    "#     x = BatchNormalization(name='bn10')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool4')(x)\n",
    "    \n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv11')(x)\n",
    "#     x = BatchNormalization(name='bn11')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv12')(x)\n",
    "#     x = BatchNormalization(name='bn12')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv13')(x)\n",
    "#     x = BatchNormalization(name='bn13')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     # Flattening and Dense layers\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "    \n",
    "#     # Reshape back to spatial dimensions for the decoder\n",
    "#     SIZE = IMAGE_SIZE[0] // 16  # Assuming input size is a multiple of 32\n",
    "#     x = Dense(SIZE * SIZE * 512, activation='relu')(x)\n",
    "#     x = Reshape((SIZE, SIZE, 512))(x)\n",
    "    \n",
    "#     # Decoding path\n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv1')(x)\n",
    "#     x = BatchNormalization(name='bn14')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv2')(x)\n",
    "#     x = BatchNormalization(name='bn15')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv3')(x)\n",
    "#     x = BatchNormalization(name='bn16')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv4')(x)\n",
    "#     x = BatchNormalization(name='bn17')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv5')(x)\n",
    "#     x = BatchNormalization(name='bn18')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv6')(x)\n",
    "#     x = BatchNormalization(name='bn19')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(128, (3, 3), padding='same', name='deconv7')(x)\n",
    "#     x = BatchNormalization(name='bn20')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(128, (3, 3), padding='same', name='deconv8')(x)\n",
    "#     x = BatchNormalization(name='bn21')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(64, (3, 3), padding='same', name='deconv9')(x)\n",
    "#     x = BatchNormalization(name='bn22')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     # Output layer\n",
    "#     output = layers.Conv2D(num_classes, (1, 1), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "#     model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "#     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "#     return model\n",
    "\n",
    "# model = segnet()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, Model\n",
    "# from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# def SegNet(input_shape=(32, 32, 3), num_classes=21):\n",
    "#     # Load the VGG16 model with batch normalization\n",
    "#     vgg16_bn = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "#     # Encoder\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "#     # Adjust the input layer if needed\n",
    "#     if input_shape[-1] != 3:\n",
    "#         x = layers.Conv2D(64, (3, 3), padding='same', name='custom_input_conv')(inputs)\n",
    "#     else:\n",
    "#         x = inputs\n",
    "\n",
    "#     # Using the pretrained VGG16_bn layers\n",
    "#     x = vgg16_bn.get_layer('block1_conv1')(x)\n",
    "#     x = vgg16_bn.get_layer('block1_conv2')(x)\n",
    "#     stage1 = x\n",
    "#     x = vgg16_bn.get_layer('block1_pool')(x)\n",
    "\n",
    "#     x = vgg16_bn.get_layer('block2_conv1')(x)\n",
    "#     x = vgg16_bn.get_layer('block2_conv2')(x)\n",
    "#     stage2 = x\n",
    "#     x = vgg16_bn.get_layer('block2_pool')(x)\n",
    "\n",
    "#     x = vgg16_bn.get_layer('block3_conv1')(x)\n",
    "#     x = vgg16_bn.get_layer('block3_conv2')(x)\n",
    "#     x = vgg16_bn.get_layer('block3_conv3')(x)\n",
    "#     stage3 = x\n",
    "#     x = vgg16_bn.get_layer('block3_pool')(x)\n",
    "\n",
    "#     x = vgg16_bn.get_layer('block4_conv1')(x)\n",
    "#     x = vgg16_bn.get_layer('block4_conv2')(x)\n",
    "#     x = vgg16_bn.get_layer('block4_conv3')(x)\n",
    "#     stage4 = x\n",
    "#     x = vgg16_bn.get_layer('block4_pool')(x)\n",
    "\n",
    "#     x = vgg16_bn.get_layer('block5_conv1')(x)\n",
    "#     x = vgg16_bn.get_layer('block5_conv2')(x)\n",
    "#     x = vgg16_bn.get_layer('block5_conv3')(x)\n",
    "#     stage5 = x\n",
    "#     x = vgg16_bn.get_layer('block5_pool')(x)\n",
    "\n",
    "#     # Flatten and add dense layer\n",
    "#     x = layers.Flatten()(x)\n",
    "#     x = layers.Dense(1024, activation='relu')(x)\n",
    "#     x = layers.Dense(1*1*512, activation='relu')(x)  # Adjust to match the new dimensions\n",
    "#     x = layers.Reshape((1, 1, 512))(x)  # Adjust to match the new dimensions\n",
    "\n",
    "#     # Decoder\n",
    "#     x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.UpSampling2D()(x)\n",
    "\n",
    "#     x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.UpSampling2D()(x)\n",
    "\n",
    "#     x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(128, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.UpSampling2D()(x)\n",
    "\n",
    "#     x = layers.Conv2DTranspose(128, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.UpSampling2D()(x)\n",
    "\n",
    "#     x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(num_classes, (3, 3), strides=2, padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "    \n",
    "#     outputs = layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "#     return Model(inputs, outputs)\n",
    "\n",
    "# # Instantiate and compile the model\n",
    "# model = SegNet(input_shape=(32, 32, 3), num_classes=2)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# # Summary of the model\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, Model\n",
    "# from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# def SegNet(input_shape=(32, 32, 3), num_classes=21):\n",
    "#     # Load the VGG16 model with batch normalization\n",
    "#     vgg16_bn = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "#     vgg16_bn.trainable = True\n",
    "    \n",
    "#     # Encoder\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     x = layers.Lambda(lambda x: x/255.)(inputs)\n",
    "    \n",
    "#     # Adjust the input layer if needed\n",
    "#     if input_shape[-1] != 3:\n",
    "#         x = layers.Conv2D(64, (3, 3), padding='same', name='custom_input_conv')(x)\n",
    "#     else:\n",
    "#         x = x\n",
    "\n",
    "#     # Using the pretrained VGG16_bn layers\n",
    "#     x = vgg16_bn.get_layer('block1_conv1')(x)\n",
    "#     x = vgg16_bn.get_layer('block1_conv2')(x)\n",
    "#     stage1 = x\n",
    "#     x = vgg16_bn.get_layer('block1_pool')(x)\n",
    "\n",
    "#     x = vgg16_bn.get_layer('block2_conv1')(x)\n",
    "#     x = vgg16_bn.get_layer('block2_conv2')(x)\n",
    "#     stage2 = x\n",
    "#     x = vgg16_bn.get_layer('block2_pool')(x)\n",
    "\n",
    "#     x = vgg16_bn.get_layer('block3_conv1')(x)\n",
    "#     x = vgg16_bn.get_layer('block3_conv2')(x)\n",
    "#     x = vgg16_bn.get_layer('block3_conv3')(x)\n",
    "#     stage3 = x\n",
    "#     x = vgg16_bn.get_layer('block3_pool')(x)\n",
    "\n",
    "#     x = vgg16_bn.get_layer('block4_conv1')(x)\n",
    "#     x = vgg16_bn.get_layer('block4_conv2')(x)\n",
    "#     x = vgg16_bn.get_layer('block4_conv3')(x)\n",
    "#     stage4 = x\n",
    "#     x = vgg16_bn.get_layer('block4_pool')(x)\n",
    "\n",
    "#     x = vgg16_bn.get_layer('block5_conv1')(x)\n",
    "#     x = vgg16_bn.get_layer('block5_conv2')(x)\n",
    "#     x = vgg16_bn.get_layer('block5_conv3')(x)\n",
    "#     stage5 = x\n",
    "#     x = vgg16_bn.get_layer('block5_pool')(x)\n",
    "\n",
    "#     # Flatten and add dense layer\n",
    "#     x = layers.Flatten()(x)\n",
    "#     x = layers.Dense(2048, activation='relu')(x)\n",
    "#     x = layers.Dense(1*1*512, activation='relu')(x)  # Adjust to match the new dimensions\n",
    "#     x = layers.Reshape((1, 1, 512))(x)  # Adjust to match the new dimensions\n",
    "\n",
    "#     # Decoder\n",
    "#     x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.UpSampling2D()(x)\n",
    "\n",
    "#     # Incorporate stage5\n",
    "#     x = layers.Concatenate()([x, stage5])\n",
    "#     x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.UpSampling2D()(x)\n",
    "\n",
    "#     # Incorporate stage4\n",
    "#     x = layers.Concatenate()([x, stage4])\n",
    "#     x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(128, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.UpSampling2D()(x)\n",
    "\n",
    "#     # Incorporate stage3\n",
    "#     x = layers.Concatenate()([x, stage3])\n",
    "#     x = layers.Conv2DTranspose(128, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.UpSampling2D()(x)\n",
    "\n",
    "#     # Incorporate stage2\n",
    "#     x = layers.Concatenate()([x, stage2])\n",
    "#     x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.UpSampling2D()(x)\n",
    "\n",
    "#     # Incorporate stage1\n",
    "#     x = layers.Concatenate()([x, stage1])\n",
    "#     x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2DTranspose(num_classes, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "    \n",
    "#     x = layers.Lambda(lambda args: apply_mask(*args))([x, inputs])\n",
    "    \n",
    "#     x = layers.Flatten()(x)\n",
    "    \n",
    "#     outputs = layers.Dense(2, activation='linear')(x)\n",
    "    \n",
    "#     # outputs = layers.Conv2D(num_classes, (1, 1), activation='linear')(x)\n",
    "\n",
    "#     return Model(inputs, outputs)\n",
    "\n",
    "# # Instantiate and compile the model\n",
    "# model = SegNet(input_shape=(32, 32, 3), num_classes=2)\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='mse', metrics=['mae'])\n",
    "\n",
    "# # Summary of the model\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_generator, validation_data=val_generator, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hpsB.values['filters']=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SegNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 32, 32, 64)   1792        ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 32, 32, 64)   36928       ['block1_conv1[1][0]']           \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)     (None, 16, 16, 64)   0           ['block1_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 16, 16, 128)  73856       ['block1_pool[1][0]']            \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 16, 16, 128)  147584      ['block2_conv1[1][0]']           \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)     (None, 8, 8, 128)    0           ['block2_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 8, 8, 256)    295168      ['block2_pool[1][0]']            \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 8, 8, 256)    590080      ['block3_conv1[1][0]']           \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)          (None, 8, 8, 256)    590080      ['block3_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)     (None, 4, 4, 256)    0           ['block3_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)          (None, 4, 4, 512)    1180160     ['block3_pool[1][0]']            \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)          (None, 4, 4, 512)    2359808     ['block4_conv1[1][0]']           \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)          (None, 4, 4, 512)    2359808     ['block4_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)     (None, 2, 2, 512)    0           ['block4_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)          (None, 2, 2, 512)    2359808     ['block4_pool[1][0]']            \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)          (None, 2, 2, 512)    2359808     ['block5_conv1[1][0]']           \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)          (None, 2, 2, 512)    2359808     ['block5_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " block5_pool (MaxPooling2D)     (None, 1, 1, 512)    0           ['block5_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 512)          0           ['block5_pool[1][0]']            \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 1024)         525312      ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 1024)         0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 1024)         1049600     ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 1024)         0           ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 1024)         1049600     ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 1024)         0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 1024)         1049600     ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 512)          524800      ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 1, 1, 512)    0           ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose_14 (Conv2DTra  (None, 1, 1, 512)   2359808     ['reshape_6[0][0]']              \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_transpose_14[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                (None, 1, 1, 512)    0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_15 (Conv2DTra  (None, 1, 1, 512)   2359808     ['re_lu_14[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_transpose_15[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)                (None, 1, 1, 512)    0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_16 (Conv2DTra  (None, 1, 1, 512)   2359808     ['re_lu_15[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_transpose_16[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                (None, 1, 1, 512)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_36 (UpSampling2D  (None, 2, 2, 512)   0           ['re_lu_16[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 2, 2, 1024)   0           ['up_sampling2d_36[0][0]',       \n",
      "                                                                  'block5_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_17 (Conv2DTra  (None, 2, 2, 512)   4719104     ['concatenate_32[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 2, 2, 512)   2048        ['conv2d_transpose_17[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)                (None, 2, 2, 512)    0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_18 (Conv2DTra  (None, 2, 2, 512)   2359808     ['re_lu_17[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 2, 2, 512)   2048        ['conv2d_transpose_18[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)                (None, 2, 2, 512)    0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_19 (Conv2DTra  (None, 2, 2, 256)   1179904     ['re_lu_18[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_transpose_19[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)                (None, 2, 2, 256)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_37 (UpSampling2D  (None, 4, 4, 256)   0           ['re_lu_19[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 4, 4, 768)    0           ['up_sampling2d_37[0][0]',       \n",
      "                                                                  'block4_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_20 (Conv2DTra  (None, 4, 4, 256)   1769728     ['concatenate_33[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 4, 4, 256)   1024        ['conv2d_transpose_20[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)                (None, 4, 4, 256)    0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_21 (Conv2DTra  (None, 4, 4, 256)   590080      ['re_lu_20[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 4, 4, 256)   1024        ['conv2d_transpose_21[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)                (None, 4, 4, 256)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_22 (Conv2DTra  (None, 4, 4, 128)   295040      ['re_lu_21[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 4, 4, 128)   512         ['conv2d_transpose_22[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)                (None, 4, 4, 128)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_38 (UpSampling2D  (None, 8, 8, 128)   0           ['re_lu_22[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenate)   (None, 8, 8, 384)    0           ['up_sampling2d_38[0][0]',       \n",
      "                                                                  'block3_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_23 (Conv2DTra  (None, 8, 8, 128)   442496      ['concatenate_34[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 128)   512         ['conv2d_transpose_23[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)                (None, 8, 8, 128)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_24 (Conv2DTra  (None, 8, 8, 64)    73792       ['re_lu_23[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 64)    256         ['conv2d_transpose_24[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_39 (UpSampling2D  (None, 16, 16, 64)  0           ['re_lu_24[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 16, 16, 192)  0           ['up_sampling2d_39[0][0]',       \n",
      "                                                                  'block2_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_25 (Conv2DTra  (None, 16, 16, 64)  110656      ['concatenate_35[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 16, 16, 64)  256         ['conv2d_transpose_25[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)                (None, 16, 16, 64)   0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_40 (UpSampling2D  (None, 32, 32, 64)  0           ['re_lu_25[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_36 (Concatenate)   (None, 32, 32, 128)  0           ['up_sampling2d_40[0][0]',       \n",
      "                                                                  'block1_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_26 (Conv2DTra  (None, 32, 32, 64)  73792       ['concatenate_36[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 32, 32, 64)  256         ['conv2d_transpose_26[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 32, 32, 64)   0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_27 (Conv2DTra  (None, 32, 32, 2)   1154        ['re_lu_26[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 32, 32, 2)   8           ['conv2d_transpose_27[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)                (None, 32, 32, 2)    0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 32, 32, 2)    6           ['re_lu_27[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 37,623,696\n",
      "Trainable params: 37,616,140\n",
      "Non-trainable params: 7,556\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the best hp.\n",
    "modelB = segnet(best_hpsB)\n",
    "modelB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n",
      "40/40 - 7s - loss: 0.0940 - mae: 0.1952 - val_loss: 22.4165 - val_mae: 4.1575 - lr: 1.0000e-04 - 7s/epoch - 177ms/step\n",
      "Epoch 2/1024\n",
      "40/40 - 3s - loss: 0.0592 - mae: 0.1469 - val_loss: 18.9749 - val_mae: 3.7314 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 3/1024\n",
      "40/40 - 3s - loss: 0.0434 - mae: 0.1127 - val_loss: 7.4914 - val_mae: 2.3246 - lr: 1.0000e-04 - 3s/epoch - 78ms/step\n",
      "Epoch 4/1024\n",
      "40/40 - 3s - loss: 0.0399 - mae: 0.1070 - val_loss: 3.5627 - val_mae: 1.5889 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 5/1024\n",
      "40/40 - 3s - loss: 0.0377 - mae: 0.1060 - val_loss: 1.2094 - val_mae: 0.9139 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 6/1024\n",
      "40/40 - 3s - loss: 0.0363 - mae: 0.1059 - val_loss: 0.7565 - val_mae: 0.7082 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 7/1024\n",
      "40/40 - 3s - loss: 0.0351 - mae: 0.1061 - val_loss: 0.3676 - val_mae: 0.4951 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 8/1024\n",
      "40/40 - 3s - loss: 0.0337 - mae: 0.1065 - val_loss: 0.2264 - val_mae: 0.3708 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 9/1024\n",
      "40/40 - 3s - loss: 0.0326 - mae: 0.1073 - val_loss: 0.1572 - val_mae: 0.2886 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 10/1024\n",
      "40/40 - 3s - loss: 0.0319 - mae: 0.1079 - val_loss: 0.0975 - val_mae: 0.2259 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 11/1024\n",
      "40/40 - 3s - loss: 0.0313 - mae: 0.1086 - val_loss: 0.0774 - val_mae: 0.1858 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 12/1024\n",
      "40/40 - 3s - loss: 0.0307 - mae: 0.1091 - val_loss: 0.0709 - val_mae: 0.1835 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 13/1024\n",
      "40/40 - 3s - loss: 0.0301 - mae: 0.1100 - val_loss: 0.0529 - val_mae: 0.1497 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 14/1024\n",
      "40/40 - 3s - loss: 0.0296 - mae: 0.1100 - val_loss: 0.0438 - val_mae: 0.1440 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 15/1024\n",
      "40/40 - 3s - loss: 0.0294 - mae: 0.1109 - val_loss: 0.0437 - val_mae: 0.1392 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 16/1024\n",
      "40/40 - 3s - loss: 0.0291 - mae: 0.1110 - val_loss: 0.0433 - val_mae: 0.1297 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 17/1024\n",
      "40/40 - 3s - loss: 0.0288 - mae: 0.1109 - val_loss: 0.0355 - val_mae: 0.1253 - lr: 1.0000e-04 - 3s/epoch - 79ms/step\n",
      "Epoch 18/1024\n",
      "40/40 - 3s - loss: 0.0287 - mae: 0.1108 - val_loss: 0.0459 - val_mae: 0.1422 - lr: 1.0000e-04 - 3s/epoch - 76ms/step\n",
      "Epoch 19/1024\n",
      "40/40 - 3s - loss: 0.0285 - mae: 0.1105 - val_loss: 0.0371 - val_mae: 0.1296 - lr: 1.0000e-04 - 3s/epoch - 75ms/step\n",
      "Epoch 20/1024\n",
      "40/40 - 3s - loss: 0.0282 - mae: 0.1100 - val_loss: 0.0328 - val_mae: 0.1209 - lr: 1.0000e-04 - 3s/epoch - 77ms/step\n",
      "Epoch 21/1024\n",
      "40/40 - 3s - loss: 0.0277 - mae: 0.1089 - val_loss: 0.0366 - val_mae: 0.1186 - lr: 1.0000e-04 - 3s/epoch - 75ms/step\n",
      "Epoch 22/1024\n",
      "40/40 - 3s - loss: 0.0278 - mae: 0.1087 - val_loss: 0.0346 - val_mae: 0.1164 - lr: 1.0000e-04 - 3s/epoch - 75ms/step\n",
      "Epoch 23/1024\n",
      "40/40 - 3s - loss: 0.0276 - mae: 0.1081 - val_loss: 0.0345 - val_mae: 0.1273 - lr: 1.0000e-04 - 3s/epoch - 76ms/step\n",
      "Epoch 24/1024\n",
      "40/40 - 3s - loss: 0.0273 - mae: 0.1074 - val_loss: 0.0346 - val_mae: 0.1187 - lr: 1.0000e-04 - 3s/epoch - 75ms/step\n",
      "Epoch 25/1024\n",
      "40/40 - 3s - loss: 0.0273 - mae: 0.1068 - val_loss: 0.0332 - val_mae: 0.1188 - lr: 1.0000e-04 - 3s/epoch - 77ms/step\n",
      "Epoch 26/1024\n",
      "40/40 - 3s - loss: 0.0271 - mae: 0.1064 - val_loss: 0.0324 - val_mae: 0.1148 - lr: 1.0000e-05 - 3s/epoch - 76ms/step\n",
      "Epoch 27/1024\n",
      "40/40 - 3s - loss: 0.0268 - mae: 0.1057 - val_loss: 0.0303 - val_mae: 0.1108 - lr: 1.0000e-05 - 3s/epoch - 76ms/step\n",
      "Epoch 28/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1049 - val_loss: 0.0288 - val_mae: 0.1084 - lr: 1.0000e-05 - 3s/epoch - 75ms/step\n",
      "Epoch 29/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1051 - val_loss: 0.0288 - val_mae: 0.1078 - lr: 1.0000e-05 - 3s/epoch - 74ms/step\n",
      "Epoch 30/1024\n",
      "40/40 - 3s - loss: 0.0268 - mae: 0.1054 - val_loss: 0.0275 - val_mae: 0.1072 - lr: 1.0000e-05 - 3s/epoch - 74ms/step\n",
      "Epoch 31/1024\n",
      "40/40 - 3s - loss: 0.0267 - mae: 0.1053 - val_loss: 0.0276 - val_mae: 0.1073 - lr: 1.0000e-05 - 3s/epoch - 74ms/step\n",
      "Epoch 32/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1049 - val_loss: 0.0273 - val_mae: 0.1065 - lr: 1.0000e-05 - 3s/epoch - 77ms/step\n",
      "Epoch 33/1024\n",
      "40/40 - 3s - loss: 0.0266 - mae: 0.1050 - val_loss: 0.0276 - val_mae: 0.1071 - lr: 1.0000e-05 - 3s/epoch - 76ms/step\n",
      "Epoch 34/1024\n",
      "40/40 - 3s - loss: 0.0266 - mae: 0.1051 - val_loss: 0.0276 - val_mae: 0.1067 - lr: 1.0000e-05 - 3s/epoch - 75ms/step\n",
      "Epoch 35/1024\n",
      "40/40 - 3s - loss: 0.0268 - mae: 0.1053 - val_loss: 0.0274 - val_mae: 0.1065 - lr: 1.0000e-05 - 3s/epoch - 75ms/step\n",
      "Epoch 36/1024\n",
      "40/40 - 3s - loss: 0.0267 - mae: 0.1054 - val_loss: 0.0276 - val_mae: 0.1068 - lr: 1.0000e-05 - 3s/epoch - 75ms/step\n",
      "Epoch 37/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1047 - val_loss: 0.0280 - val_mae: 0.1075 - lr: 1.0000e-05 - 3s/epoch - 75ms/step\n",
      "Epoch 38/1024\n",
      "40/40 - 3s - loss: 0.0266 - mae: 0.1045 - val_loss: 0.0275 - val_mae: 0.1068 - lr: 1.0000e-06 - 3s/epoch - 75ms/step\n",
      "Epoch 39/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1047 - val_loss: 0.0274 - val_mae: 0.1066 - lr: 1.0000e-06 - 3s/epoch - 75ms/step\n",
      "Epoch 40/1024\n",
      "40/40 - 3s - loss: 0.0263 - mae: 0.1043 - val_loss: 0.0275 - val_mae: 0.1067 - lr: 1.0000e-06 - 3s/epoch - 75ms/step\n",
      "Epoch 41/1024\n",
      "40/40 - 3s - loss: 0.0262 - mae: 0.1043 - val_loss: 0.0274 - val_mae: 0.1066 - lr: 1.0000e-06 - 3s/epoch - 75ms/step\n",
      "Epoch 42/1024\n",
      "40/40 - 3s - loss: 0.0267 - mae: 0.1050 - val_loss: 0.0274 - val_mae: 0.1066 - lr: 1.0000e-06 - 3s/epoch - 75ms/step\n",
      "Epoch 43/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1045 - val_loss: 0.0273 - val_mae: 0.1066 - lr: 1.0000e-07 - 3s/epoch - 76ms/step\n",
      "Epoch 44/1024\n",
      "40/40 - 3s - loss: 0.0268 - mae: 0.1052 - val_loss: 0.0273 - val_mae: 0.1066 - lr: 1.0000e-07 - 3s/epoch - 75ms/step\n",
      "Epoch 45/1024\n",
      "40/40 - 3s - loss: 0.0261 - mae: 0.1040 - val_loss: 0.0273 - val_mae: 0.1066 - lr: 1.0000e-07 - 3s/epoch - 75ms/step\n",
      "Epoch 46/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1044 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-07 - 3s/epoch - 77ms/step\n",
      "Epoch 47/1024\n",
      "40/40 - 3s - loss: 0.0269 - mae: 0.1053 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-07 - 3s/epoch - 77ms/step\n",
      "Epoch 48/1024\n",
      "40/40 - 3s - loss: 0.0263 - mae: 0.1042 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-08 - 3s/epoch - 77ms/step\n",
      "Epoch 49/1024\n",
      "40/40 - 3s - loss: 0.0266 - mae: 0.1046 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-08 - 3s/epoch - 75ms/step\n",
      "Epoch 50/1024\n",
      "40/40 - 3s - loss: 0.0266 - mae: 0.1048 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-08 - 3s/epoch - 77ms/step\n",
      "Epoch 51/1024\n",
      "40/40 - 3s - loss: 0.0265 - mae: 0.1046 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-08 - 3s/epoch - 77ms/step\n",
      "Epoch 52/1024\n",
      "40/40 - 3s - loss: 0.0261 - mae: 0.1040 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-08 - 3s/epoch - 77ms/step\n",
      "Epoch 53/1024\n",
      "40/40 - 3s - loss: 0.0261 - mae: 0.1042 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 54/1024\n",
      "40/40 - 3s - loss: 0.0265 - mae: 0.1046 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 77ms/step\n",
      "Epoch 55/1024\n",
      "40/40 - 3s - loss: 0.0266 - mae: 0.1048 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 56/1024\n",
      "40/40 - 3s - loss: 0.0269 - mae: 0.1053 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 76ms/step\n",
      "Epoch 57/1024\n",
      "40/40 - 3s - loss: 0.0262 - mae: 0.1045 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 76ms/step\n",
      "Epoch 58/1024\n",
      "40/40 - 3s - loss: 0.0267 - mae: 0.1050 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 78ms/step\n",
      "Epoch 59/1024\n",
      "40/40 - 3s - loss: 0.0262 - mae: 0.1043 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 77ms/step\n",
      "Epoch 60/1024\n",
      "40/40 - 3s - loss: 0.0260 - mae: 0.1040 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 61/1024\n",
      "40/40 - 3s - loss: 0.0266 - mae: 0.1048 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 62/1024\n",
      "40/40 - 3s - loss: 0.0263 - mae: 0.1044 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 63/1024\n",
      "40/40 - 3s - loss: 0.0265 - mae: 0.1045 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 77ms/step\n",
      "Epoch 64/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1045 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 78ms/step\n",
      "Epoch 65/1024\n",
      "40/40 - 3s - loss: 0.0267 - mae: 0.1049 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 66/1024\n",
      "40/40 - 3s - loss: 0.0265 - mae: 0.1048 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 76ms/step\n",
      "Epoch 67/1024\n",
      "40/40 - 3s - loss: 0.0265 - mae: 0.1048 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 68/1024\n",
      "40/40 - 3s - loss: 0.0262 - mae: 0.1043 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 69/1024\n",
      "40/40 - 3s - loss: 0.0265 - mae: 0.1048 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 70/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1044 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 71/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1043 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 72/1024\n",
      "40/40 - 3s - loss: 0.0265 - mae: 0.1048 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Epoch 73/1024\n",
      "40/40 - 3s - loss: 0.0266 - mae: 0.1049 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 74ms/step\n",
      "Epoch 74/1024\n",
      "40/40 - 3s - loss: 0.0266 - mae: 0.1047 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 74ms/step\n",
      "Epoch 75/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1044 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 74ms/step\n",
      "Epoch 76/1024\n",
      "40/40 - 3s - loss: 0.0265 - mae: 0.1046 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 74ms/step\n",
      "Epoch 77/1024\n",
      "40/40 - 3s - loss: 0.0263 - mae: 0.1045 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 74ms/step\n",
      "Epoch 78/1024\n",
      "40/40 - 3s - loss: 0.0264 - mae: 0.1047 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 74ms/step\n",
      "Epoch 79/1024\n",
      "40/40 - 3s - loss: 0.0265 - mae: 0.1047 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 74ms/step\n",
      "Epoch 80/1024\n",
      "40/40 - 3s - loss: 0.0263 - mae: 0.1045 - val_loss: 0.0273 - val_mae: 0.1067 - lr: 1.0000e-09 - 3s/epoch - 75ms/step\n",
      "Training time: 248.68642616271973s\n"
     ]
    }
   ],
   "source": [
    "## DESIGN B\n",
    "import time \n",
    "start = time.time()\n",
    "historyB = modelB.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [ES, REDUCE_LR])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[298], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Prepare set of x values and y values\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m modelB_train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_X_values_2_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_values_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_values_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoisture: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m, pH: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(modelB_train_accuracy[\u001b[38;5;241m0\u001b[39m], modelB_train_accuracy[\u001b[38;5;241m1\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[279], line 20\u001b[0m, in \u001b[0;36mget_model_accuracy\u001b[1;34m(model, x_values, y_values, copy)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_accuracy\u001b[39m(model, x_values, y_values, copy\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m---> 20\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     samples \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m copy:\n",
      "File \u001b[1;32mc:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\keras\\engine\\data_adapter.py:935\u001b[0m, in \u001b[0;36mGeneratorDataAdapter._peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_peek_and_restore\u001b[39m(x):\n\u001b[1;32m--> 935\u001b[0m     peek \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m peek, itertools\u001b[38;5;241m.\u001b[39mchain([peek], x)\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Prepare set of x values and y values\n",
    "\n",
    "modelB_train_accuracy = get_model_accuracy(modelB, gen_X_values_2_1, y_values_1, X_values_1)\n",
    "print(\"Train Accuracy\")\n",
    "print(\"Moisture: {:.2f}, pH: {:.2f}\".format(modelB_train_accuracy[0], modelB_train_accuracy[1]))\n",
    "\n",
    "modelB_test_accuracy = get_model_accuracy(modelB, gen_X_values_2_2, y_values_2, X_values_2)\n",
    "print(\"Test Accuracy\")\n",
    "print(\"Moisture: {:.2f}, pH: {:.2f}\".format(modelB_test_accuracy[0], modelB_test_accuracy[1]))\n",
    "\n",
    "\n",
    "predictions = modelB.predict(np.array([SAMPLE[0][0]]))\n",
    "truth = unprocess_label_wmask(SAMPLE[0][0],SAMPLE[1][0])\n",
    "pred = unprocess_label_wmask(SAMPLE[0][0],predictions[0])\n",
    "print(\"TRUTH moisture: {:.2f}, ph: {:.2f}\".format(truth[0],truth[1]))\n",
    "print(\"PREDICTIONS moisture: {:.2f}, ph: {:.2f}\".format(pred[0],pred[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 512\n",
    "# steps_per_epoch = 512 // batch_size + 1  # we usually consider 1 epoch to be\n",
    "#                                             # the point where the model has seen\n",
    "#                                             # all the training samples at least once\n",
    "# min_lr = 1e-8\n",
    "# factor = 0.1\n",
    "# SCHEDULE_EPOCH = 200\n",
    "# STEPS = 200\n",
    "\n",
    "\n",
    "# historyB = {\"history\":{\"loss\":[],\"mae\":[],\"val_loss\":[],\"val_mae\":[]}}\n",
    "# for e in range(epochs):\n",
    "#     for i, (images, y_batch) in enumerate(train_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#        loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#     #    val = modelB.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyB[\"history\"][\"loss\"].append(loss[0])\n",
    "#             historyB[\"history\"][\"mae\"].append(loss[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             break  \n",
    "#     for i, (images, y_batch) in enumerate(val_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#     #    loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#        val = modelB.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyB[\"history\"][\"val_loss\"].append(val[0])\n",
    "#             historyB[\"history\"][\"val_mae\"].append(val[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             curr_lr = modelB.optimizer.learning_rate\n",
    "#             if e>=SCHEDULE_EPOCH and curr_lr>min_lr: \n",
    "#                K.set_value(modelB.optimizer.learning_rate, curr_lr*factor)\n",
    "#                curr_lr = modelB.optimizer.learning_rate\n",
    "#                SCHEDULE_EPOCH+=STEPS\n",
    "#             # patience = 2\n",
    "#             # if e>patience and curr_lr>min_lr:\n",
    "#                # curr_loss = historyA[\"history\"][\"val_loss\"][-1]\n",
    "#                # for i in historyA[\"history\"][\"val_loss\"][-patience-1:-1]:\n",
    "#                #    if curr_loss >= i: patience-=1\n",
    "#                #    if patience < 1:\n",
    "#                      # K.set_value(modelA.optimizer.learning_rate, curr_lr*factor)\n",
    "#                      # curr_lr = modelA.optimizer.learning_rate\n",
    "#             break  \n",
    "#     print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}   CURR_LR {:.2E}\".format(e+1, loss[0], loss[1],val[0],val[1], curr_lr.numpy()))\n",
    "#     train_generator.on_epoch_end()  # this shuffles the data at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyB[\"history\"][\"loss\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyB[\"history\"][\"val_loss\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyB[\"history\"][\"mae\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyB[\"history\"][\"val_mae\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.suptitle(\"Model B: SegNet\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(historyB[\"history\"][\"loss\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1,\n",
    "#                               patience=10, min_lr=1e-7)\n",
    "# # stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=10)\n",
    "# history = modelB.fit(train_generator, epochs=2,# validation_data=val_generator,\n",
    "#                      callbacks=[red_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n",
    "# result = modelB(sample[0])\n",
    "# for x,i in enumerate(result):\n",
    "#     RGB  = cv2.cvtColor(sample[0][x],cv2.COLOR_HSV2RGB)\n",
    "#     MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "#     ARRAY = np.array(result[5]).reshape(image_size) * MASK\n",
    "#     OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "#     print(np.average(OUTPUT), np.average(sample[1][x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.imshow(result[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(sample[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# RGB  = cv2.cvtColor(sample[0][5],cv2.COLOR_HSV2RGB)\n",
    "# MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "# ARRAY = np.array(result[5]).reshape(image_size) * MASK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVE = np.average(ARRAY)\n",
    "# TRUTH = sample[1][5]\n",
    "# print(AVE)\n",
    "# print(TRUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(ARRAY)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelA.save('design_models/BEST/designA_V3.h5')\n",
    "# modelB.save('design_models/BEST/designB_V3.h5')\n",
    "# modelC.save('design_models/BEST/designC_V3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shutil import rmtree\n",
    "# # removing directory \n",
    "# rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelB.save('design_models/designB_v6.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Conv2DTranspose, Dropout, Flatten, Dense, Reshape, AveragePooling2D, Concatenate\n",
    "# from tensorflow.keras import layers, initializers\n",
    "# import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "\n",
    "# def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n",
    "#     x = Conv2D(\n",
    "#         num_filters,\n",
    "#         kernel_size=kernel_size,\n",
    "#         dilation_rate=dilation_rate,\n",
    "#         padding=padding,\n",
    "#         use_bias=use_bias,\n",
    "#         kernel_initializer=initializers.HeNormal(),\n",
    "#     )(block_input)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     return Activation('relu')(x)\n",
    "\n",
    "# def DilatedSpatialPyramidPooling(dspp_input, num_filters):\n",
    "#     dims = dspp_input.shape\n",
    "#     x = AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "#     x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "#     out_pool = UpSampling2D(\n",
    "#         size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "#     )(x)\n",
    "\n",
    "#     out_1 = convolution_block(dspp_input, num_filters=num_filters, kernel_size=1, dilation_rate=1)\n",
    "#     out_6 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=6)\n",
    "#     out_12 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=12)\n",
    "#     out_18 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=18)\n",
    "\n",
    "#     x = Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "#     output = convolution_block(x, kernel_size=1)\n",
    "#     return output\n",
    "\n",
    "# def DeeplabV3Plus(hp, image_size=(64, 64)):\n",
    "#     model_input = Input(shape=image_size+(3,))\n",
    "#     resnet50 = tf.keras.applications.ResNet50(\n",
    "#         weights='imagenet', include_top=False, input_tensor=model_input\n",
    "#     )\n",
    "#     for layer in resnet50.layers:\n",
    "#         layer.trainable=True\n",
    "    \n",
    "#     hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "#     x = resnet50.get_layer(\"conv1_relu\").output\n",
    "#     x = DilatedSpatialPyramidPooling(x, hp_filters*2)\n",
    "#     print(x.shape)\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dense(hp_filters*hp_filters, activation='relu')(x)\n",
    "\n",
    "#     UNITS = hp_filters*hp_filters\n",
    "#     hp.Boolean(\"dropouts\", default=False)\n",
    "#     hp.Boolean(\"batch_normalization\", default=False)\n",
    "\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         x = Dropout(0.5)(x)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         x = Dropout(0.5)(x)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     if hp.Boolean(\"4th_dense\", default=False):\n",
    "#         x = Dense(UNITS, activation='relu')(x)\n",
    "#         if hp.Boolean(\"dropouts\"):\n",
    "#             x = Dropout(0.5)(x)\n",
    "#         if hp.Boolean(\"batch_normalization\"):\n",
    "#             x = BatchNormalization()(x)\n",
    "\n",
    "#     SIZE = image_size[0]\n",
    "#     temp = 0\n",
    "#     if hp_filters == 16:\n",
    "#         temp = hp_filters\n",
    "#     if hp_filters == 32:\n",
    "#         temp = hp_filters * 2\n",
    "#     if hp_filters == 64:\n",
    "#         temp = hp_filters * 4\n",
    "#     x = Reshape((SIZE // 16, SIZE // 16, temp))(x)\n",
    "\n",
    "#     x = UpSampling2D(size=(SIZE // 4 // x.shape[1], SIZE // 4 // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "#     input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "#     input_b = convolution_block(input_b, num_filters=hp_filters, kernel_size=1)\n",
    "\n",
    "#     x = Concatenate(axis=-1)([x, input_b])\n",
    "#     x = convolution_block(x, num_filters=hp_filters*4)\n",
    "#     x = convolution_block(x, num_filters=hp_filters*4)\n",
    "#     x = UpSampling2D(size=(SIZE // x.shape[1], SIZE // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "    \n",
    "#     outputs = Conv2D(2, 1, activation='sigmoid')(x)\n",
    "    \n",
    "#     model = Model(inputs=model_input, outputs=outputs)\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=[\"binary_crossentropy\"], metrics=['acc'])\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DeeplabV3Plus(hp, image_size=(32, 32)):\n",
    "#     model_input = Input(shape=image_size + (3,))\n",
    "#     resnet50 = tf.keras.applications.ResNet50(\n",
    "#         weights='imagenet', include_top=False, input_tensor=model_input\n",
    "#     )\n",
    "#     for layer in resnet50.layers:\n",
    "#         layer.trainable = True\n",
    "    \n",
    "#     hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "#     x = resnet50.get_layer(\"conv1_relu\").output\n",
    "#     x = DilatedSpatialPyramidPooling(x, hp_filters * 2)\n",
    "#     print(x.shape)\n",
    "#     x = Flatten()(x)\n",
    "    \n",
    "#     UNITS = hp.Choice('units', values=[256, 512, 1024, 2048], default=256)\n",
    "#     DROPOUT = hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1, default=0.0)\n",
    "#     BATCHNORM = hp.Boolean('batchnorm', default=False)\n",
    "\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "\n",
    "#     SIZE = image_size[0]\n",
    "#     temp = 0\n",
    "#     if hp_filters == 16:\n",
    "#         temp = hp_filters\n",
    "#     if hp_filters == 32:\n",
    "#         temp = hp_filters * 2\n",
    "#     if hp_filters == 64:\n",
    "#         temp = hp_filters * 4\n",
    "#     x = Reshape((SIZE // 16, SIZE // 16, temp))(x)\n",
    "\n",
    "#     x = UpSampling2D(size=(SIZE // 4 // x.shape[1], SIZE // 4 // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "#     input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "#     input_b = convolution_block(input_b, num_filters=hp_filters, kernel_size=1)\n",
    "\n",
    "#     x = Concatenate(axis=-1)([x, input_b])\n",
    "#     x = convolution_block(x, num_filters=hp_filters * 4)\n",
    "#     x = convolution_block(x, num_filters=hp_filters * 4)\n",
    "#     x = UpSampling2D(size=(SIZE // x.shape[1], SIZE // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "    \n",
    "#     outputs = Conv2D(2, 1, activation='sigmoid')(x)\n",
    "    \n",
    "#     model = Model(inputs=model_input, outputs=outputs)\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=[\"binary_crossentropy\"], metrics=['acc'])\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DeeplabV3Plus(hp,image_size=(64, 64)):\n",
    "#     model_input = Input(shape=image_size + (3,))\n",
    "#     resnet50 = tf.keras.applications.ResNet50(\n",
    "#         weights='imagenet', include_top=False, input_tensor=model_input\n",
    "#     )\n",
    "#     for layer in resnet50.layers:\n",
    "#         layer.trainable = True\n",
    "#     hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "    \n",
    "#     x = resnet50.get_layer(\"conv1_relu\").output\n",
    "#     x = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "#     x = DilatedSpatialPyramidPooling(x, hp_filters * 2)\n",
    "#     print(x.shape)\n",
    "#     x = Flatten()(x)\n",
    "    \n",
    "#     UNITS = hp.Choice('units', values=[256, 512, 1024, 2048], default=256)\n",
    "#     DROPOUT = hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1, default=0.0)\n",
    "#     BATCHNORM = hp.Boolean('batchnorm', default=False)\n",
    "\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "\n",
    "#     if BATCHNORM:\n",
    "#         x = BatchNormalization()(x)\n",
    "\n",
    "#     SIZE = image_size[0]//4\n",
    "#     temp = 0\n",
    "#     if hp_filters == 16:\n",
    "#         temp = hp_filters\n",
    "#     if hp_filters == 32:\n",
    "#         temp = hp_filters * 2\n",
    "#     if hp_filters == 64:\n",
    "#         temp = hp_filters * 4\n",
    "#     x = Dense((SIZE // 8)* (SIZE // 8)* temp, activation='relu')(x)\n",
    "#     x = Reshape((SIZE // 8, SIZE // 8, temp))(x)\n",
    "\n",
    "#     x = UpSampling2D(size=(SIZE // 4 // x.shape[1], SIZE // 4 // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "#     input_b = resnet50.get_layer(\"conv4_block4_out\").output\n",
    "#     input_b = convolution_block(input_b, num_filters=hp_filters, kernel_size=1)\n",
    "\n",
    "#     x = Concatenate(axis=-1)([x, input_b])\n",
    "#     x = convolution_block(x, num_filters=hp_filters * 4)\n",
    "#     x = convolution_block(x, num_filters=hp_filters * 4)\n",
    "#     x = UpSampling2D(size=( 16, 16), interpolation=\"bilinear\")(x)\n",
    "    \n",
    "#     outputs = Conv2D(2, 1, activation='sigmoid')(x)\n",
    "    \n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model = Model(inputs=model_input, outputs=outputs)\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=[\"binary_crossentropy\"], metrics=['acc'])\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Example usage and model summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, applications\n",
    "\n",
    "# def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n",
    "#     x = layers.Conv2D(\n",
    "#         num_filters,\n",
    "#         kernel_size=kernel_size,\n",
    "#         dilation_rate=dilation_rate,\n",
    "#         padding=padding,\n",
    "#         use_bias=use_bias,\n",
    "#         kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "#     )(block_input)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     return tf.nn.relu(x)\n",
    "\n",
    "# def DilatedSpatialPyramidPooling(dspp_input):\n",
    "#     dims = dspp_input.shape\n",
    "#     x = layers.AveragePooling2D(pool_size=(dims[1], dims[2]))(dspp_input)\n",
    "#     x = convolution_block(x, num_filters=256, kernel_size=1, use_bias=True)\n",
    "#     out_pool = layers.UpSampling2D(\n",
    "#         size=(dims[1] // x.shape[1], dims[2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "#     )(x)\n",
    "\n",
    "#     out_1 = convolution_block(dspp_input, num_filters=256, kernel_size=1, dilation_rate=1)\n",
    "#     out_6 = convolution_block(dspp_input, num_filters=256, kernel_size=3, dilation_rate=6)\n",
    "#     out_12 = convolution_block(dspp_input, num_filters=256, kernel_size=3, dilation_rate=12)\n",
    "#     out_18 = convolution_block(dspp_input, num_filters=256, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "#     x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "#     output = convolution_block(x, num_filters=256, kernel_size=1)\n",
    "#     return output\n",
    "\n",
    "# def DeeplabV3Plus(hp, image_size=(64, 64, 3), num_classes=2):\n",
    "#     base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=image_size)\n",
    "#     base_model_output = base_model.get_layer(\"conv4_block6_2_relu\").output\n",
    "\n",
    "#     x = DilatedSpatialPyramidPooling(base_model_output)\n",
    "\n",
    "#     input_a = layers.UpSampling2D(size=(image_size[0] // 4 // x.shape[1], image_size[1] // 4 // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "#     input_b = base_model.get_layer(\"conv2_block3_2_relu\").output\n",
    "#     input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "#     x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "#     x = convolution_block(x, num_filters=256)\n",
    "#     x = convolution_block(x, num_filters=256)\n",
    "#     x = layers.UpSampling2D(size=(image_size[0] // x.shape[1], image_size[1] // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "#     x = layers.Conv2D(2, (1, 1), padding=\"same\")(x)\n",
    "#     outputs = tf.keras.layers.Activation('sigmoid')(x)\n",
    "\n",
    "#     model = tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=[\"binary_crossentropy\"], metrics=['acc'])\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "def DeeplabV3Plus(hp, input_shape=(32, 32, 3), num_classes=2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder: ResNet50\n",
    "    resnet50 = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    \n",
    "    # Extract layers\n",
    "    layer_names = [\n",
    "        'conv1_relu', 'conv2_block3_out', 'conv3_block4_out',\n",
    "        'conv4_block6_out', 'conv5_block3_out'\n",
    "    ]\n",
    "    layers_output = [resnet50.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    # Define the encoder model\n",
    "    encoder = Model(inputs=resnet50.input, outputs=layers_output)\n",
    "\n",
    "    # Feature extraction\n",
    "    stage1, stage2, stage3, stage4, stage5 = encoder(inputs)\n",
    "    \n",
    "    # Concatenate feature maps from stage5\n",
    "    stage5_concat = layers.Concatenate()(stage5) if isinstance(stage5, list) else stage5\n",
    "\n",
    "    UNITS = hp.Choice('units',values = [256,512,1024,2048], default=256)\n",
    "    DROPOUT = hp.Float('dropout',min_value=0.0, max_value=0.5, step=0.1, default=0.0)\n",
    "    BATCHNORM = hp.Boolean('batchnorm',default=False)\n",
    "    \n",
    "    # Dense layers between encoder and decoder\n",
    "    x = layers.Flatten()(stage5_concat)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    if BATCHNORM:\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "    # Flatten and add dense layer\n",
    "    x = layers.Dense(1 * 1 * 512, activation='relu')(x)\n",
    "    x = layers.Reshape((1, 1, 512))(x)\n",
    "    \n",
    "    # ASPP\n",
    "    def aspp_block(x, out_channels, kernel_size, dilation):\n",
    "        return layers.Conv2D(out_channels, kernel_size, padding='same', dilation_rate=dilation, activation='relu')(x)\n",
    "\n",
    "    pool = layers.GlobalAveragePooling2D()(x)\n",
    "    pool = layers.Reshape((1, 1, 512))(pool)\n",
    "    pool = layers.Conv2D(256, (1, 1), activation='relu')(pool)\n",
    "\n",
    "    # Use K.int_shape to get the static shape as a tuple of integers\n",
    "    shape_before = tf.keras.backend.int_shape(x)\n",
    "    pool = layers.UpSampling2D(size=(shape_before[1], shape_before[2]))(pool)\n",
    "\n",
    "    b1 = aspp_block(x, 256, 1, 1)\n",
    "    b2 = aspp_block(x, 256, 3, 6)\n",
    "    b3 = aspp_block(x, 256, 3, 12)\n",
    "    b4 = aspp_block(x, 256, 3, 18)\n",
    "    \n",
    "    x = layers.Concatenate()([b1, b2, b3, b4, pool])\n",
    "    x = layers.Conv2D(256, (1, 1), activation='relu')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.UpSampling2D(size=(8, 8))(x)\n",
    "    x = layers.Concatenate()([x, stage2])\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "\n",
    "    x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "    x = layers.Concatenate()([x, stage1])\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "\n",
    "    x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "    \n",
    "    outputs = Conv2D(2, (1, 1), padding=\"same\", activation=\"linear\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DeepLabV3Plus')\n",
    "\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['sgd', 'rmsprop', 'adam'], default='adam')\n",
    "    lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5], default=1e-3)\n",
    "\n",
    "    if optimizer_choice == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\design_c\\tuner0.json\n",
      "{'units': 256, 'dropout': 0.1, 'batchnorm': True, 'optimizer': 'adam', 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "tunerC = kt.BayesianOptimization(DeeplabV3Plus,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 30,\n",
    "                     project_name='design_c',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8)\n",
    "\n",
    "tunerC.search(train_generator, epochs=50, validation_data=val_generator, callbacks=[stop_early, reduce_lr])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsC=tunerC.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(best_hpsC.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mask_tensor(image):\n",
    "#     # image = image.astype(np.uint8)\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "#     binr = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "#     binr = np.invert(binr)\n",
    "\n",
    "#     kernel = np.ones((3, 3), np.uint8)\n",
    "#     mask = cv2.erode(binr, kernel, iterations=3)\n",
    "    \n",
    "#     return mask\n",
    "\n",
    "# def apply_mask(args):\n",
    "#     image = args[0]\n",
    "#     tensor = args[1]\n",
    "#     print(image)\n",
    "    \n",
    "#     # Convert the image to grayscale\n",
    "#     gray = tf.image.rgb_to_grayscale(image)\n",
    "    \n",
    "#     # Thresholding\n",
    "#     mask = tf.cast(tf.less(gray, 128), dtype=tf.float32)\n",
    "    \n",
    "#     # # Expand dimensions to match the tensor shape\n",
    "#     # mask = tf.expand_dims(mask, axis=-1)\n",
    "#     # print(mask)\n",
    "    \n",
    "#     # Apply the mask to each channel of the tensor\n",
    "#     tensor = tensor * mask\n",
    "    \n",
    "#     print(tensor)\n",
    "    \n",
    "#     return tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, Model\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# def DeeplabV3Plus(input_shape=(32, 32, 3), num_classes=21):\n",
    "#     inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "#     # Encoder: ResNet50\n",
    "#     resnet50 = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    \n",
    "#     # Extract layers\n",
    "#     layer_names = [\n",
    "#         'conv1_relu', 'conv2_block3_out', 'conv3_block4_out',\n",
    "#         'conv4_block6_out', 'conv5_block3_out'\n",
    "#     ]\n",
    "#     layers_output = [resnet50.get_layer(name).output for name in layer_names]\n",
    "\n",
    "#     # Define the encoder model\n",
    "#     encoder = Model(inputs=resnet50.input, outputs=layers_output)\n",
    "\n",
    "#     # Feature extraction\n",
    "#     stage1, stage2, stage3, stage4, stage5 = encoder(inputs)\n",
    "    \n",
    "#     # Concatenate feature maps from stage5\n",
    "#     stage5_concat = layers.Concatenate()(stage5) if isinstance(stage5, list) else stage5\n",
    "\n",
    "#     # Flatten and add dense layer\n",
    "#     x = layers.Flatten()(stage5_concat)\n",
    "#     x = layers.Dense(1024, activation='relu')(x)\n",
    "#     x = layers.Dense(1 * 1 * 512, activation='relu')(x)\n",
    "#     x = layers.Reshape((1, 1, 512))(x)\n",
    "    \n",
    "#     # ASPP\n",
    "#     def aspp_block(x, out_channels, kernel_size, dilation):\n",
    "#         return layers.Conv2D(out_channels, kernel_size, padding='same', dilation_rate=dilation, activation='relu')(x)\n",
    "\n",
    "#     pool = layers.GlobalAveragePooling2D()(x)\n",
    "#     pool = layers.Reshape((1, 1, 512))(pool)\n",
    "#     pool = layers.Conv2D(256, (1, 1), activation='relu')(pool)\n",
    "\n",
    "#     # Use K.int_shape to get the static shape as a tuple of integers\n",
    "#     shape_before = tf.keras.backend.int_shape(x)\n",
    "#     pool = layers.UpSampling2D(size=(shape_before[1], shape_before[2]))(pool)\n",
    "\n",
    "#     b1 = aspp_block(x, 256, 1, 1)\n",
    "#     b2 = aspp_block(x, 256, 3, 6)\n",
    "#     b3 = aspp_block(x, 256, 3, 12)\n",
    "#     b4 = aspp_block(x, 256, 3, 18)\n",
    "    \n",
    "#     x = layers.Concatenate()([b1, b2, b3, b4, pool])\n",
    "#     x = layers.Conv2D(256, (1, 1), activation='relu')(x)\n",
    "    \n",
    "#     # Decoder\n",
    "#     x = layers.UpSampling2D(size=(8, 8))(x)\n",
    "#     x = layers.Concatenate()([x, stage2])\n",
    "#     x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "#     x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "\n",
    "#     x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "#     x = layers.Concatenate()([x, stage1])\n",
    "#     x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "#     x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "\n",
    "#     x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "    \n",
    "#     x = layers.Conv2D(2, (1, 1), activation='linear')(x)\n",
    "#     # Apply post-processing\n",
    "#     # mask = tf.cast(tf.less(tf.image.rgb_to_grayscale(inputs), 128), dtype=tf.float32)\n",
    "#     # x = x * mask\n",
    "\n",
    "#     # x = layers.Flatten()(x)\n",
    "    \n",
    "#     # x = layers.Dense(2, activation=\"linear\")(x)\n",
    "\n",
    "#     return Model(inputs, x)\n",
    "\n",
    "# # Instantiate and compile the model\n",
    "# model = DeeplabV3Plus(input_shape=(32, 32, 3), num_classes=2)\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# # Summary of the model\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n",
    "# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8)\n",
    "# model.fit(train_generator, validation_data=val_generator, epochs=100, callbacks=[reduce_lr, stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread('.\\Images\\Indirect-Agro\\indirect_agro_1.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = next(train_generator)\n",
    "# sample = model.predict(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TRUTH\n",
    "# unprocess_true = unprocess_label(test_data[1][0], MAXPH, MINPH, MAXMOISTURE, MINMOISTURE)\n",
    "# print(np.mean(unprocess_true[0]))\n",
    "# print(np.mean(unprocess_true[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## PREDICTED\n",
    "# unprocess_predict = unprocess_label(sample[0], MAXPH, MINPH, MAXMOISTURE, MINMOISTURE)\n",
    "# print(np.mean(unprocess_predict[0]))\n",
    "# print(np.mean(unprocess_predict[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_moisture, accuracy_ph = get_accuracy(test_data[1][0],sample[0])\n",
    "\n",
    "# print(\"Moisture Accuracy: {:.2f}%\".format(accuracy_moisture))\n",
    "# print(\"pH Accuracy: {:.2f}%\".format(accuracy_ph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(sample[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepLabV3Plus\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " model (Functional)             [(None, 16, 16, 64)  23587712    ['input_6[0][0]']                \n",
      "                                , (None, 8, 8, 256)                                               \n",
      "                                , (None, 4, 4, 512)                                               \n",
      "                                , (None, 2, 2, 1024                                               \n",
      "                                ),                                                                \n",
      "                                 (None, 1, 1, 2048)                                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 2048)         0           ['model[0][4]']                  \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 256)          524544      ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 256)          0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 256)          65792       ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 256)          0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 256)          65792       ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 256)          0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 256)          65792       ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 256)         1024        ['dense_23[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 512)          131584      ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 1, 1, 512)    0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 512)         0           ['reshape_4[0][0]']              \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 1, 1, 512)    0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 1, 1, 256)    131328      ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 1, 1, 256)    131328      ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 1, 1, 256)    1179904     ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 1, 1, 256)    1179904     ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 1, 1, 256)    1179904     ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_32 (UpSampling2D  (None, 1, 1, 256)   0           ['conv2d_52[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 1, 1, 1280)   0           ['conv2d_53[0][0]',              \n",
      "                                                                  'conv2d_54[0][0]',              \n",
      "                                                                  'conv2d_55[0][0]',              \n",
      "                                                                  'conv2d_56[0][0]',              \n",
      "                                                                  'up_sampling2d_32[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 1, 1, 256)    327936      ['concatenate_29[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_33 (UpSampling2D  (None, 8, 8, 256)   0           ['conv2d_57[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 8, 8, 512)    0           ['up_sampling2d_33[0][0]',       \n",
      "                                                                  'model[0][1]']                  \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 8, 8, 256)    1179904     ['concatenate_30[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 8, 8, 256)    590080      ['conv2d_58[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_34 (UpSampling2D  (None, 16, 16, 256)  0          ['conv2d_59[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 16, 16, 320)  0           ['up_sampling2d_34[0][0]',       \n",
      "                                                                  'model[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 16, 16, 256)  737536      ['concatenate_31[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 16, 16, 256)  590080      ['conv2d_60[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_35 (UpSampling2D  (None, 32, 32, 256)  0          ['conv2d_61[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 32, 32, 2)    514         ['up_sampling2d_35[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,670,658\n",
      "Trainable params: 31,617,026\n",
      "Non-trainable params: 53,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# best_hpsC.values['filters']=64\n",
    "# best_hpsC.values['learning_rate']=1e-6\n",
    "# Build the model with the best hp.\n",
    "modelC = DeeplabV3Plus(best_hpsC)\n",
    "modelC.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n",
      "40/40 - 10s - loss: 33.7004 - mae: 0.8967 - val_loss: 0.0581 - val_mae: 0.1553 - lr: 0.0010 - 10s/epoch - 240ms/step\n",
      "Epoch 2/1024\n",
      "40/40 - 3s - loss: 0.0337 - mae: 0.1232 - val_loss: 0.0714 - val_mae: 0.1686 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 3/1024\n",
      "40/40 - 3s - loss: 0.0318 - mae: 0.1175 - val_loss: 0.0322 - val_mae: 0.1291 - lr: 0.0010 - 3s/epoch - 71ms/step\n",
      "Epoch 4/1024\n",
      "40/40 - 3s - loss: 0.0313 - mae: 0.1130 - val_loss: 0.0293 - val_mae: 0.1167 - lr: 0.0010 - 3s/epoch - 71ms/step\n",
      "Epoch 5/1024\n",
      "40/40 - 3s - loss: 0.0311 - mae: 0.1125 - val_loss: 0.0292 - val_mae: 0.1170 - lr: 0.0010 - 3s/epoch - 73ms/step\n",
      "Epoch 6/1024\n",
      "40/40 - 3s - loss: 0.0309 - mae: 0.1125 - val_loss: 0.0296 - val_mae: 0.1035 - lr: 0.0010 - 3s/epoch - 69ms/step\n",
      "Epoch 7/1024\n",
      "40/40 - 3s - loss: 0.0309 - mae: 0.1115 - val_loss: 0.0287 - val_mae: 0.1110 - lr: 0.0010 - 3s/epoch - 71ms/step\n",
      "Epoch 8/1024\n",
      "40/40 - 3s - loss: 0.0303 - mae: 0.1088 - val_loss: 0.0288 - val_mae: 0.1076 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 9/1024\n",
      "40/40 - 3s - loss: 0.0317 - mae: 0.1132 - val_loss: 0.0354 - val_mae: 0.1020 - lr: 0.0010 - 3s/epoch - 67ms/step\n",
      "Epoch 10/1024\n",
      "40/40 - 3s - loss: 0.0308 - mae: 0.1102 - val_loss: 0.0301 - val_mae: 0.1021 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 11/1024\n",
      "40/40 - 3s - loss: 0.0303 - mae: 0.1097 - val_loss: 0.0290 - val_mae: 0.1008 - lr: 0.0010 - 3s/epoch - 69ms/step\n",
      "Epoch 12/1024\n",
      "40/40 - 3s - loss: 0.0293 - mae: 0.1065 - val_loss: 0.0298 - val_mae: 0.1224 - lr: 0.0010 - 3s/epoch - 68ms/step\n",
      "Epoch 13/1024\n",
      "40/40 - 3s - loss: 0.0291 - mae: 0.1066 - val_loss: 0.0283 - val_mae: 0.1007 - lr: 1.0000e-04 - 3s/epoch - 71ms/step\n",
      "Epoch 14/1024\n",
      "40/40 - 3s - loss: 0.0287 - mae: 0.1048 - val_loss: 0.0282 - val_mae: 0.0993 - lr: 1.0000e-04 - 3s/epoch - 71ms/step\n",
      "Epoch 15/1024\n",
      "40/40 - 3s - loss: 0.0289 - mae: 0.1038 - val_loss: 0.0286 - val_mae: 0.0984 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 16/1024\n",
      "40/40 - 3s - loss: 0.0286 - mae: 0.1035 - val_loss: 0.0283 - val_mae: 0.0983 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 17/1024\n",
      "40/40 - 3s - loss: 0.0287 - mae: 0.1046 - val_loss: 0.0281 - val_mae: 0.0988 - lr: 1.0000e-04 - 3s/epoch - 73ms/step\n",
      "Epoch 18/1024\n",
      "40/40 - 3s - loss: 0.0287 - mae: 0.1039 - val_loss: 0.0278 - val_mae: 0.1002 - lr: 1.0000e-04 - 3s/epoch - 71ms/step\n",
      "Epoch 19/1024\n",
      "40/40 - 3s - loss: 0.0285 - mae: 0.1035 - val_loss: 0.0281 - val_mae: 0.0986 - lr: 1.0000e-04 - 3s/epoch - 71ms/step\n",
      "Epoch 20/1024\n",
      "40/40 - 3s - loss: 0.0286 - mae: 0.1036 - val_loss: 0.0283 - val_mae: 0.0967 - lr: 1.0000e-04 - 3s/epoch - 70ms/step\n",
      "Epoch 21/1024\n",
      "40/40 - 3s - loss: 0.0284 - mae: 0.1034 - val_loss: 0.0283 - val_mae: 0.0955 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 22/1024\n",
      "40/40 - 3s - loss: 0.0282 - mae: 0.1031 - val_loss: 0.0287 - val_mae: 0.0945 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 23/1024\n",
      "40/40 - 3s - loss: 0.0285 - mae: 0.1031 - val_loss: 0.0279 - val_mae: 0.0961 - lr: 1.0000e-04 - 3s/epoch - 68ms/step\n",
      "Epoch 24/1024\n",
      "40/40 - 3s - loss: 0.0285 - mae: 0.1041 - val_loss: 0.0279 - val_mae: 0.0947 - lr: 1.0000e-05 - 3s/epoch - 69ms/step\n",
      "Epoch 25/1024\n",
      "40/40 - 3s - loss: 0.0284 - mae: 0.1032 - val_loss: 0.0277 - val_mae: 0.0938 - lr: 1.0000e-05 - 3s/epoch - 71ms/step\n",
      "Epoch 26/1024\n",
      "40/40 - 3s - loss: 0.0283 - mae: 0.1030 - val_loss: 0.0273 - val_mae: 0.0935 - lr: 1.0000e-05 - 3s/epoch - 71ms/step\n",
      "Epoch 27/1024\n",
      "40/40 - 3s - loss: 0.0285 - mae: 0.1034 - val_loss: 0.0272 - val_mae: 0.0931 - lr: 1.0000e-05 - 3s/epoch - 71ms/step\n",
      "Epoch 28/1024\n",
      "40/40 - 3s - loss: 0.0282 - mae: 0.1028 - val_loss: 0.0271 - val_mae: 0.0928 - lr: 1.0000e-05 - 3s/epoch - 71ms/step\n",
      "Epoch 29/1024\n",
      "40/40 - 3s - loss: 0.0282 - mae: 0.1027 - val_loss: 0.0270 - val_mae: 0.0928 - lr: 1.0000e-05 - 3s/epoch - 71ms/step\n",
      "Epoch 30/1024\n",
      "40/40 - 3s - loss: 0.0286 - mae: 0.1034 - val_loss: 0.0270 - val_mae: 0.0926 - lr: 1.0000e-05 - 3s/epoch - 68ms/step\n",
      "Epoch 31/1024\n",
      "40/40 - 3s - loss: 0.0283 - mae: 0.1024 - val_loss: 0.0266 - val_mae: 0.0929 - lr: 1.0000e-05 - 3s/epoch - 71ms/step\n",
      "Epoch 32/1024\n",
      "40/40 - 3s - loss: 0.0283 - mae: 0.1029 - val_loss: 0.0261 - val_mae: 0.0930 - lr: 1.0000e-05 - 3s/epoch - 72ms/step\n",
      "Epoch 33/1024\n",
      "40/40 - 3s - loss: 0.0284 - mae: 0.1028 - val_loss: 0.0260 - val_mae: 0.0931 - lr: 1.0000e-05 - 3s/epoch - 71ms/step\n",
      "Epoch 34/1024\n",
      "40/40 - 3s - loss: 0.0283 - mae: 0.1028 - val_loss: 0.0258 - val_mae: 0.0937 - lr: 1.0000e-05 - 3s/epoch - 70ms/step\n",
      "Epoch 35/1024\n",
      "40/40 - 3s - loss: 0.0280 - mae: 0.1022 - val_loss: 0.0259 - val_mae: 0.0938 - lr: 1.0000e-05 - 3s/epoch - 67ms/step\n",
      "Epoch 36/1024\n",
      "40/40 - 3s - loss: 0.0286 - mae: 0.1034 - val_loss: 0.0257 - val_mae: 0.0946 - lr: 1.0000e-05 - 3s/epoch - 71ms/step\n",
      "Epoch 37/1024\n",
      "40/40 - 3s - loss: 0.0286 - mae: 0.1028 - val_loss: 0.0256 - val_mae: 0.0957 - lr: 1.0000e-05 - 3s/epoch - 72ms/step\n",
      "Epoch 38/1024\n",
      "40/40 - 3s - loss: 0.0285 - mae: 0.1036 - val_loss: 0.0255 - val_mae: 0.0966 - lr: 1.0000e-05 - 3s/epoch - 70ms/step\n",
      "Epoch 39/1024\n",
      "40/40 - 3s - loss: 0.0283 - mae: 0.1032 - val_loss: 0.0256 - val_mae: 0.0963 - lr: 1.0000e-05 - 3s/epoch - 68ms/step\n",
      "Epoch 40/1024\n",
      "40/40 - 3s - loss: 0.0284 - mae: 0.1030 - val_loss: 0.0255 - val_mae: 0.0976 - lr: 1.0000e-05 - 3s/epoch - 67ms/step\n",
      "Epoch 41/1024\n",
      "40/40 - 3s - loss: 0.0282 - mae: 0.1026 - val_loss: 0.0256 - val_mae: 0.0974 - lr: 1.0000e-05 - 3s/epoch - 68ms/step\n",
      "Epoch 42/1024\n",
      "40/40 - 3s - loss: 0.0283 - mae: 0.1026 - val_loss: 0.0256 - val_mae: 0.0980 - lr: 1.0000e-05 - 3s/epoch - 67ms/step\n",
      "Epoch 43/1024\n",
      "40/40 - 3s - loss: 0.0281 - mae: 0.1021 - val_loss: 0.0256 - val_mae: 0.0988 - lr: 1.0000e-05 - 3s/epoch - 67ms/step\n",
      "Epoch 44/1024\n",
      "40/40 - 3s - loss: 0.0286 - mae: 0.1038 - val_loss: 0.0256 - val_mae: 0.0991 - lr: 1.0000e-06 - 3s/epoch - 67ms/step\n",
      "Epoch 45/1024\n",
      "40/40 - 3s - loss: 0.0282 - mae: 0.1029 - val_loss: 0.0257 - val_mae: 0.0992 - lr: 1.0000e-06 - 3s/epoch - 67ms/step\n",
      "Epoch 46/1024\n",
      "40/40 - 3s - loss: 0.0283 - mae: 0.1031 - val_loss: 0.0257 - val_mae: 0.0993 - lr: 1.0000e-06 - 3s/epoch - 67ms/step\n",
      "Epoch 47/1024\n",
      "40/40 - 3s - loss: 0.0285 - mae: 0.1035 - val_loss: 0.0257 - val_mae: 0.0992 - lr: 1.0000e-06 - 3s/epoch - 67ms/step\n",
      "Epoch 48/1024\n",
      "40/40 - 3s - loss: 0.0283 - mae: 0.1029 - val_loss: 0.0257 - val_mae: 0.0991 - lr: 1.0000e-06 - 3s/epoch - 67ms/step\n",
      "Epoch 49/1024\n",
      "40/40 - 3s - loss: 0.0282 - mae: 0.1026 - val_loss: 0.0257 - val_mae: 0.0991 - lr: 1.0000e-07 - 3s/epoch - 67ms/step\n",
      "Epoch 50/1024\n",
      "40/40 - 3s - loss: 0.0286 - mae: 0.1034 - val_loss: 0.0256 - val_mae: 0.0990 - lr: 1.0000e-07 - 3s/epoch - 67ms/step\n",
      "Epoch 51/1024\n",
      "40/40 - 3s - loss: 0.0285 - mae: 0.1032 - val_loss: 0.0256 - val_mae: 0.0991 - lr: 1.0000e-07 - 3s/epoch - 66ms/step\n",
      "Epoch 52/1024\n",
      "40/40 - 3s - loss: 0.0282 - mae: 0.1027 - val_loss: 0.0256 - val_mae: 0.0988 - lr: 1.0000e-07 - 3s/epoch - 67ms/step\n",
      "Epoch 53/1024\n",
      "40/40 - 3s - loss: 0.0285 - mae: 0.1032 - val_loss: 0.0256 - val_mae: 0.0989 - lr: 1.0000e-07 - 3s/epoch - 67ms/step\n",
      "Epoch 54/1024\n",
      "40/40 - 3s - loss: 0.0282 - mae: 0.1025 - val_loss: 0.0257 - val_mae: 0.0990 - lr: 1.0000e-08 - 3s/epoch - 69ms/step\n",
      "Training time: 156.5701916217804s\n"
     ]
    }
   ],
   "source": [
    "## DESIGN C\n",
    "import time \n",
    "start = time.time()\n",
    "historyC = modelC.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [ES, REDUCE_LR])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 26ms/step\n",
      "Train Accuracy\n",
      "Moisture: 98.90, pH: 98.58\n",
      "10/10 [==============================] - 0s 22ms/step\n",
      "Test Accuracy\n",
      "Moisture: 98.76, pH: 98.67\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "TRUTH moisture: 0.13, ph: 3.74\n",
      "PREDICTIONS moisture: 0.14, ph: 3.79\n"
     ]
    }
   ],
   "source": [
    "## Prepare set of x values and y values\n",
    "\n",
    "modelC_train_accuracy = get_model_accuracy(modelC, gen_X_values_3_1, y_values_1, X_values_1)\n",
    "print(\"Train Accuracy\")\n",
    "print(\"Moisture: {:.2f}, pH: {:.2f}\".format(modelC_train_accuracy[0], modelC_train_accuracy[1]))\n",
    "\n",
    "modelC_test_accuracy = get_model_accuracy(modelC, gen_X_values_3_2, y_values_2, X_values_2)\n",
    "print(\"Test Accuracy\")\n",
    "print(\"Moisture: {:.2f}, pH: {:.2f}\".format(modelC_test_accuracy[0], modelC_test_accuracy[1]))\n",
    "\n",
    "\n",
    "predictions = modelC.predict(np.array([SAMPLE[0][0]]))\n",
    "truth = unprocess_label_wmask(SAMPLE[0][0],SAMPLE[1][0])\n",
    "pred = unprocess_label_wmask(SAMPLE[0][0],predictions[0])\n",
    "print(\"TRUTH moisture: {:.2f}, ph: {:.2f}\".format(truth[0],truth[1]))\n",
    "print(\"PREDICTIONS moisture: {:.2f}, ph: {:.2f}\".format(pred[0],pred[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 512\n",
    "# steps_per_epoch = 512 // batch_size + 1  # we usually consider 1 epoch to be\n",
    "#                                             # the point where the model has seen\n",
    "#                                             # all the training samples at least once\n",
    "# min_lr = 1e-8\n",
    "# factor = 0.1\n",
    "# SCHEDULE_EPOCH = 200\n",
    "# STEPS = 200\n",
    "\n",
    "# historyC = {\"history\":{\"loss\":[],\"mae\":[],\"val_loss\":[],\"val_mae\":[]}}\n",
    "# for e in range(epochs):\n",
    "#     for i, (images, y_batch) in enumerate(train_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#        loss = modelC.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#     #    val = modelB.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyC[\"history\"][\"loss\"].append(loss[0])\n",
    "#             historyC[\"history\"][\"mae\"].append(loss[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             break  \n",
    "#     for i, (images, y_batch) in enumerate(val_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#     #    loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#        val = modelC.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyC[\"history\"][\"val_loss\"].append(val[0])\n",
    "#             historyC[\"history\"][\"val_mae\"].append(val[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             curr_lr = modelC.optimizer.learning_rate\n",
    "#             if e>=SCHEDULE_EPOCH and curr_lr>min_lr: \n",
    "#                K.set_value(modelC.optimizer.learning_rate, curr_lr*factor)\n",
    "#                curr_lr = modelC.optimizer.learning_rate\n",
    "#                SCHEDULE_EPOCH+=STEPS\n",
    "#             # patience = 2\n",
    "#             # if e>patience and curr_lr>min_lr:\n",
    "#                # curr_loss = historyA[\"history\"][\"val_loss\"][-1]\n",
    "#                # for i in historyA[\"history\"][\"val_loss\"][-patience-1:-1]:\n",
    "#                #    if curr_loss >= i: patience-=1\n",
    "#                #    if patience < 1:\n",
    "#                      # K.set_value(modelA.optimizer.learning_rate, curr_lr*factor)\n",
    "#                      # curr_lr = modelA.optimizer.learning_rate\n",
    "#             break  \n",
    "#     print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}   CURR_LR {:.2E}\".format(e+1, loss[0], loss[1],val[0],val[1], curr_lr.numpy()))\n",
    "#     train_generator.on_epoch_end()  # this shuffles the data at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyC[\"history\"][\"loss\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyC[\"history\"][\"val_loss\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyC[\"history\"][\"mae\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyC[\"history\"][\"val_mae\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.suptitle(\"Model C: DeepLabV3+\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n",
    "# result = modelC(sample[0])\n",
    "# for x,i in enumerate(result):\n",
    "#     RGB  = cv2.cvtColor(sample[0][x],cv2.COLOR_HSV2RGB)\n",
    "#     MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "#     ARRAY = np.array(result[5]).reshape(image_size) * MASK\n",
    "#     OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "#     print(np.average(OUTPUT), np.average(sample[1][x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelC.save('design_models/designC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shutil import rmtree\n",
    "# # removing directory \n",
    "# rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelC.save('design_models/designC_v6.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANUFACTURABILITY: TRAINING TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = tf.keras.models.load_model('design_models/designA_v6.h5')\n",
    "modelB = tf.keras.models.load_model('design_models/designB_v6.h5')\n",
    "modelC = tf.keras.models.load_model('design_models/designC_v6.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 128\n",
    "REDUCE_LR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=5, min_lr=1e-9)\n",
    "ES = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=16, mode=\"min\", restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DESIGN A\n",
    "# import time \n",
    "# start = time.time()\n",
    "# historyA = modelA.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [es, reduce_lr])\n",
    "# stop = time.time()\n",
    "# print(f\"Training time: {stop - start}s\")\n",
    "\n",
    "# # ## FUNCTIONALITY: INFERENCE TIME\n",
    "# # modelC.evaluate(train_generator[1][0][0].reshape(1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DESIGN B\n",
    "# import time \n",
    "# start = time.time()\n",
    "# historyB = modelB.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [es, reduce_lr])\n",
    "# stop = time.time()\n",
    "# print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DESIGN C\n",
    "# import time \n",
    "# start = time.time()\n",
    "# historyC = modelC.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [es, reduce_lr])\n",
    "# stop = time.time()\n",
    "# print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPS: 492177674\n"
     ]
    }
   ],
   "source": [
    "## Design A\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculate FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsA = get_flops(modelA, batch_size=1)\n",
    "print(f\"FLOPS: {flopsA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPS: 1076748438\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculate FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsB = get_flops(modelB, batch_size=1)\n",
    "print(f\"FLOPS: {flopsB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPS: 2257500296\n"
     ]
    }
   ],
   "source": [
    "## Design C\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculate FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsC = get_flops(modelC, batch_size=1)\n",
    "print(f\"FLOPS: {flopsC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONALITY: INFERENCE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sample = train_generator[1][0][0].reshape(1,32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "Inference time: 235.0521ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "## Design A\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelA.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "Inference time: 53.0114ms\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelB.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "Inference time: 66.7467ms\n"
     ]
    }
   ],
   "source": [
    "## Design AC\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelC.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORMANCE: COEFFICIENT OF DETERMINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare set of x values and y values for performance constraint\n",
    "X_values,y_values = [],[]\n",
    "for i in range(10):\n",
    "    values = next(val_generator)\n",
    "    # for j in range(values[0].shape[0]):\n",
    "    X_values.append(values[0])\n",
    "    y_values.append(values[1])\n",
    "\n",
    "## create X_values generator\n",
    "gen_X_values_1 = (x for x in X_values)\n",
    "gen_X_values_2 = (x for x in X_values)\n",
    "gen_X_values_3 = (x for x in X_values)\n",
    "y_values = [y  for y_set in y_values for y in y_set]\n",
    "y_values_0 = np.array(y_values)[:,:,:,0]\n",
    "y_values_1 = np.array(y_values)[:,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 43ms/step\n",
      "R^2 for class 0: 0.5327\n",
      "R^2 for class 1: 0.5218\n",
      "Average R^2 score: 0.5273\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_values = np.array(y_values)\n",
    "predictionsA = modelA.predict(gen_X_values_1)\n",
    "\n",
    "y_values_flat_0 = y_values[:, :, :, 0].flatten()\n",
    "predictionsA_flat_0 = predictionsA[:, :, :, 0].flatten()\n",
    "\n",
    "y_values_flat_1 = y_values[:, :, :, 1].flatten()\n",
    "predictionsA_flat_1 = predictionsA[:, :, :, 1].flatten()\n",
    "\n",
    "# Calculate R^2 score for each class\n",
    "r2_score_0 = r2_score(y_values_flat_0, predictionsA_flat_0)\n",
    "r2_score_1 = r2_score(y_values_flat_1, predictionsA_flat_1)\n",
    "\n",
    "print(f'R^2 for class 0: {r2_score_0:.4f}')\n",
    "print(f'R^2 for class 1: {r2_score_1:.4f}')\n",
    "\n",
    "print(f\"Average R^2 score: {(r2_score_0+r2_score_1)/2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 18ms/step\n",
      "R^2 for class 0: 0.4947\n",
      "R^2 for class 1: 0.4940\n",
      "Average R^2 score: 0.4943\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## PERFORMANCE: COEFFICIENT OF DETERMINATION\n",
    "from sklearn.metrics import roc_auc_score\n",
    "predictionsB = modelB.predict(gen_X_values_2)\n",
    "\n",
    "y_values_flat_0 = y_values[:, :, :, 0].flatten()\n",
    "predictionsB_flat_0 = predictionsB[:, :, :, 0].flatten()\n",
    "\n",
    "y_values_flat_1 = y_values[:, :, :, 1].flatten()\n",
    "predictionsB_flat_1 = predictionsB[:, :, :, 1].flatten()\n",
    "\n",
    "# Calculate R^2 score for each class\n",
    "r2_score_0 = r2_score(y_values_flat_0, predictionsB_flat_0)\n",
    "r2_score_1 = r2_score(y_values_flat_1, predictionsB_flat_1)\n",
    "\n",
    "print(f'R^2 for class 0: {r2_score_0:.4f}')\n",
    "print(f'R^2 for class 1: {r2_score_1:.4f}')\n",
    "\n",
    "print(f\"Average R^2 score: {(r2_score_0+r2_score_1)/2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 22ms/step\n",
      "R^2 for class 0: 0.4550\n",
      "R^2 for class 1: 0.4748\n",
      "Average R^2 score: 0.4649\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## PERFORMANCE: COEFFICIENT OF DETERMINATION\n",
    "from sklearn.metrics import roc_auc_score\n",
    "predictionsC = modelC.predict(gen_X_values_3)\n",
    "\n",
    "y_values_flat_0 = y_values[:, :, :, 0].flatten()\n",
    "predictionsC_flat_0 = predictionsC[:, :, :, 0].flatten()\n",
    "\n",
    "y_values_flat_1 = y_values[:, :, :, 1].flatten()\n",
    "predictionsC_flat_1 = predictionsC[:, :, :, 1].flatten()\n",
    "\n",
    "# Calculate R^2 score for each class\n",
    "r2_score_0 = r2_score(y_values_flat_0, predictionsC_flat_0)\n",
    "r2_score_1 = r2_score(y_values_flat_1, predictionsC_flat_1)\n",
    "\n",
    "print(f'R^2 for class 0: {r2_score_0:.4f}')\n",
    "print(f'R^2 for class 1: {r2_score_1:.4f}')\n",
    "\n",
    "print(f\"Average R^2 score: {(r2_score_0+r2_score_1)/2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFFICIENCY: STORAGE CONSUMPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weight size in megabytes: 470.6279\n"
     ]
    }
   ],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsA = modelA.get_weights()\n",
    "total_sizeA = 0\n",
    "for weight in weightsA:\n",
    "    total_sizeA += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeA*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weight size in megabytes: 300.9896\n"
     ]
    }
   ],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsB = modelB.get_weights()\n",
    "total_sizeB = 0\n",
    "for weight in weightsB:\n",
    "    total_sizeB += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeB*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weight size in megabytes: 253.3653\n"
     ]
    }
   ],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsC = modelC.get_weights()\n",
    "total_sizeC = 0\n",
    "for weight in weightsC:\n",
    "    total_sizeC += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeC*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving final trained and constrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA.save('design_models/designA_v8.h5')\n",
    "modelB.save('design_models/designB_v8.h5')\n",
    "modelC.save('design_models/designC_v8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Accuracy of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare set of x values and y values for performance constraint\n",
    "X_values,y_values = [],[]\n",
    "for i in range(100):\n",
    "    values = next(val_generator)\n",
    "    # for j in range(values[0].shape[0]):\n",
    "    X_values.append(values[0])\n",
    "    y_values.append(values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
