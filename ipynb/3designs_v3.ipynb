{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Flatten, LeakyReLU, ReLU, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, concatenate, Activation\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.backend import clear_session\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop , SGD\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "# from tensorflow.keras.regularizers import L2\n",
    "# from tensorflow.keras import metrics\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from shutil import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 196 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('.\\\\final_data.csv',  names=[\"path\",\"value\"])\n",
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    # # rescale=1./255\n",
    "        # rescale=1.,\n",
    "        # width_shift_range = 0.5,\n",
    "        # height_shift_range = 0.5, \n",
    "        # zoom_range = 0.2,\n",
    "        # shear_range = 0.2,\n",
    "        # horizontal_flip = True,\n",
    "        # vertical_flip = True,\n",
    "    #  channel_shift_range = 64.0,\n",
    "        # brightness_range = (0.7,1.0),\n",
    "        # rotation_range = 45,\n",
    "    )\n",
    "\n",
    "data_generator = generator.flow_from_dataframe(\n",
    "    df, \n",
    "    x_col=\"path\", \n",
    "    y_col=\"value\", \n",
    "    class_mode='raw', \n",
    "    batch_size=198,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = []\n",
    "Ys = []\n",
    "iterations = 3\n",
    "for i in range(iterations):\n",
    "    x,y = next(data_generator)\n",
    "    Xs.extend([np.array(value).astype(int) for value in x])\n",
    "    Ys.extend([np.array(value.replace(\"'\",\"\")[1:-1].split(', ')).astype(float) for value in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXPH = np.max(np.array(Ys)[:,1])\n",
    "MINPH = np.min(np.array(Ys)[:,1])\n",
    "\n",
    "MAXMOISTURE = np.max(np.array(Ys)[:,0])\n",
    "MINMOISTURE = np.min(np.array(Ys)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(image):\n",
    "    image = image.astype(np.uint8)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    binr = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    binr = np.invert(binr)\n",
    "\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    mask = cv2.erode(binr, kernel, iterations=3)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def process_image(image):\n",
    "    rgb_image = image.astype(np.uint8)\n",
    "\n",
    "    rgb_planes = cv2.split(rgb_image)\n",
    "    result_planes = []\n",
    "    for plane in rgb_planes:\n",
    "        processed_image = cv2.medianBlur(plane, 2)\n",
    "        result_planes.append(processed_image)\n",
    "    result = cv2.merge(result_planes)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(image, label, maxPh, minPh, maxMoisture, minMoisture):\n",
    "    mask = get_mask(np.array(image))\n",
    "    shape = mask.shape \n",
    "    array = np.ones(shape=(shape[0], shape[1], 1))\n",
    "\n",
    "    moisture = cv2.bitwise_and(array, array, mask=mask)\n",
    "    ph = cv2.bitwise_and(array, array, mask=mask)\n",
    "\n",
    "    moisture_value = (label[0] - minMoisture) / (maxMoisture - minMoisture)\n",
    "    ph_value = (label[1] - minPh) / (maxPh - minPh)\n",
    "    moisture[moisture > 0] = moisture_value\n",
    "    ph[ph > 0] = ph_value\n",
    "\n",
    "    output = np.stack([moisture, ph], axis=-1)  \n",
    "\n",
    "    return output\n",
    "\n",
    "# def process_label(image, label, maxPh, minPh, maxMoisture, minMoisture):\n",
    "#     # mask = get_mask(np.array(image))\n",
    "#     # shape = mask.shape \n",
    "#     # array = np.ones(shape=(shape[0], shape[1], 1))\n",
    "\n",
    "#     # moisture = cv2.bitwise_and(array, array, mask=mask)\n",
    "#     # ph = cv2.bitwise_and(array, array, mask=mask)\n",
    "\n",
    "#     moisture_value = (label[0] - minMoisture) / (maxMoisture - minMoisture)\n",
    "#     ph_value = (label[1] - minPh) / (maxPh - minPh)\n",
    "#     # moisture[moisture > 0] = moisture_value\n",
    "#     # ph[ph > 0] = ph_value\n",
    "\n",
    "#     output = np.stack([moisture_value, ph_value], axis=-1)  \n",
    "\n",
    "#     return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n"
     ]
    }
   ],
   "source": [
    "Y_values = []\n",
    "X_values = []\n",
    "for i, x in enumerate(Xs):\n",
    "    print(i)\n",
    "    Y_values.append(process_label(Xs[i], Ys[i], MAXPH, MINPH, MAXMOISTURE, MINMOISTURE))\n",
    "    # Y_values.append(process_label(Ys[i], MAXPH, MINPH, MAXMOISTURE, MINMOISTURE))\n",
    "    X_values.append(process_image(Xs[i]))\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_values,Y_values , \n",
    "                                #    random_state=23,  \n",
    "                                   test_size=0.20,  \n",
    "                                   shuffle=True) \n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "        width_shift_range = 0.1,\n",
    "        height_shift_range = 0.1, \n",
    "        # zoom_range = 0.2,\n",
    "        # shear_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True,\n",
    "    #  channel_shift_range = 64.0,\n",
    "        brightness_range = (0.8,1.0),\n",
    "        rotation_range = 15,\n",
    "    ).flow(x=np.array(X_train), y=y_train, batch_size=6) \n",
    "\n",
    "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    "    ).flow(x=np.array(X_test), y=y_test, batch_size=6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.58974359 0.14814815]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "for i in next(train_generator)[1][0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = \"path\"\n",
    "y_col = \"pH\"\n",
    "batch_size = 16\n",
    "epochs = 1024\n",
    "lr = 1e-5\n",
    "image_size = (IMAGE_SIZE[0],IMAGE_SIZE[1])\n",
    "channels = 3\n",
    "shuffle = True\n",
    "class_mode =\"raw\"\n",
    "color_mode = \"rgb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "#                                      width_shift_range = 0.2,\n",
    "#                                      height_shift_range = 0.2, \n",
    "#                                      zoom_range = 0.2,\n",
    "#                                      shear_range = 0.2,\n",
    "#                                      horizontal_flip = True,\n",
    "#                                      vertical_flip = True,\n",
    "#                                     #  channel_shift_range = 64.0,\n",
    "#                                      brightness_range = (0.5,1.0),\n",
    "#                                      rotation_range = 45,\n",
    "#                                      )\n",
    "# train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\n",
    "#                                               x_col=x_col, y_col=y_col, has_ext=True, \n",
    "#                                               class_mode=class_mode, target_size=image_size, \n",
    "#                                               batch_size=batch_size, color_mode = color_mode,\n",
    "#                                               shuffle = shuffle)\n",
    "# val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# val_generator = val_datagen.flow_from_dataframe(dataframe=val_df,\n",
    "#                                               x_col=x_col, y_col=y_col, has_ext=True, \n",
    "#                                               class_mode=class_mode, target_size=image_size, \n",
    "#                                               batch_size=batch_size,  color_mode = color_mode,\n",
    "#                                               shuffle = shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design A: Nested U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 70, 70, 3)    0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 32, 32, 64)   9472        ['conv1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 32, 32, 64)   256         ['conv1_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 32, 32, 64)   0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 34, 34, 64)   0           ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 16, 16, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 16, 16, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 16, 16, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 16, 16, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 16, 16, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 16, 16, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 16, 16, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 16, 16, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 16, 16, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 16, 16, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 16, 16, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 16, 16, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 16, 16, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 16, 16, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 16, 16, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 16, 16, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 16, 16, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 8, 8, 128)    32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 8, 8, 512)    131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 8, 8, 512)    0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 8, 8, 128)    65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 8, 8, 512)    0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 8, 8, 128)    65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 8, 8, 512)    0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 8, 8, 128)    65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 8, 8, 512)    0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 4, 4, 256)    131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 4, 4, 1024)   525312      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                                                  'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block1_out[0][0]',       \n",
      "                                                                  'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block2_out[0][0]',       \n",
      "                                                                  'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block3_out[0][0]',       \n",
      "                                                                  'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block4_out[0][0]',       \n",
      "                                                                  'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block5_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block5_out[0][0]',       \n",
      "                                                                  'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block6_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 2, 2, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 2, 2, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 2, 2, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 2, 2, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 2, 2, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 2, 2, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 2, 2, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 2, 2, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 2, 2, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 2, 2, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 2, 2, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 2, 2, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 2, 2, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 2, 2, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 2, 2, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 2, 2, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 2, 2, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 2, 2, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 2, 2, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 2, 2, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 2, 2, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 2, 2, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 8192)         0           ['conv5_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1024)         8389632     ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 1024)         0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1024)         1049600     ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 2048)         2099200     ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 2, 2, 512)    0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " up_sampling2d_10 (UpSampling2D  (None, 4, 4, 512)   0           ['reshape_2[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 4, 4, 512)    1049088     ['up_sampling2d_10[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 4, 4, 1536)   0           ['conv2d_28[0][0]',              \n",
      "                                                                  'conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 4, 4, 512)    7077888     ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 4, 4, 512)    0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_11 (UpSampling2D  (None, 8, 8, 512)   0           ['activation_22[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 256)    524544      ['up_sampling2d_11[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 8, 8, 768)    0           ['conv2d_30[0][0]',              \n",
      "                                                                  'conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 256)    1769472     ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_12 (UpSampling2D  (None, 16, 16, 256)  0          ['activation_23[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 16, 16, 128)  131200      ['up_sampling2d_12[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 16, 16, 384)  0           ['conv2d_32[0][0]',              \n",
      "                                                                  'conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 16, 16, 128)  442368      ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 16, 16, 128)  512        ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_13 (UpSampling2D  (None, 32, 32, 128)  0          ['activation_24[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 32, 32, 64)   32832       ['up_sampling2d_13[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 32, 32, 128)  0           ['conv2d_34[0][0]',              \n",
      "                                                                  'conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 32, 32, 64)   73728       ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 32, 32, 64)  256         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 32, 32, 1)    65          ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 46,231,169\n",
      "Trainable params: 46,176,129\n",
      "Non-trainable params: 55,040\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining the Convolutional Block\n",
    "def conv_block(inputs, num_filters):\n",
    "\t# Applying the sequence of Convolutional, Batch Normalization\n",
    "\t# and Activation Layers to the input tensor\n",
    "\tx = Sequential([\n",
    "\t\t# Convolutional Layer\n",
    "\t\tConv2D(num_filters, 1, padding='same'),\n",
    "\t\t# Batch Normalization Layer\n",
    "\t\tBatchNormalization(),\n",
    "\t\t# Activation Layer\n",
    "\t\tReLU(),\n",
    "\t\t# Convolutional Layer\n",
    "\t\tConv2D(num_filters, 1, padding='same'),\n",
    "\t\t# Batch Normalization Layer\n",
    "\t\tBatchNormalization(),\n",
    "\t\t# Activation Layer\n",
    "\t\tReLU()\n",
    "\t])(inputs)\n",
    "\n",
    "\t# Returning the output of the Convolutional Block\n",
    "\treturn x\n",
    "\n",
    "# Defining the Unet++ Model\n",
    "def unet_plus_plus_model(input_shape=(image_size[0], image_size[1], channels), num_classes=1, deep_supervision=True):\n",
    "\tinputs = Input(shape=input_shape)\n",
    "\n",
    "\t# Encoding Path\n",
    "\tx_00 = conv_block(inputs, 64)\n",
    "\tx_10 = conv_block(MaxPooling2D()(x_00), 128)\n",
    "\tx_20 = conv_block(MaxPooling2D()(x_10), 256)\n",
    "\tx_30 = conv_block(MaxPooling2D()(x_20), 512)\n",
    "\tx_40 = conv_block(MaxPooling2D()(x_30), 1024)\n",
    "\n",
    "\t# Nested Decoding Path\n",
    "\tx_01 = conv_block(concatenate(\n",
    "\t\t[x_00, UpSampling2D()(x_10)]), 64)\n",
    "\tx_11 = conv_block(concatenate(\n",
    "\t\t[x_10, UpSampling2D()(x_20)]), 128)\n",
    "\tx_21 = conv_block(concatenate(\n",
    "\t\t[x_20, UpSampling2D()(x_30)]), 256)\n",
    "\tx_31 = conv_block(concatenate(\n",
    "\t\t[x_30, UpSampling2D()(x_40)]), 512)\n",
    "\n",
    "\tx_02 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, UpSampling2D()(x_11)]), 64)\n",
    "\tx_12 = conv_block(concatenate(\n",
    "\t\t[x_10, x_11, UpSampling2D()(x_21)]), 128)\n",
    "\tx_22 = conv_block(concatenate(\n",
    "\t\t[x_20, x_21, UpSampling2D()(x_31)]), 256)\n",
    "\n",
    "\tx_03 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, x_02, UpSampling2D()(x_12)]), 64)\n",
    "\tx_13 = conv_block(concatenate(\n",
    "\t\t[x_10, x_11, x_12, UpSampling2D()(x_22)]), 128)\n",
    "\n",
    "\tx_04 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), 64)\n",
    "\n",
    "\t# Deep Supervision Path\n",
    "\t# If deep supervision is enabled, then the model will output the segmentation maps\n",
    "\t# at each stage of the decoding path\n",
    "\tif deep_supervision:\n",
    "\t\toutputs = [\n",
    "\t\t\tConv2D(num_classes, 1)(x_01),\n",
    "\t\t\tConv2D(num_classes, 1)(x_02),\n",
    "\t\t\tConv2D(num_classes, 1)(x_03),\n",
    "\t\t\tConv2D(num_classes, 1)(x_04)\n",
    "\t\t]\n",
    "\t\t# Concatenating the segmentation maps\n",
    "\t\toutputs = concatenate(outputs, axis=0)\n",
    "\n",
    "\t# If deep supervision is disabled, then the model will output the final segmentation map\n",
    "\t# which is the segmentation map at the end of the decoding path\n",
    "\telse:\n",
    "\t\t# flatten  = layers.Flatten()(x_04)\n",
    "\t\t# dense = Dense(1024, activation='relu')(flatten)\n",
    "\t\t# dense = Dense(512, activation='relu')(dense)\n",
    "\t\t# outputs = Dense(2, activation='sigmoid')(flatten)\n",
    "\t\toutputs = tf.keras.layers.Conv2D(2, 1, activation='sigmoid')(x_04)\n",
    "\t# Creating the model\n",
    "\tmodel = tf.keras.Model(\n",
    "\t\tinputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "\n",
    "\t# Returning the model\n",
    "\treturn model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Unet_plus_plus\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential_18 (Sequential)     (None, 32, 32, 64)   4928        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 64)  0           ['sequential_18[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_19 (Sequential)     (None, 16, 16, 128)  25856       ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 128)   0           ['sequential_19[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_20 (Sequential)     (None, 8, 8, 256)    100864      ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 4, 4, 256)   0           ['sequential_20[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_21 (Sequential)     (None, 4, 4, 512)    398336      ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 2, 2, 512)   0           ['sequential_21[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_22 (Sequential)     (None, 2, 2, 1024)   1583104     ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " up_sampling2d_13 (UpSampling2D  (None, 4, 4, 1024)  0           ['sequential_22[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_12 (UpSampling2D  (None, 8, 8, 512)   0           ['sequential_21[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 4, 4, 1536)   0           ['sequential_21[0][0]',          \n",
      "                                                                  'up_sampling2d_13[0][0]']       \n",
      "                                                                                                  \n",
      " up_sampling2d_11 (UpSampling2D  (None, 16, 16, 256)  0          ['sequential_20[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 8, 8, 768)    0           ['sequential_20[0][0]',          \n",
      "                                                                  'up_sampling2d_12[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_26 (Sequential)     (None, 4, 4, 512)    1053696     ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_10 (UpSampling2D  (None, 32, 32, 128)  0          ['sequential_19[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 16, 16, 384)  0           ['sequential_19[0][0]',          \n",
      "                                                                  'up_sampling2d_11[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_25 (Sequential)     (None, 8, 8, 256)    264704      ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_16 (UpSampling2D  (None, 8, 8, 512)   0           ['sequential_26[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 32, 32, 192)  0           ['sequential_18[0][0]',          \n",
      "                                                                  'up_sampling2d_10[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_24 (Sequential)     (None, 16, 16, 128)  66816       ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_15 (UpSampling2D  (None, 16, 16, 256)  0          ['sequential_25[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 8, 8, 1024)   0           ['sequential_20[0][0]',          \n",
      "                                                                  'sequential_25[0][0]',          \n",
      "                                                                  'up_sampling2d_16[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_23 (Sequential)     (None, 32, 32, 64)   17024       ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_14 (UpSampling2D  (None, 32, 32, 128)  0          ['sequential_24[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 16, 16, 512)  0           ['sequential_19[0][0]',          \n",
      "                                                                  'sequential_24[0][0]',          \n",
      "                                                                  'up_sampling2d_15[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_29 (Sequential)     (None, 8, 8, 256)    330240      ['concatenate_16[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 32, 32, 256)  0           ['sequential_18[0][0]',          \n",
      "                                                                  'sequential_23[0][0]',          \n",
      "                                                                  'up_sampling2d_14[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_28 (Sequential)     (None, 16, 16, 128)  83200       ['concatenate_15[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_18 (UpSampling2D  (None, 16, 16, 256)  0          ['sequential_29[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " sequential_27 (Sequential)     (None, 32, 32, 64)   21120       ['concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_17 (UpSampling2D  (None, 32, 32, 128)  0          ['sequential_28[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 16, 16, 640)  0           ['sequential_19[0][0]',          \n",
      "                                                                  'sequential_24[0][0]',          \n",
      "                                                                  'sequential_28[0][0]',          \n",
      "                                                                  'up_sampling2d_18[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 32, 32, 320)  0           ['sequential_18[0][0]',          \n",
      "                                                                  'sequential_23[0][0]',          \n",
      "                                                                  'sequential_27[0][0]',          \n",
      "                                                                  'up_sampling2d_17[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_31 (Sequential)     (None, 16, 16, 128)  99584       ['concatenate_18[0][0]']         \n",
      "                                                                                                  \n",
      " sequential_30 (Sequential)     (None, 32, 32, 64)   25216       ['concatenate_17[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_19 (UpSampling2D  (None, 32, 32, 128)  0          ['sequential_31[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 32, 32, 384)  0           ['sequential_18[0][0]',          \n",
      "                                                                  'sequential_23[0][0]',          \n",
      "                                                                  'sequential_27[0][0]',          \n",
      "                                                                  'sequential_30[0][0]',          \n",
      "                                                                  'up_sampling2d_19[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_32 (Sequential)     (None, 32, 32, 64)   29312       ['concatenate_19[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 32, 32, 2)    130         ['sequential_32[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,104,130\n",
      "Trainable params: 4,089,538\n",
      "Non-trainable params: 14,592\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating the model\n",
    "modelA = unet_plus_plus_model(input_shape=(\n",
    "\timage_size[0], image_size[1], channels), deep_supervision=False)\n",
    "modelA.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='binary_crossentropy', metrics=['acc'])\n",
    "modelA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# historyA = modelA.fit(train_generator, epochs=10, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Graphing Result of training\n",
    "# plt.figure(figsize=(15,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyA.history['mae'], color ='r')\n",
    "# plt.plot(historyA.history['val_mae'])\n",
    "# plt.title('mae')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mae')\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyA.history['loss'], color ='r')\n",
    "# plt.plot(historyA.history['val_loss'])\n",
    "# plt.title('loss')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mean squared error')\n",
    "\n",
    "# # displaying the figure\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Best model saved\n",
    "# train_loss = historyA.history['loss']\n",
    "# train_mae = historyA.history['mae']\n",
    "# val_loss = historyA.history['val_loss']\n",
    "# val_mae = historyA.history['val_mae']\n",
    "\n",
    "# best_epoch = np.argmin(np.array(val_loss))\n",
    "\n",
    "# print(\"Best Epoch: \", best_epoch)\n",
    "# print(\"Final Train Loss: {:.4f}\".format(train_loss[best_epoch]))\n",
    "# print(\"Final Validation Loss: {:.4f}\".format(val_loss[best_epoch]))\n",
    "# print(\"Final Train MAE: {:.2f}\".format(train_mae[best_epoch]*100))\n",
    "# print(\"Final Test MAE: {:.2f}\".format(val_mae[best_epoch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design B: SegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segnet(input_shape):\n",
    "\n",
    "    # Encoding layer\n",
    "    img_input = Input(shape= input_shape)\n",
    "    x = Conv2D(64, (3, 3), padding='same', name='conv1',strides= (1,1))(img_input)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', name='conv2')(x)\n",
    "    x = BatchNormalization(name='bn2')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), padding='same', name='conv3')(x)\n",
    "    x = BatchNormalization(name='bn3')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', name='conv4')(x)\n",
    "    x = BatchNormalization(name='bn4')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='conv5')(x)\n",
    "    x = BatchNormalization(name='bn5')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='conv6')(x)\n",
    "    x = BatchNormalization(name='bn6')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='conv7')(x)\n",
    "    x = BatchNormalization(name='bn7')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv8')(x)\n",
    "    x = BatchNormalization(name='bn8')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv9')(x)\n",
    "    x = BatchNormalization(name='bn9')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv10')(x)\n",
    "    x = BatchNormalization(name='bn10')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    \n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv11')(x)\n",
    "    x = BatchNormalization(name='bn11')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv12')(x)\n",
    "    x = BatchNormalization(name='bn12')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv13')(x)\n",
    "    x = BatchNormalization(name='bn13')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Dense(1024, activation = 'relu', name='fc1')(x)\n",
    "    x = Dense(1024, activation = 'relu', name='fc2')(x)\n",
    "    # Decoding Layer \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv1')(x)\n",
    "    x = BatchNormalization(name='bn14')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv2')(x)\n",
    "    x = BatchNormalization(name='bn15')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv3')(x)\n",
    "    x = BatchNormalization(name='bn16')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv4')(x)\n",
    "    x = BatchNormalization(name='bn17')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv5')(x)\n",
    "    x = BatchNormalization(name='bn18')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv6')(x)\n",
    "    x = BatchNormalization(name='bn19')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv7')(x)\n",
    "    x = BatchNormalization(name='bn20')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv8')(x)\n",
    "    x = BatchNormalization(name='bn21')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(128, (3, 3), padding='same', name='deconv9')(x)\n",
    "    x = BatchNormalization(name='bn22')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(128, (3, 3), padding='same', name='deconv10')(x)\n",
    "    x = BatchNormalization(name='bn23')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), padding='same', name='deconv11')(x)\n",
    "    x = BatchNormalization(name='bn24')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), padding='same', name='deconv12')(x)\n",
    "    x = BatchNormalization(name='bn25')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(1, (3, 3), padding='same', name='deconv13')(x)\n",
    "    x = BatchNormalization(name='bn26')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    # pred = Reshape((192,256))(x)\n",
    "    # x = Flatten()(x)\n",
    "    # x=Dense(1024, activation='relu')(x)\n",
    "    # otuput=Dense(4, activation='softmax')(x)\n",
    "    outputs = tf.keras.layers.Conv2D(2, 1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=img_input, outputs=outputs)\n",
    "    \n",
    "    model.compile(optimizer= tf.keras.optimizers.Adam(lr=lr), loss= [\"binary_crossentropy\"]\n",
    "                  , metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " bn1 (BatchNormalization)    (None, 32, 32, 64)        256       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " bn2 (BatchNormalization)    (None, 32, 32, 64)        256       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " bn3 (BatchNormalization)    (None, 16, 16, 128)       512       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv4 (Conv2D)              (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " bn4 (BatchNormalization)    (None, 16, 16, 128)       512       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv5 (Conv2D)              (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " bn5 (BatchNormalization)    (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv6 (Conv2D)              (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn6 (BatchNormalization)    (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv7 (Conv2D)              (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn7 (BatchNormalization)    (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 4, 4, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv8 (Conv2D)              (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " bn8 (BatchNormalization)    (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv9 (Conv2D)              (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn9 (BatchNormalization)    (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv10 (Conv2D)             (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn10 (BatchNormalization)   (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 2, 2, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv11 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn11 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv12 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn12 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv13 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn13 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 1, 1, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 1, 1, 1024)        525312    \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 1, 1, 1024)        1049600   \n",
      "                                                                 \n",
      " up_sampling2d_10 (UpSamplin  (None, 2, 2, 1024)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv1 (Conv2DTranspose)   (None, 2, 2, 512)         4719104   \n",
      "                                                                 \n",
      " bn14 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " deconv2 (Conv2DTranspose)   (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn15 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " deconv3 (Conv2DTranspose)   (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn16 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " up_sampling2d_11 (UpSamplin  (None, 4, 4, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv4 (Conv2DTranspose)   (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn17 (BatchNormalization)   (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " deconv5 (Conv2DTranspose)   (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn18 (BatchNormalization)   (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " deconv6 (Conv2DTranspose)   (None, 4, 4, 256)         1179904   \n",
      "                                                                 \n",
      " bn19 (BatchNormalization)   (None, 4, 4, 256)         1024      \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_12 (UpSamplin  (None, 8, 8, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv7 (Conv2DTranspose)   (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn20 (BatchNormalization)   (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " deconv8 (Conv2DTranspose)   (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn21 (BatchNormalization)   (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " deconv9 (Conv2DTranspose)   (None, 8, 8, 128)         295040    \n",
      "                                                                 \n",
      " bn22 (BatchNormalization)   (None, 8, 8, 128)         512       \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " up_sampling2d_13 (UpSamplin  (None, 16, 16, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv10 (Conv2DTranspose)  (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " bn23 (BatchNormalization)   (None, 16, 16, 128)       512       \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " deconv11 (Conv2DTranspose)  (None, 16, 16, 64)        73792     \n",
      "                                                                 \n",
      " bn24 (BatchNormalization)   (None, 16, 16, 64)        256       \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " up_sampling2d_14 (UpSamplin  (None, 32, 32, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv12 (Conv2DTranspose)  (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " bn25 (BatchNormalization)   (None, 32, 32, 64)        256       \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " deconv13 (Conv2DTranspose)  (None, 32, 32, 1)         577       \n",
      "                                                                 \n",
      " bn26 (BatchNormalization)   (None, 32, 32, 1)         4         \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 32, 32, 2)         4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,393,673\n",
      "Trainable params: 33,377,799\n",
      "Non-trainable params: 15,874\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelB = segnet(input_shape=image_size+(3,))\n",
    "modelB.summary()\n",
    "\n",
    "\n",
    "# model.save(savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = modelB.fit(train_generator, epochs= 10, validation_data= val_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelB = segnet(input_shape=image_size+(3,))\n",
    "# modelB.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mse', metrics=['mae'])\n",
    "# modelB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## MANUFACTURABILITY: TRAINING TIME\n",
    "# import time \n",
    "# start = time.time()\n",
    "# historyB = modelB.fit(train_generator, epochs=10, validation_data=val_generator, verbose=0)\n",
    "# stop = time.time()\n",
    "# print(f\"Training time: {stop - start}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Graphing Result of training\n",
    "# plt.figure(figsize=(15,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyB.history['mae'], color ='r')\n",
    "# plt.plot(historyB.history['val_mae'])\n",
    "# plt.title('mae')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mae')\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyB.history['loss'], color ='r')\n",
    "# plt.plot(historyB.history['val_loss'])\n",
    "# plt.title('loss')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mean squared error')\n",
    "\n",
    "# # displaying the figure\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Best model saved\n",
    "# train_loss = historyB.history['loss']\n",
    "# train_mae = historyB.history['mae']\n",
    "# val_loss = historyB.history['val_loss']\n",
    "# val_mae = historyB.history['val_mae']\n",
    "\n",
    "# best_epoch = np.argmin(np.array(val_loss))\n",
    "\n",
    "# print(\"Best Epoch: \", best_epoch)\n",
    "# print(\"Final Train Loss: {:.4f}\".format(train_loss[best_epoch]))\n",
    "# print(\"Final Validation Loss: {:.4f}\".format(val_loss[best_epoch]))\n",
    "# print(\"Final Train MAE: {:.2f}\".format(train_mae[best_epoch]*100))\n",
    "# print(\"Final Test MAE: {:.2f}\".format(val_mae[best_epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design C: DeepLabv3+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_block(\n",
    "    block_input,\n",
    "    num_filters=256,\n",
    "    kernel_size=3,\n",
    "    dilation_rate=1,\n",
    "    padding=\"same\",\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding=\"same\",\n",
    "        use_bias=use_bias,\n",
    "        kernel_initializer=keras.initializers.HeNormal(),\n",
    "    )(block_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def DilatedSpatialPyramidPooling(dspp_input):\n",
    "    dims = dspp_input.shape\n",
    "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "    out_pool = layers.UpSampling2D(\n",
    "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "\n",
    "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
    "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
    "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
    "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "    output = convolution_block(x, kernel_size=1)\n",
    "    return output\n",
    "\n",
    "def DeeplabV3Plus(image_size):\n",
    "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    resnet50 = keras.applications.ResNet50(\n",
    "        weights=None, include_top=False, input_tensor=model_input\n",
    "    )\n",
    "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
    "    x = DilatedSpatialPyramidPooling(x)\n",
    "\n",
    "    input_a = layers.UpSampling2D(\n",
    "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "    x = convolution_block(x)\n",
    "    x = convolution_block(x)\n",
    "    x = layers.UpSampling2D(\n",
    "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    # x = layers.GlobalAveragePooling2D()(x)\n",
    "    # x= Dense(1024, activation=\"relu\",)(x)\n",
    "    # model_output = layers.Dense(4, activation='softmax')(x)\n",
    "    outputs = tf.keras.layers.Conv2D(2, 1, activation='sigmoid')(x)\n",
    "    return keras.Model(inputs=model_input, outputs=outputs)\n",
    "\n",
    "\n",
    "modelC = DeeplabV3Plus(image_size=image_size[0])\n",
    "modelC.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelC.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# historyC = modelC.fit(train_generator, epochs=10, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Graphing Result of training\n",
    "# plt.figure(figsize=(15,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyC.history['mae'], color ='r')\n",
    "# plt.plot(historyC.history['val_mae'])\n",
    "# plt.title('mae')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mae')\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyC.history['loss'], color ='r')\n",
    "# plt.plot(historyC.history['val_loss'])\n",
    "# plt.title('loss')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mean squared error')\n",
    "\n",
    "# # displaying the figure\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Best model saved\n",
    "# train_loss = historyC.history['loss']\n",
    "# train_mae = historyC.history['mae']\n",
    "# val_loss = historyC.history['val_loss']\n",
    "# val_mae = historyC.history['val_mae']\n",
    "\n",
    "# best_epoch = np.argmin(np.array(val_loss))\n",
    "\n",
    "# print(\"Best Epoch: \", best_epoch)\n",
    "# print(\"Final Train Loss: {:.4f}\".format(train_loss[best_epoch]))\n",
    "# print(\"Final Validation Loss: {:.4f}\".format(val_loss[best_epoch]))\n",
    "# print(\"Final Train MAE: {:.2f}\".format(train_mae[best_epoch]*100))\n",
    "# print(\"Final Test MAE: {:.2f}\".format(val_mae[best_epoch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Designs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of Designs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Convolutional Block\n",
    "def conv_block(inputs, num_filters):\n",
    "\t# Applying the sequence of Convolutional, Batch Normalization\n",
    "\t# and Activation Layers to the input tensor\n",
    "\tx = Sequential([\n",
    "\t\t# Convolutional Layer\n",
    "\t\tConv2D(num_filters, 1, padding='same'),\n",
    "\t\t# Batch Normalization Layer\n",
    "\t\tBatchNormalization(),\n",
    "\t\t# Activation Layer\n",
    "\t\tReLU(),\n",
    "\t\t# Convolutional Layer\n",
    "\t\tConv2D(num_filters, 1, padding='same'),\n",
    "\t\t# Batch Normalization Layer\n",
    "\t\tBatchNormalization(),\n",
    "\t\t# Activation Layer\n",
    "\t\tReLU()\n",
    "\t])(inputs)\n",
    "\n",
    "\t# Returning the output of the Convolutional Block\n",
    "\treturn x\n",
    "def dense_block(units, dropout_rate):\n",
    "    return Sequential([\n",
    "        Dense(units, activation='relu'),\n",
    "    ])\n",
    "# Defining the Unet++ Model\n",
    "def unet_plus_plus_model(hp):\n",
    "\tinputs = Input(shape=image_size+(3,))\n",
    "\thp_filters = hp.Choice('filters',values = [16,32,64])\n",
    "\t# Encoding Path\n",
    "\tx_00 = conv_block(inputs, hp_filters)\n",
    "\tx_10 = conv_block(MaxPooling2D()(x_00), hp_filters*2)\n",
    "\tx_20 = conv_block(MaxPooling2D()(x_10), hp_filters*4)\n",
    "\tx_30 = conv_block(MaxPooling2D()(x_20), hp_filters*8)\n",
    "\tx_40 = conv_block(MaxPooling2D()(x_30), hp_filters*16)\n",
    "\t\n",
    "\thp.Boolean(\"dropouts\", default=False)\n",
    "\thp.Boolean(\"batch_normalization\", default=False)\n",
    "\tflattened = Flatten()(x_40)\n",
    "\tdense = dense_block(hp_filters*hp_filters, 0.2)(flattened)\n",
    "\tif hp.Boolean(\"dropouts\"):\n",
    "\t\tdense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\tif hp.Boolean(\"batch_normalization\"):\n",
    "\t\tdense = tf.keras.layers.BatchNormalization()(dense)\n",
    "\tdense = dense_block(hp_filters*hp_filters, 0.2)(dense)\n",
    "\tif hp.Boolean(\"dropouts\"):\n",
    "\t\tdense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\tif hp.Boolean(\"batch_normalization\"):\n",
    "\t\tdense = tf.keras.layers.BatchNormalization()(dense)\n",
    "\tdense = dense_block(hp_filters*hp_filters, 0.2)(dense);hp.Boolean(\"4th dense\", default=False);\n",
    "\tif hp.Boolean(\"4th dense\"):\n",
    "\t\tdense = dense_block(hp_filters*hp_filters, 0.2)(dense)\n",
    "\t\tif hp.Boolean(\"dropouts\"):\n",
    "\t\t\tdense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\t\tif hp.Boolean(\"batch_normalization\"):\n",
    "\t\t\tdense = tf.keras.layers.BatchNormalization()(dense)\n",
    "\t# dense = dense_block(4096, 0.2)(dense)\n",
    "\t# dense = dense_block(4096, 0.2)(dense)\n",
    "\treshaped = tf.keras.layers.Reshape((x_40.shape[1], x_40.shape[1], (hp_filters*hp_filters)//(x_40.shape[1]*x_40.shape[1])))(dense)  # Reshape to reintroduce spatial dimensions\n",
    "\n",
    "\n",
    "\t# Nested Decoding Path\n",
    "\tx_01 = conv_block(concatenate(\n",
    "\t\t[x_00, UpSampling2D()(x_10)]), hp_filters)\n",
    "\tx_11 = conv_block(concatenate(\n",
    "\t\t[x_10, UpSampling2D()(x_20)]), hp_filters*2)\n",
    "\tx_21 = conv_block(concatenate(\n",
    "\t\t[x_20, UpSampling2D()(x_30)]), hp_filters*4)\n",
    "\tx_31 = conv_block(concatenate(\n",
    "\t\t[x_30, UpSampling2D()(reshaped)]), hp_filters*8)\n",
    "\n",
    "\tx_02 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, UpSampling2D()(x_11)]), hp_filters)\n",
    "\tx_12 = conv_block(concatenate(\n",
    "\t\t[x_10, x_11, UpSampling2D()(x_21)]), hp_filters*2)\n",
    "\tx_22 = conv_block(concatenate(\n",
    "\t\t[x_20, x_21, UpSampling2D()(x_31)]), hp_filters*4)\n",
    "\n",
    "\tx_03 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, x_02, UpSampling2D()(x_12)]), hp_filters)\n",
    "\tx_13 = conv_block(concatenate(\n",
    "\t\t[x_10, x_11, x_12, UpSampling2D()(x_22)]), hp_filters*2)\n",
    "\n",
    "\tx_04 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), hp_filters)\n",
    "\t\n",
    "\toutputs = tf.keras.layers.Conv2D(2, 1, activation='sigmoid')(x_04);print(outputs.shape)\n",
    "\n",
    "\t# Creating the model\n",
    "\tmodel = tf.keras.Model(\n",
    "\t\tinputs=inputs, outputs=outputs, name='Unet_plus_plus');lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5]);model.compile(optimizer= tf.keras.optimizers.Adam(lr=lr), loss= [\"binary_crossentropy\"], metrics=['acc'])\n",
    "\t# Returning the model\n",
    "\treturn model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Dropout, Concatenate, Flatten, Dense, Reshape\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "def convolution_block(inputs, num_filters, kernel_size=3, padding=\"same\", use_bias=False):\n",
    "    x = Conv2D(num_filters, kernel_size=kernel_size, padding=padding, use_bias=use_bias, kernel_initializer=HeNormal())(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def upsample_block(inputs, skip_features, num_filters):\n",
    "    x = UpSampling2D((2, 2))(inputs)\n",
    "    x = Conv2D(num_filters, (2, 2), padding=\"same\")(x)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = convolution_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def dense_upsampling_block(inputs, skip_features, num_filters):\n",
    "    x = UpSampling2D((2, 2))(inputs)\n",
    "    x = Conv2D(num_filters, (2, 2), padding=\"same\")(x)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = convolution_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def unet_plus_plus_model(hp,input_shape=(64,64,3)):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    resnet50 = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    resnet50.trainable = True\n",
    "\n",
    "    # Encoder\n",
    "    s1 = resnet50.get_layer(\"conv1_relu\").output\n",
    "    s2 = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "    s3 = resnet50.get_layer(\"conv3_block4_out\").output\n",
    "    s4 = resnet50.get_layer(\"conv4_block6_out\").output\n",
    "\n",
    "    b1 = resnet50.get_layer(\"conv5_block3_out\").output\n",
    "    # Flatten the bottleneck output\n",
    "    x = Flatten()(b1)\n",
    "\tUNITS = hp.Choice('units',values = [256,512,1024,2048])\n",
    "    \n",
    "    # Dense layers between encoder and decoder\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    # Reshape back to spatial dimensions for the decoder\n",
    "    SIZE = input_shape[0] // 32  # Assuming input size is a multiple of 32\n",
    "    x = Dense(SIZE * SIZE * 2048, activation='relu')(x)\n",
    "    x = Reshape((SIZE, SIZE, 2048))(x)\n",
    "\n",
    "    # Nested U-Net\n",
    "    d4_2 = dense_upsampling_block(x, s4, 512)\n",
    "    d3_2 = dense_upsampling_block(d4_2, s3, 256)\n",
    "    d2_2 = dense_upsampling_block(d3_2, s2, 128)\n",
    "    d1_2 = dense_upsampling_block(d2_2, s1, 64)\n",
    "\n",
    "    d4_1 = upsample_block(x, s4, 512)\n",
    "    d3_1 = upsample_block(d4_1, s3, 256)\n",
    "    d2_1 = upsample_block(d3_1, s2, 128)\n",
    "    d1_1 = upsample_block(d2_1, s1, 64)\n",
    "\n",
    "    outputs = UpSampling2D()(d1_1)\n",
    "    outputs = Conv2D(2, (1, 1), padding=\"same\", activation=\"sigmoid\")(outputs)\n",
    "    model = tf.keras.Model(\n",
    "\t\tinputs=inputs, outputs=outputs, name='Unet_plus_plus');lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5]);model.compile(optimizer= tf.keras.optimizers.Adam(lr=lr), loss= [\"binary_crossentropy\"], metrics=['acc'])\n",
    "\t# Returning the model\n",
    "    return model\n",
    "\n",
    "# # Define input shape and build model\n",
    "# input_shape = (256, 256, 3)\n",
    "# model = build_resnet50_unetpp(input_shape)\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# # Print model summary\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the convolutional block\n",
    "# def conv_block(inputs, num_filters):\n",
    "#     x = Sequential([\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU(),\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU()\n",
    "#     ])(inputs)\n",
    "#     return x\n",
    "\n",
    "# # Define the dense block\n",
    "# def dense_block(units, dropout_rate):\n",
    "#     return Sequential([\n",
    "#         Dense(units, activation='relu'),\n",
    "#         Dropout(dropout_rate)\n",
    "#     ])\n",
    "\n",
    "# # Define the Unet++ model\n",
    "# def unet_plus_plus_model(hp, image_size=(224, 224)):\n",
    "#     inputs = Input(shape=image_size + (3,))\n",
    "\n",
    "#     # Load ResNet50 as encoder\n",
    "#     resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "#     encoder_layers = [resnet.get_layer(name).output for name in ['conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block6_out', 'conv5_block3_out']]\n",
    "    \n",
    "#     # Encoding path\n",
    "#     x_00 = encoder_layers[0]\n",
    "#     x_10 = encoder_layers[1]\n",
    "#     x_20 = encoder_layers[2]\n",
    "#     x_30 = encoder_layers[3]\n",
    "#     x_40 = encoder_layers[4]\n",
    "\n",
    "#     flattened = Flatten()(x_40)\n",
    "#     dense = dense_block(2048, 0.2)(flattened)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(2048, 0.2)(dense)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(2048, 0.2)(dense)\n",
    "#     if hp.Boolean(\"4th_dense\"):\n",
    "#         dense = dense_block(2048, 0.2)(dense)\n",
    "#         if hp.Boolean(\"dropouts\"):\n",
    "#             dense = Dropout(0.5)(dense)\n",
    "#         if hp.Boolean(\"batch_normalization\"):\n",
    "#             dense = BatchNormalization()(dense)\n",
    "\n",
    "#     reshaped = Reshape((x_40.shape[1], x_40.shape[2], 32))(dense)\n",
    "\n",
    "#     # Nested decoding path\n",
    "#     x_01 = conv_block(concatenate([x_00, UpSampling2D()(x_10)]), 64)\n",
    "#     x_11 = conv_block(concatenate([x_10, UpSampling2D()(x_20)]), 128)\n",
    "#     x_21 = conv_block(concatenate([x_20, UpSampling2D()(x_30)]), 256)\n",
    "#     x_31 = conv_block(concatenate([x_30, UpSampling2D()(reshaped)]), 512)\n",
    "\n",
    "#     x_02 = conv_block(concatenate([x_00, x_01, UpSampling2D()(x_11)]), 64)\n",
    "#     x_12 = conv_block(concatenate([x_10, x_11, UpSampling2D()(x_21)]), 128)\n",
    "#     x_22 = conv_block(concatenate([x_20, x_21, UpSampling2D()(x_31)]), 256)\n",
    "\n",
    "#     x_03 = conv_block(concatenate([x_00, x_01, x_02, UpSampling2D()(x_12)]), 64)\n",
    "#     x_13 = conv_block(concatenate([x_10, x_11, x_12, UpSampling2D()(x_22)]), 128)\n",
    "\n",
    "#     x_04 = conv_block(concatenate([x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), 64)\n",
    "\n",
    "#     outputs = Conv2D(2, 1, activation='sigmoid')(x_04)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, ReLU, Flatten, Dense, Dropout, concatenate, Reshape\n",
    "# import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "\n",
    "# # Define the convolutional block\n",
    "# def conv_block(inputs, num_filters):\n",
    "#     x = Sequential([\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU(),\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU()\n",
    "#     ])(inputs)\n",
    "#     return x\n",
    "\n",
    "# # Define the dense block\n",
    "# def dense_block(units, dropout_rate):\n",
    "#     return Sequential([\n",
    "#         Dense(units, activation='relu'),\n",
    "#         Dropout(dropout_rate)\n",
    "#     ])\n",
    "\n",
    "# # Define the Unet++ model\n",
    "# def unet_plus_plus_model(hp, image_size=(32, 32)):\n",
    "#     inputs = Input(shape=image_size + (3,))\n",
    "\n",
    "#     # Load ResNet50 as encoder\n",
    "#     resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "#     encoder_layers = [resnet.get_layer(name).output for name in ['conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block6_out', 'conv5_block3_out']]\n",
    "    \n",
    "#     # Encoding path\n",
    "#     x_00 = encoder_layers[0]\n",
    "#     x_10 = encoder_layers[1]\n",
    "#     x_20 = encoder_layers[2]\n",
    "#     x_30 = encoder_layers[3]\n",
    "#     x_40 = encoder_layers[4]\n",
    "#     print(x_40.shape)\n",
    "#     flattened = Flatten()(x_40)\n",
    "#     dense = dense_block(2048, 0.2)(flattened)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(2048, 0.2)(dense)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(2048, 0.2)(dense)\n",
    "#     if hp.Boolean(\"4th_dense\"):\n",
    "#         dense = dense_block(2048, 0.2)(dense)\n",
    "#         if hp.Boolean(\"dropouts\"):\n",
    "#             dense = Dropout(0.5)(dense)\n",
    "#         if hp.Boolean(\"batch_normalization\"):\n",
    "#             dense = BatchNormalization()(dense)\n",
    "#     reshaped = tf.keras.layers.Reshape((x_40.shape[1], x_40.shape[1], 2048))(dense)  # Reshape to reintroduce spatial dimensions\n",
    "\n",
    "#     # Nested decoding path\n",
    "#     x_01 = conv_block(concatenate([x_00, UpSampling2D()(x_10)]), 64)\n",
    "#     x_11 = conv_block(concatenate([x_10, UpSampling2D()(x_20)]), 128)\n",
    "#     x_21 = conv_block(concatenate([x_20, UpSampling2D()(x_30)]), 256)\n",
    "#     x_31 = conv_block(concatenate([x_30, UpSampling2D()(reshaped)]), 512)\n",
    "\n",
    "#     x_02 = conv_block(concatenate([x_00, x_01, UpSampling2D()(x_11)]), 64)\n",
    "#     x_12 = conv_block(concatenate([x_10, x_11, UpSampling2D()(x_21)]), 128)\n",
    "#     x_22 = conv_block(concatenate([x_20, x_21, UpSampling2D()(x_31)]), 256)\n",
    "\n",
    "#     x_03 = conv_block(concatenate([x_00, x_01, x_02, UpSampling2D()(x_12)]), 64)\n",
    "#     x_13 = conv_block(concatenate([x_10, x_11, x_12, UpSampling2D()(x_22)]), 128)\n",
    "\n",
    "#     x_04 = conv_block(concatenate([x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), 64)\n",
    "    \n",
    "#     x_final = UpSampling2D(size=(2, 2))(x_04)\n",
    "\n",
    "#     outputs = Conv2D(2, 1, activation='sigmoid')(x_final)\n",
    "#     print(outputs.shape)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the convolutional block\n",
    "# def conv_block(inputs, num_filters):\n",
    "#     x = Sequential([\n",
    "#         Conv2D(num_filters, 1, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU(),\n",
    "#         Conv2D(num_filters, 1, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU()\n",
    "#     ])(inputs)\n",
    "#     return x\n",
    "\n",
    "# # Define the dense block\n",
    "# def dense_block(units, dropout_rate):\n",
    "#     return Sequential([\n",
    "#         Dense(units, activation='relu')\n",
    "#     ])\n",
    "\n",
    "# # Define the Unet++ model\n",
    "# def unet_plus_plus_model(hp):\n",
    "#     inputs = Input(shape=(32, 32, 3))\n",
    "#     hp_filters = hp.Choice('filters', values=[16, 32, 64,])\n",
    "\n",
    "#     # Encoding Path\n",
    "#     x_00 = conv_block(inputs, hp_filters)\n",
    "#     x_10 = conv_block(MaxPooling2D()(x_00), hp_filters * 2)\n",
    "#     x_20 = conv_block(MaxPooling2D()(x_10), hp_filters * 4)\n",
    "#     x_30 = conv_block(MaxPooling2D()(x_20), hp_filters * 8)\n",
    "#     x_40 = conv_block(MaxPooling2D()(x_30), hp_filters * 16)\n",
    "\n",
    "#     # Dense layers and reshape\n",
    "#     flattened = Flatten()(x_40)\n",
    "#     dense = dense_block(hp_filters * hp_filters, 0.2)(flattened)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(hp_filters * hp_filters, 0.2)(dense)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(hp_filters * hp_filters, 0.2)(dense)\n",
    "#     if hp.Boolean(\"4th_dense\"):\n",
    "#         dense = dense_block(hp_filters * hp_filters, 0.2)(dense)\n",
    "#         if hp.Boolean(\"dropouts\"):\n",
    "#             dense = Dropout(0.5)(dense)\n",
    "#         if hp.Boolean(\"batch_normalization\"):\n",
    "#             dense = BatchNormalization()(dense)\n",
    "\n",
    "#     reshaped = tf.keras.layers.Reshape((x_40.shape[1], x_40.shape[1], (hp_filters * hp_filters) // (x_40.shape[1] * x_40.shape[1])))(dense)\n",
    "\n",
    "#     # Nested Decoding Path\n",
    "#     x_01 = conv_block(concatenate([x_00, UpSampling2D()(x_10)]), hp_filters)\n",
    "#     x_11 = conv_block(concatenate([x_10, UpSampling2D()(x_20)]), hp_filters * 2)\n",
    "#     x_21 = conv_block(concatenate([x_20, UpSampling2D()(x_30)]), hp_filters * 4)\n",
    "#     x_31 = conv_block(concatenate([x_30, UpSampling2D()(reshaped)]), hp_filters * 8)\n",
    "\n",
    "#     x_02 = conv_block(concatenate([x_00, x_01, UpSampling2D()(x_11)]), hp_filters)\n",
    "#     x_12 = conv_block(concatenate([x_10, x_11, UpSampling2D()(x_21)]), hp_filters * 2)\n",
    "#     x_22 = conv_block(concatenate([x_20, x_21, UpSampling2D()(x_31)]), hp_filters * 4)\n",
    "\n",
    "#     x_03 = conv_block(concatenate([x_00, x_01, x_02, UpSampling2D()(x_12)]), hp_filters)\n",
    "#     x_13 = conv_block(concatenate([x_10, x_11, x_12, UpSampling2D()(x_22)]), hp_filters * 2)\n",
    "\n",
    "#     x_04 = conv_block(concatenate([x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), hp_filters)\n",
    "\n",
    "#     # Flattening the final output\n",
    "#     flat_output = Flatten()(x_04)\n",
    "\n",
    "#     # Adding Dense layers\n",
    "#     dense_output = Dense(hp_filters * hp_filters, activation='relu')(flat_output)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense_output = Dropout(0.5)(dense_output)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense_output = BatchNormalization()(dense_output)\n",
    "\n",
    "#     # Final Dense layer with sigmoid activation for binary classification\n",
    "#     outputs = Dense(2, activation='sigmoid')(dense_output)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 05m 44s]\n",
      "val_loss: 0.31658250093460083\n",
      "\n",
      "Best val_loss So Far: 0.31658250093460083\n",
      "Total elapsed time: 00h 12m 37s\n",
      "{'learning_rate': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "tunerA = kt.BayesianOptimization(unet_plus_plus_model,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 10,\n",
    "                     project_name='design_a',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8)\n",
    "\n",
    "tunerA.search(train_generator, epochs=100, validation_data=val_generator, callbacks=[stop_early, reduce_lr])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsA=tunerA.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "print(best_hpsA.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hpsA.values['filters']=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 2)\n",
      "Model: \"Unet_plus_plus\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential_18 (Sequential)     (None, 64, 64, 32)   1440        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 32)  0           ['sequential_18[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_19 (Sequential)     (None, 32, 32, 64)   6784        ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 64)  0           ['sequential_19[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_20 (Sequential)     (None, 16, 16, 128)  25856       ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 8, 8, 128)   0           ['sequential_20[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_21 (Sequential)     (None, 8, 8, 256)    100864      ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 4, 4, 256)   0           ['sequential_21[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_22 (Sequential)     (None, 4, 4, 512)    398336      ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 8192)         0           ['sequential_22[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_23 (Sequential)     (None, 1024)         8389632     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " sequential_24 (Sequential)     (None, 1024)         1049600     ['sequential_23[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_25 (Sequential)     (None, 1024)         1049600     ['sequential_24[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_26 (Sequential)     (None, 1024)         1049600     ['sequential_25[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 4, 4, 64)     0           ['sequential_26[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_13 (UpSampling2D  (None, 8, 8, 64)    0           ['reshape_1[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_12 (UpSampling2D  (None, 16, 16, 256)  0          ['sequential_21[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 8, 8, 320)    0           ['sequential_21[0][0]',          \n",
      "                                                                  'up_sampling2d_13[0][0]']       \n",
      "                                                                                                  \n",
      " up_sampling2d_11 (UpSampling2D  (None, 32, 32, 128)  0          ['sequential_20[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 16, 16, 384)  0           ['sequential_20[0][0]',          \n",
      "                                                                  'up_sampling2d_12[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_30 (Sequential)     (None, 8, 8, 256)    150016      ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_10 (UpSampling2D  (None, 64, 64, 64)  0           ['sequential_19[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 32, 32, 192)  0           ['sequential_19[0][0]',          \n",
      "                                                                  'up_sampling2d_11[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_29 (Sequential)     (None, 16, 16, 128)  66816       ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_16 (UpSampling2D  (None, 16, 16, 256)  0          ['sequential_30[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 64, 64, 96)   0           ['sequential_18[0][0]',          \n",
      "                                                                  'up_sampling2d_10[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_28 (Sequential)     (None, 32, 32, 64)   17024       ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_15 (UpSampling2D  (None, 32, 32, 128)  0          ['sequential_29[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 16, 16, 512)  0           ['sequential_20[0][0]',          \n",
      "                                                                  'sequential_29[0][0]',          \n",
      "                                                                  'up_sampling2d_16[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_27 (Sequential)     (None, 64, 64, 32)   4416        ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_14 (UpSampling2D  (None, 64, 64, 64)  0           ['sequential_28[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 32, 32, 256)  0           ['sequential_19[0][0]',          \n",
      "                                                                  'sequential_28[0][0]',          \n",
      "                                                                  'up_sampling2d_15[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_33 (Sequential)     (None, 16, 16, 128)  83200       ['concatenate_16[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 64, 64, 128)  0           ['sequential_18[0][0]',          \n",
      "                                                                  'sequential_27[0][0]',          \n",
      "                                                                  'up_sampling2d_14[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_32 (Sequential)     (None, 32, 32, 64)   21120       ['concatenate_15[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_18 (UpSampling2D  (None, 32, 32, 128)  0          ['sequential_33[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " sequential_31 (Sequential)     (None, 64, 64, 32)   5440        ['concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_17 (UpSampling2D  (None, 64, 64, 64)  0           ['sequential_32[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 32, 32, 320)  0           ['sequential_19[0][0]',          \n",
      "                                                                  'sequential_28[0][0]',          \n",
      "                                                                  'sequential_32[0][0]',          \n",
      "                                                                  'up_sampling2d_18[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 64, 64, 160)  0           ['sequential_18[0][0]',          \n",
      "                                                                  'sequential_27[0][0]',          \n",
      "                                                                  'sequential_31[0][0]',          \n",
      "                                                                  'up_sampling2d_17[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_35 (Sequential)     (None, 32, 32, 64)   25216       ['concatenate_18[0][0]']         \n",
      "                                                                                                  \n",
      " sequential_34 (Sequential)     (None, 64, 64, 32)   6464        ['concatenate_17[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_19 (UpSampling2D  (None, 64, 64, 64)  0           ['sequential_35[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 64, 64, 192)  0           ['sequential_18[0][0]',          \n",
      "                                                                  'sequential_27[0][0]',          \n",
      "                                                                  'sequential_31[0][0]',          \n",
      "                                                                  'sequential_34[0][0]',          \n",
      "                                                                  'up_sampling2d_19[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_36 (Sequential)     (None, 64, 64, 32)   7488        ['concatenate_19[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 64, 64, 2)    66          ['sequential_36[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,458,978\n",
      "Trainable params: 12,451,682\n",
      "Non-trainable params: 7,296\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the model with the best hp.\n",
    "modelA = unet_plus_plus_model(best_hpsA)\n",
    "modelA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#    Manual Training with y data alteration\n",
    "# '''\n",
    "# ## Variable declaration\n",
    "# epochs = 512\n",
    "# steps_per_epoch = 512 // batch_size + 1\n",
    "# min_lr = 1e-8\n",
    "# factor = 0.1\n",
    "# SCHEDULE_EPOCH = 50\n",
    "# STEPS = 50\n",
    "# MONITOR = \"loss\"\n",
    "# PATIENCE = 10\n",
    "# WEIGHTS = []\n",
    "\n",
    "# historyA = {\"history\": {\"loss\": [], \"mae\": [], \"val_loss\": [], \"val_mae\": []}}\n",
    "# for e in range(epochs):\n",
    "#     for i, (images, y_batch) in enumerate(train_generator):\n",
    "#         new_y_batch = []\n",
    "\n",
    "#         ## Train data y alteration\n",
    "#         for x, img in enumerate(images):\n",
    "#             array = np.ones(image_size + (3,))\n",
    "#             array *= img > 0\n",
    "#             array[array > 0] = y_batch[x]\n",
    "#             new_y_batch.append(array)\n",
    "#         new_y_batch = np.array(new_y_batch)\n",
    "#         loss = modelA.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "\n",
    "#         if i >= steps_per_epoch:  # manually detect the end of the epoch train set\n",
    "#             historyA[\"history\"][\"loss\"].append(loss[0])\n",
    "#             historyA[\"history\"][\"mae\"].append(loss[1])\n",
    "#             break\n",
    "\n",
    "#     for i, (images, y_batch) in enumerate(val_generator):\n",
    "#         new_y_batch = []\n",
    "\n",
    "#         ## Val data y alteration\n",
    "#         for x, img in enumerate(images):\n",
    "#             array = np.ones(image_size + (3,))\n",
    "#             array *= img > 0\n",
    "#             array[array > 0] = y_batch[x]\n",
    "#             new_y_batch.append(array)\n",
    "#         new_y_batch = np.array(new_y_batch)\n",
    "#         val = modelA.test_on_batch(images, new_y_batch)\n",
    "\n",
    "#         if i >= steps_per_epoch:  # manually detect the end of the epoch validation set\n",
    "#             historyA[\"history\"][\"val_loss\"].append(val[0])\n",
    "#             historyA[\"history\"][\"val_mae\"].append(val[1])\n",
    "#             break\n",
    "        \n",
    "#     ## LR Scheduler\n",
    "#     curr_lr = modelA.optimizer.learning_rate\n",
    "#     if e >= SCHEDULE_EPOCH and curr_lr > min_lr:\n",
    "#         K.set_value(modelA.optimizer.learning_rate, curr_lr * factor)\n",
    "#         curr_lr = modelA.optimizer.learning_rate\n",
    "#         SCHEDULE_EPOCH += STEPS\n",
    "\n",
    "#     # ## Early Stopping\n",
    "#     # if e > PATIENCE:\n",
    "#     #     PAST = historyA[\"history\"][MONITOR][-PATIENCE-1:-1]\n",
    "#     #     LATEST = historyA[\"history\"][MONITOR][-1]\n",
    "#     #     for i,P in enumerate(PAST):\n",
    "#     #         if LATEST > P:\n",
    "#     #             PATIENCE -= 1  ## decrement patience if previous monitor is worse than the current value\n",
    "\n",
    "#     # if PATIENCE <= 0:\n",
    "#     #     break  ## if no more patience left, early stop training\n",
    "#     # PATIENCE = 10\n",
    "\n",
    "\n",
    "#     print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}   CURR_LR {:.2E}\".format(\n",
    "#         e + 1, loss[0], loss[1], val[0], val[1], curr_lr.numpy()))\n",
    "#     train_generator.on_epoch_end()  # shuffles the data at the end of each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyA[\"history\"][\"loss\"], color='g',alpha=.5)\n",
    "# plt.plot(historyA[\"history\"][\"val_loss\"], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyA[\"history\"][\"mae\"], color='g',alpha=.5)\n",
    "# plt.plot(historyA[\"history\"][\"val_mae\"], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.suptitle(\"Model A: Nested U-Net\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(modelA(sample[0]))\n",
    "# print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validation\n",
    "# val_sample = next(val_generator)\n",
    "# val_result = modelA.predict(val_sample[0])\n",
    "\n",
    "# for x, i in enumerate(val_result):\n",
    "#     RGB = cv2.cvtColor(val_sample[0][x], cv2.COLOR_HSV2RGB)\n",
    "#     MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "#     ARRAY = np.array(val_result[x]).reshape(image_size) * MASK\n",
    "#     OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "#     print(\"Validation - Average Output:\", np.average(OUTPUT), \"Ground Truth:\", np.average(val_sample[1][x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelC.save('design_models/designA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelA = tf.keras.models.load_model('design_models/designA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shutil import rmtree\n",
    "# # removing directory \n",
    "# rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Conv2DTranspose, Dropout, Flatten, Dense, Reshape\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "def segnet(hp):\n",
    "    hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "    \n",
    "    # Encoding path\n",
    "    inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    \n",
    "    # Encoder\n",
    "    x = Conv2D(hp_filters, (3, 3), padding='same', name='conv1')(inputs)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters, (3, 3), padding='same', name='conv2')(x)\n",
    "    x = BatchNormalization(name='bn2')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(name='pool1')(x)\n",
    "    \n",
    "    x = Conv2D(hp_filters * 2, (3, 3), padding='same', name='conv3')(x)\n",
    "    x = BatchNormalization(name='bn3')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 2, (3, 3), padding='same', name='conv4')(x)\n",
    "    x = BatchNormalization(name='bn4')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(name='pool2')(x)\n",
    "    \n",
    "    x = Conv2D(hp_filters * 4, (3, 3), padding='same', name='conv5')(x)\n",
    "    x = BatchNormalization(name='bn5')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 4, (3, 3), padding='same', name='conv6')(x)\n",
    "    x = BatchNormalization(name='bn6')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 4, (3, 3), padding='same', name='conv7')(x)\n",
    "    x = BatchNormalization(name='bn7')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(name='pool3')(x)\n",
    "    \n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv8')(x)\n",
    "    x = BatchNormalization(name='bn8')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv9')(x)\n",
    "    x = BatchNormalization(name='bn9')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv10')(x)\n",
    "    x = BatchNormalization(name='bn10')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(name='pool4')(x)\n",
    "    \n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv11')(x)\n",
    "    x = BatchNormalization(name='bn11')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv12')(x)\n",
    "    x = BatchNormalization(name='bn12')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv13')(x)\n",
    "    x = BatchNormalization(name='bn13')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    qwe = MaxPooling2D(name='pool5')(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Flatten()(qwe)\n",
    "    x = Dense(hp_filters * 16, activation='relu', name='fc1')(x)\n",
    "    \n",
    "    if hp.Boolean(\"dropouts\"):\n",
    "        x = Dropout(0.5)(x)\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(hp_filters * 16, activation='relu', name='fc2')(x)\n",
    "    \n",
    "    if hp.Boolean(\"dropouts\"):\n",
    "        x = Dropout(0.5)(x)\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(hp_filters * 16, activation='relu', name='fc3')(x)\n",
    "    \n",
    "    if hp.Boolean(\"4th_dense\"):\n",
    "        x = Dense(hp_filters * 16, activation='relu', name='fc4')(x)\n",
    "        if hp.Boolean(\"dropouts\"):\n",
    "            x = Dropout(0.5)(x)\n",
    "        if hp.Boolean(\"batch_normalization\"):\n",
    "            x = BatchNormalization()(x)\n",
    "    \n",
    "    # Reshape to reintroduce spatial dimensions\n",
    "    print(qwe.shape)\n",
    "    x = Reshape((qwe.shape[1], qwe.shape[2], hp_filters*4))(x)\n",
    "    \n",
    "    # Decoding path\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters * 8, (3, 3), padding='same', name='deconv1')(x)\n",
    "    x = BatchNormalization(name='bn14')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters * 8, (3, 3), padding='same', name='deconv2')(x)\n",
    "    x = BatchNormalization(name='bn15')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters * 8, (3, 3), padding='same', name='deconv3')(x)\n",
    "    x = BatchNormalization(name='bn16')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters * 4, (3, 3), padding='same', name='deconv4')(x)\n",
    "    x = BatchNormalization(name='bn17')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters * 4, (3, 3), padding='same', name='deconv5')(x)\n",
    "    x = BatchNormalization(name='bn18')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters * 4, (3, 3), padding='same', name='deconv6')(x)\n",
    "    x = BatchNormalization(name='bn19')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters * 2, (3, 3), padding='same', name='deconv7')(x)\n",
    "    x = BatchNormalization(name='bn20')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters * 2, (3, 3), padding='same', name='deconv8')(x)\n",
    "    x = BatchNormalization(name='bn21')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters, (3, 3), padding='same', name='deconv9')(x)\n",
    "    x = BatchNormalization(name='bn22')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters, (3, 3), padding='same', name='deconv10')(x)\n",
    "    x = BatchNormalization(name='bn23')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters, (3, 3), padding='same', name='deconv11')(x)\n",
    "    x = BatchNormalization(name='bn24')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(1, (3, 3), padding='same', name='deconv12')(x)\n",
    "    x = BatchNormalization(name='bn25')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(1, (3, 3), padding='same', name='deconv13')(x)\n",
    "    x = BatchNormalization(name='bn26')(x)\n",
    "    \n",
    "    # Output layer with sigmoid activation for binary segmentation\n",
    "    outputs = Conv2D(2, (1, 1), activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs);lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5]);model.compile(optimizer= tf.keras.optimizers.Adam(lr=lr), loss= [\"binary_crossentropy\"], metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 19s]\n",
      "val_loss: 0.3817465901374817\n",
      "\n",
      "Best val_loss So Far: 0.3729792535305023\n",
      "Total elapsed time: 00h 25m 34s\n",
      "{'filters': 16, 'dropouts': False, 'batch_normalization': True, '4th_dense': False, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "tunerB = kt.BayesianOptimization(segnet,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 10,\n",
    "                     project_name='design_b',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8)\n",
    "\n",
    "\n",
    "tunerB.search(train_generator, epochs=100, validation_data=val_generator, callbacks=[stop_early, reduce_lr])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsB=tunerB.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "\n",
    "print(best_hpsB.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hpsB.values['filters']=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2, 2, 128)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 64, 64, 16)        448       \n",
      "                                                                 \n",
      " bn1 (BatchNormalization)    (None, 64, 64, 16)        64        \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 64, 64, 16)        2320      \n",
      "                                                                 \n",
      " bn2 (BatchNormalization)    (None, 64, 64, 16)        64        \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " pool1 (MaxPooling2D)        (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " bn3 (BatchNormalization)    (None, 32, 32, 32)        128       \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv4 (Conv2D)              (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " bn4 (BatchNormalization)    (None, 32, 32, 32)        128       \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " pool2 (MaxPooling2D)        (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv5 (Conv2D)              (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " bn5 (BatchNormalization)    (None, 16, 16, 64)        256       \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv6 (Conv2D)              (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " bn6 (BatchNormalization)    (None, 16, 16, 64)        256       \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv7 (Conv2D)              (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " bn7 (BatchNormalization)    (None, 16, 16, 64)        256       \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " pool3 (MaxPooling2D)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv8 (Conv2D)              (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " bn8 (BatchNormalization)    (None, 8, 8, 128)         512       \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv9 (Conv2D)              (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " bn9 (BatchNormalization)    (None, 8, 8, 128)         512       \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv10 (Conv2D)             (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " bn10 (BatchNormalization)   (None, 8, 8, 128)         512       \n",
      "                                                                 \n",
      " activation_34 (Activation)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " pool4 (MaxPooling2D)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv11 (Conv2D)             (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " bn11 (BatchNormalization)   (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_35 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv12 (Conv2D)             (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " bn12 (BatchNormalization)   (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv13 (Conv2D)             (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " bn13 (BatchNormalization)   (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " pool5 (MaxPooling2D)        (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 256)               65792     \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " up_sampling2d_5 (UpSampling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " deconv1 (Conv2DTranspose)   (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " bn14 (BatchNormalization)   (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " deconv2 (Conv2DTranspose)   (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " bn15 (BatchNormalization)   (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " deconv3 (Conv2DTranspose)   (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " bn16 (BatchNormalization)   (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " up_sampling2d_6 (UpSampling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " deconv4 (Conv2DTranspose)   (None, 8, 8, 64)          73792     \n",
      "                                                                 \n",
      " bn17 (BatchNormalization)   (None, 8, 8, 64)          256       \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " deconv5 (Conv2DTranspose)   (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " bn18 (BatchNormalization)   (None, 8, 8, 64)          256       \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " deconv6 (Conv2DTranspose)   (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " bn19 (BatchNormalization)   (None, 8, 8, 64)          256       \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " up_sampling2d_7 (UpSampling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " deconv7 (Conv2DTranspose)   (None, 16, 16, 32)        18464     \n",
      "                                                                 \n",
      " bn20 (BatchNormalization)   (None, 16, 16, 32)        128       \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " deconv8 (Conv2DTranspose)   (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " bn21 (BatchNormalization)   (None, 16, 16, 32)        128       \n",
      "                                                                 \n",
      " activation_45 (Activation)  (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " deconv9 (Conv2DTranspose)   (None, 16, 16, 16)        4624      \n",
      "                                                                 \n",
      " bn22 (BatchNormalization)   (None, 16, 16, 16)        64        \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 32, 32, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " deconv10 (Conv2DTranspose)  (None, 32, 32, 16)        2320      \n",
      "                                                                 \n",
      " bn23 (BatchNormalization)   (None, 32, 32, 16)        64        \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " deconv11 (Conv2DTranspose)  (None, 32, 32, 16)        2320      \n",
      "                                                                 \n",
      " bn24 (BatchNormalization)   (None, 32, 32, 16)        64        \n",
      "                                                                 \n",
      " activation_48 (Activation)  (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " deconv12 (Conv2DTranspose)  (None, 32, 32, 1)         145       \n",
      "                                                                 \n",
      " bn25 (BatchNormalization)   (None, 32, 32, 1)         4         \n",
      "                                                                 \n",
      " activation_49 (Activation)  (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_9 (UpSampling  (None, 64, 64, 1)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " deconv13 (Conv2DTranspose)  (None, 64, 64, 1)         10        \n",
      "                                                                 \n",
      " bn26 (BatchNormalization)   (None, 64, 64, 1)         4         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 2)         4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,746,535\n",
      "Trainable params: 1,742,019\n",
      "Non-trainable params: 4,516\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the best hp.\n",
    "modelB = segnet(best_hpsB)\n",
    "modelB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 512\n",
    "# steps_per_epoch = 512 // batch_size + 1  # we usually consider 1 epoch to be\n",
    "#                                             # the point where the model has seen\n",
    "#                                             # all the training samples at least once\n",
    "# min_lr = 1e-8\n",
    "# factor = 0.1\n",
    "# SCHEDULE_EPOCH = 200\n",
    "# STEPS = 200\n",
    "\n",
    "\n",
    "# historyB = {\"history\":{\"loss\":[],\"mae\":[],\"val_loss\":[],\"val_mae\":[]}}\n",
    "# for e in range(epochs):\n",
    "#     for i, (images, y_batch) in enumerate(train_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#        loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#     #    val = modelB.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyB[\"history\"][\"loss\"].append(loss[0])\n",
    "#             historyB[\"history\"][\"mae\"].append(loss[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             break  \n",
    "#     for i, (images, y_batch) in enumerate(val_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#     #    loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#        val = modelB.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyB[\"history\"][\"val_loss\"].append(val[0])\n",
    "#             historyB[\"history\"][\"val_mae\"].append(val[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             curr_lr = modelB.optimizer.learning_rate\n",
    "#             if e>=SCHEDULE_EPOCH and curr_lr>min_lr: \n",
    "#                K.set_value(modelB.optimizer.learning_rate, curr_lr*factor)\n",
    "#                curr_lr = modelB.optimizer.learning_rate\n",
    "#                SCHEDULE_EPOCH+=STEPS\n",
    "#             # patience = 2\n",
    "#             # if e>patience and curr_lr>min_lr:\n",
    "#                # curr_loss = historyA[\"history\"][\"val_loss\"][-1]\n",
    "#                # for i in historyA[\"history\"][\"val_loss\"][-patience-1:-1]:\n",
    "#                #    if curr_loss >= i: patience-=1\n",
    "#                #    if patience < 1:\n",
    "#                      # K.set_value(modelA.optimizer.learning_rate, curr_lr*factor)\n",
    "#                      # curr_lr = modelA.optimizer.learning_rate\n",
    "#             break  \n",
    "#     print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}   CURR_LR {:.2E}\".format(e+1, loss[0], loss[1],val[0],val[1], curr_lr.numpy()))\n",
    "#     train_generator.on_epoch_end()  # this shuffles the data at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyB[\"history\"][\"loss\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyB[\"history\"][\"val_loss\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyB[\"history\"][\"mae\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyB[\"history\"][\"val_mae\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.suptitle(\"Model B: SegNet\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(historyB[\"history\"][\"loss\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "#                               patience=10, min_lr=1e-7)\n",
    "# # stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "# history = modelB.fit(train_generator, epochs=2,# validation_data=val_generator,\n",
    "#                      callbacks=[red_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n",
    "# result = modelB(sample[0])\n",
    "# for x,i in enumerate(result):\n",
    "#     RGB  = cv2.cvtColor(sample[0][x],cv2.COLOR_HSV2RGB)\n",
    "#     MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "#     ARRAY = np.array(result[5]).reshape(image_size) * MASK\n",
    "#     OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "#     print(np.average(OUTPUT), np.average(sample[1][x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.imshow(result[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(sample[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# RGB  = cv2.cvtColor(sample[0][5],cv2.COLOR_HSV2RGB)\n",
    "# MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "# ARRAY = np.array(result[5]).reshape(image_size) * MASK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVE = np.average(ARRAY)\n",
    "# TRUTH = sample[1][5]\n",
    "# print(AVE)\n",
    "# print(TRUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(ARRAY)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelA.save('design_models/BEST/designA_V3.h5')\n",
    "# modelB.save('design_models/BEST/designB_V3.h5')\n",
    "# modelC.save('design_models/BEST/designC_V3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shutil import rmtree\n",
    "# # removing directory \n",
    "# rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Conv2DTranspose, Dropout, Flatten, Dense, Reshape, AveragePooling2D, Concatenate\n",
    "from tensorflow.keras import layers, initializers\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n",
    "    x = Conv2D(\n",
    "        num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "        kernel_initializer=initializers.HeNormal(),\n",
    "    )(block_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation('relu')(x)\n",
    "\n",
    "def DilatedSpatialPyramidPooling(dspp_input, num_filters):\n",
    "    dims = dspp_input.shape\n",
    "    x = AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "    out_pool = UpSampling2D(\n",
    "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "\n",
    "    out_1 = convolution_block(dspp_input, num_filters=num_filters, kernel_size=1, dilation_rate=1)\n",
    "    out_6 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=6)\n",
    "    out_12 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=12)\n",
    "    out_18 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=18)\n",
    "\n",
    "    x = Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "    output = convolution_block(x, kernel_size=1)\n",
    "    return output\n",
    "\n",
    "def DeeplabV3Plus(hp, image_size=(64, 64)):\n",
    "    model_input = Input(shape=image_size+(3,))\n",
    "    resnet50 = tf.keras.applications.ResNet50(\n",
    "        weights='imagenet', include_top=False, input_tensor=model_input\n",
    "    )\n",
    "    for layer in resnet50.layers:\n",
    "        layer.trainable=True\n",
    "    \n",
    "    hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
    "    x = DilatedSpatialPyramidPooling(x, hp_filters*2)\n",
    "    print(x.shape)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(hp_filters*hp_filters, activation='relu')(x)\n",
    "\n",
    "    UNITS = hp_filters*hp_filters\n",
    "    hp.Boolean(\"dropouts\", default=False)\n",
    "    hp.Boolean(\"batch_normalization\", default=False)\n",
    "\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    if hp.Boolean(\"dropouts\"):\n",
    "        x = Dropout(0.5)(x)\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    if hp.Boolean(\"dropouts\"):\n",
    "        x = Dropout(0.5)(x)\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    if hp.Boolean(\"4th_dense\", default=False):\n",
    "        x = Dense(UNITS, activation='relu')(x)\n",
    "        if hp.Boolean(\"dropouts\"):\n",
    "            x = Dropout(0.5)(x)\n",
    "        if hp.Boolean(\"batch_normalization\"):\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "    SIZE = image_size[0]\n",
    "    temp = 0\n",
    "    if hp_filters == 16:\n",
    "        temp = hp_filters\n",
    "    if hp_filters == 32:\n",
    "        temp = hp_filters * 2\n",
    "    if hp_filters == 64:\n",
    "        temp = hp_filters * 4\n",
    "    x = Reshape((SIZE // 16, SIZE // 16, temp))(x)\n",
    "\n",
    "    x = UpSampling2D(size=(SIZE // 4 // x.shape[1], SIZE // 4 // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    input_b = convolution_block(input_b, num_filters=hp_filters, kernel_size=1)\n",
    "\n",
    "    x = Concatenate(axis=-1)([x, input_b])\n",
    "    x = convolution_block(x, num_filters=hp_filters*4)\n",
    "    x = convolution_block(x, num_filters=hp_filters*4)\n",
    "    x = UpSampling2D(size=(SIZE // x.shape[1], SIZE // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "    \n",
    "    outputs = Conv2D(2, 1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=model_input, outputs=outputs)\n",
    "    lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=[\"binary_crossentropy\"], metrics=['acc'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, applications\n",
    "\n",
    "# def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n",
    "#     x = layers.Conv2D(\n",
    "#         num_filters,\n",
    "#         kernel_size=kernel_size,\n",
    "#         dilation_rate=dilation_rate,\n",
    "#         padding=padding,\n",
    "#         use_bias=use_bias,\n",
    "#         kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "#     )(block_input)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     return tf.nn.relu(x)\n",
    "\n",
    "# def DilatedSpatialPyramidPooling(dspp_input):\n",
    "#     dims = dspp_input.shape\n",
    "#     x = layers.AveragePooling2D(pool_size=(dims[1], dims[2]))(dspp_input)\n",
    "#     x = convolution_block(x, num_filters=256, kernel_size=1, use_bias=True)\n",
    "#     out_pool = layers.UpSampling2D(\n",
    "#         size=(dims[1] // x.shape[1], dims[2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "#     )(x)\n",
    "\n",
    "#     out_1 = convolution_block(dspp_input, num_filters=256, kernel_size=1, dilation_rate=1)\n",
    "#     out_6 = convolution_block(dspp_input, num_filters=256, kernel_size=3, dilation_rate=6)\n",
    "#     out_12 = convolution_block(dspp_input, num_filters=256, kernel_size=3, dilation_rate=12)\n",
    "#     out_18 = convolution_block(dspp_input, num_filters=256, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "#     x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "#     output = convolution_block(x, num_filters=256, kernel_size=1)\n",
    "#     return output\n",
    "\n",
    "# def DeeplabV3Plus(hp, image_size=(64, 64, 3), num_classes=2):\n",
    "#     base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=image_size)\n",
    "#     base_model_output = base_model.get_layer(\"conv4_block6_2_relu\").output\n",
    "\n",
    "#     x = DilatedSpatialPyramidPooling(base_model_output)\n",
    "\n",
    "#     input_a = layers.UpSampling2D(size=(image_size[0] // 4 // x.shape[1], image_size[1] // 4 // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "#     input_b = base_model.get_layer(\"conv2_block3_2_relu\").output\n",
    "#     input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "#     x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "#     x = convolution_block(x, num_filters=256)\n",
    "#     x = convolution_block(x, num_filters=256)\n",
    "#     x = layers.UpSampling2D(size=(image_size[0] // x.shape[1], image_size[1] // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "#     x = layers.Conv2D(2, (1, 1), padding=\"same\")(x)\n",
    "#     outputs = tf.keras.layers.Activation('sigmoid')(x)\n",
    "\n",
    "#     model = tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=[\"binary_crossentropy\"], metrics=['acc'])\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 02m 19s]\n",
      "val_loss: 0.3647870719432831\n",
      "\n",
      "Best val_loss So Far: 0.3272910416126251\n",
      "Total elapsed time: 00h 34m 56s\n",
      "{'filters': 64, 'dropouts': True, 'batch_normalization': False, '4th_dense': False, 'learning_rate': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "tunerC = kt.BayesianOptimization(DeeplabV3Plus,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 10,\n",
    "                     project_name='design_c',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8)\n",
    "\n",
    "tunerC.search(train_generator, epochs=100, validation_data=val_generator, callbacks=[stop_early, reduce_lr])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsC=tunerC.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(best_hpsC.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 4, 4, 256)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 70, 70, 3)    0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 32, 32, 64)   9472        ['conv1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 32, 32, 64)   256         ['conv1_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 32, 32, 64)   0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 34, 34, 64)   0           ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 16, 16, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 16, 16, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 16, 16, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 16, 16, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 16, 16, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 16, 16, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 16, 16, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 16, 16, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 16, 16, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 16, 16, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 16, 16, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 16, 16, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 16, 16, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 16, 16, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 16, 16, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 16, 16, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 16, 16, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 16, 16, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 16, 16, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 8, 8, 128)    32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 8, 8, 512)    131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 8, 8, 512)    0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 8, 8, 128)    65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 8, 8, 512)    0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 8, 8, 128)    65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 8, 8, 512)    0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 8, 8, 128)    65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 8, 8, 128)    147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 8, 8, 128)   0           ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 8, 8, 512)    66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 8, 8, 512)    0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 8, 8, 512)    0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 4, 4, 256)    131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 4, 4, 1024)   525312      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                                                  'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block1_out[0][0]',       \n",
      "                                                                  'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block2_out[0][0]',       \n",
      "                                                                  'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block3_out[0][0]',       \n",
      "                                                                  'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 4, 4, 1024)   263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 4, 4, 1024)  4096        ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 4, 4, 1024)   0           ['conv4_block4_out[0][0]',       \n",
      "                                                                  'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 4, 4, 1024)   0           ['conv4_block5_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 4, 4, 256)    262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 4, 4, 256)    590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 4, 4, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 4, 4, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 1, 1, 256)   0           ['conv4_block6_2_relu[0][0]']    \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 1, 1, 256)    65792       ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 1, 1, 256)   1024        ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 4, 4, 128)    32768       ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 4, 4, 128)    294912      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 4, 4, 128)    294912      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 4, 4, 128)    294912      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 1, 1, 256)    0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 4, 4, 128)   512         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 4, 4, 128)   512         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 4, 4, 128)   512         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 4, 4, 128)   512         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 4, 4, 256)   0           ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 4, 4, 768)    0           ['up_sampling2d_3[0][0]',        \n",
      "                                                                  'activation_10[0][0]',          \n",
      "                                                                  'activation_11[0][0]',          \n",
      "                                                                  'activation_12[0][0]',          \n",
      "                                                                  'activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 4, 4, 256)    196608      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 4, 4, 256)   1024        ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 4, 4, 256)    0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 4096)         0           ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096)         16781312    ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096)         16781312    ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096)         16781312    ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096)         0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096)         16781312    ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 64)   4096        ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 4, 4, 256)    0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 64)  256         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_4 (UpSampling2D)  (None, 16, 16, 256)  0          ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 16, 16, 320)  0           ['up_sampling2d_4[0][0]',        \n",
      "                                                                  'activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 256)  737280      ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 256)  1024       ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 16, 16, 256)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 256)  589824      ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16, 16, 256)  1024       ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 16, 16, 256)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_5 (UpSampling2D)  (None, 64, 64, 256)  0          ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 64, 64, 2)    514         ['up_sampling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 77,965,186\n",
      "Trainable params: 77,933,442\n",
      "Non-trainable params: 31,744\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# best_hpsC.values['filters']=64\n",
    "# best_hpsC.values['learning_rate']=1e-6\n",
    "# Build the model with the best hp.\n",
    "modelC = DeeplabV3Plus(best_hpsC)\n",
    "modelC.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 512\n",
    "# steps_per_epoch = 512 // batch_size + 1  # we usually consider 1 epoch to be\n",
    "#                                             # the point where the model has seen\n",
    "#                                             # all the training samples at least once\n",
    "# min_lr = 1e-8\n",
    "# factor = 0.1\n",
    "# SCHEDULE_EPOCH = 200\n",
    "# STEPS = 200\n",
    "\n",
    "# historyC = {\"history\":{\"loss\":[],\"mae\":[],\"val_loss\":[],\"val_mae\":[]}}\n",
    "# for e in range(epochs):\n",
    "#     for i, (images, y_batch) in enumerate(train_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#        loss = modelC.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#     #    val = modelB.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyC[\"history\"][\"loss\"].append(loss[0])\n",
    "#             historyC[\"history\"][\"mae\"].append(loss[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             break  \n",
    "#     for i, (images, y_batch) in enumerate(val_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#     #    loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#        val = modelC.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyC[\"history\"][\"val_loss\"].append(val[0])\n",
    "#             historyC[\"history\"][\"val_mae\"].append(val[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             curr_lr = modelC.optimizer.learning_rate\n",
    "#             if e>=SCHEDULE_EPOCH and curr_lr>min_lr: \n",
    "#                K.set_value(modelC.optimizer.learning_rate, curr_lr*factor)\n",
    "#                curr_lr = modelC.optimizer.learning_rate\n",
    "#                SCHEDULE_EPOCH+=STEPS\n",
    "#             # patience = 2\n",
    "#             # if e>patience and curr_lr>min_lr:\n",
    "#                # curr_loss = historyA[\"history\"][\"val_loss\"][-1]\n",
    "#                # for i in historyA[\"history\"][\"val_loss\"][-patience-1:-1]:\n",
    "#                #    if curr_loss >= i: patience-=1\n",
    "#                #    if patience < 1:\n",
    "#                      # K.set_value(modelA.optimizer.learning_rate, curr_lr*factor)\n",
    "#                      # curr_lr = modelA.optimizer.learning_rate\n",
    "#             break  \n",
    "#     print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}   CURR_LR {:.2E}\".format(e+1, loss[0], loss[1],val[0],val[1], curr_lr.numpy()))\n",
    "#     train_generator.on_epoch_end()  # this shuffles the data at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyC[\"history\"][\"loss\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyC[\"history\"][\"val_loss\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyC[\"history\"][\"mae\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyC[\"history\"][\"val_mae\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.suptitle(\"Model C: DeepLabV3+\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n",
    "# result = modelC(sample[0])\n",
    "# for x,i in enumerate(result):\n",
    "#     RGB  = cv2.cvtColor(sample[0][x],cv2.COLOR_HSV2RGB)\n",
    "#     MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "#     ARRAY = np.array(result[5]).reshape(image_size) * MASK\n",
    "#     OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "#     print(np.average(OUTPUT), np.average(sample[1][x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelC.save('design_models/designC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shutil import rmtree\n",
    "# # removing directory \n",
    "# rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANUFACTURABILITY: TRAINING TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 128\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1,\n",
    "                              patience=6, min_lr=1e-9)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=16, mode=\"min\", restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "79/79 - 4s - loss: 0.3924 - acc: 0.8617 - val_loss: 0.4071 - val_acc: 0.8421 - lr: 0.0010 - 4s/epoch - 48ms/step\n",
      "Epoch 2/128\n",
      "79/79 - 2s - loss: 0.3904 - acc: 0.8605 - val_loss: 0.4206 - val_acc: 0.8302 - lr: 0.0010 - 2s/epoch - 30ms/step\n",
      "Epoch 3/128\n",
      "79/79 - 2s - loss: 0.3908 - acc: 0.8638 - val_loss: 0.3989 - val_acc: 0.8510 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 4/128\n",
      "79/79 - 2s - loss: 0.3889 - acc: 0.8650 - val_loss: 0.3902 - val_acc: 0.8020 - lr: 0.0010 - 2s/epoch - 29ms/step\n",
      "Epoch 5/128\n",
      "79/79 - 2s - loss: 0.3872 - acc: 0.8643 - val_loss: 0.3781 - val_acc: 0.8492 - lr: 0.0010 - 2s/epoch - 29ms/step\n",
      "Epoch 6/128\n",
      "79/79 - 2s - loss: 0.3886 - acc: 0.8632 - val_loss: 0.3865 - val_acc: 0.8074 - lr: 0.0010 - 2s/epoch - 29ms/step\n",
      "Epoch 7/128\n",
      "79/79 - 2s - loss: 0.3876 - acc: 0.8629 - val_loss: 0.3777 - val_acc: 0.8475 - lr: 0.0010 - 2s/epoch - 29ms/step\n",
      "Epoch 8/128\n",
      "79/79 - 2s - loss: 0.3853 - acc: 0.8658 - val_loss: 0.3681 - val_acc: 0.8515 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 9/128\n",
      "79/79 - 2s - loss: 0.3851 - acc: 0.8675 - val_loss: 0.3682 - val_acc: 0.8387 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 10/128\n",
      "79/79 - 2s - loss: 0.3848 - acc: 0.8650 - val_loss: 0.3703 - val_acc: 0.8506 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 11/128\n",
      "79/79 - 2s - loss: 0.3860 - acc: 0.8652 - val_loss: 0.3729 - val_acc: 0.8477 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 12/128\n",
      "79/79 - 2s - loss: 0.3843 - acc: 0.8642 - val_loss: 0.3639 - val_acc: 0.8527 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 13/128\n",
      "79/79 - 2s - loss: 0.3820 - acc: 0.8670 - val_loss: 0.3654 - val_acc: 0.8533 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 14/128\n",
      "79/79 - 2s - loss: 0.3817 - acc: 0.8671 - val_loss: 0.3686 - val_acc: 0.8515 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 15/128\n",
      "79/79 - 2s - loss: 0.3813 - acc: 0.8671 - val_loss: 0.3701 - val_acc: 0.8487 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 16/128\n",
      "79/79 - 2s - loss: 0.3788 - acc: 0.8671 - val_loss: 0.3685 - val_acc: 0.8411 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 17/128\n",
      "79/79 - 2s - loss: 0.3799 - acc: 0.8644 - val_loss: 0.3712 - val_acc: 0.8542 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 18/128\n",
      "79/79 - 2s - loss: 0.3797 - acc: 0.8656 - val_loss: 0.3642 - val_acc: 0.8518 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 19/128\n",
      "79/79 - 2s - loss: 0.3796 - acc: 0.8518 - val_loss: 0.3642 - val_acc: 0.8501 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 20/128\n",
      "79/79 - 2s - loss: 0.3764 - acc: 0.8627 - val_loss: 0.3630 - val_acc: 0.7724 - lr: 0.0010 - 2s/epoch - 30ms/step\n",
      "Epoch 21/128\n",
      "79/79 - 2s - loss: 0.3778 - acc: 0.8594 - val_loss: 0.3697 - val_acc: 0.8358 - lr: 0.0010 - 2s/epoch - 29ms/step\n",
      "Epoch 22/128\n",
      "79/79 - 2s - loss: 0.3769 - acc: 0.8569 - val_loss: 0.3608 - val_acc: 0.8539 - lr: 0.0010 - 2s/epoch - 28ms/step\n",
      "Epoch 23/128\n",
      "79/79 - 2s - loss: 0.3781 - acc: 0.8527 - val_loss: 0.3693 - val_acc: 0.8507 - lr: 0.0010 - 2s/epoch - 29ms/step\n",
      "Epoch 24/128\n",
      "79/79 - 2s - loss: 0.3760 - acc: 0.8655 - val_loss: 0.3615 - val_acc: 0.8510 - lr: 1.0000e-04 - 2s/epoch - 29ms/step\n",
      "Epoch 25/128\n",
      "79/79 - 2s - loss: 0.3745 - acc: 0.8654 - val_loss: 0.3607 - val_acc: 0.8532 - lr: 1.0000e-04 - 2s/epoch - 29ms/step\n",
      "Epoch 26/128\n",
      "79/79 - 2s - loss: 0.3736 - acc: 0.8654 - val_loss: 0.3609 - val_acc: 0.8536 - lr: 1.0000e-04 - 2s/epoch - 29ms/step\n",
      "Epoch 27/128\n",
      "79/79 - 2s - loss: 0.3742 - acc: 0.8659 - val_loss: 0.3609 - val_acc: 0.8535 - lr: 1.0000e-04 - 2s/epoch - 29ms/step\n",
      "Epoch 28/128\n",
      "79/79 - 2s - loss: 0.3738 - acc: 0.8645 - val_loss: 0.3585 - val_acc: 0.8535 - lr: 1.0000e-04 - 2s/epoch - 30ms/step\n",
      "Epoch 29/128\n",
      "79/79 - 2s - loss: 0.3721 - acc: 0.8645 - val_loss: 0.3604 - val_acc: 0.8535 - lr: 1.0000e-04 - 2s/epoch - 29ms/step\n",
      "Epoch 30/128\n",
      "79/79 - 2s - loss: 0.3726 - acc: 0.8645 - val_loss: 0.3597 - val_acc: 0.8534 - lr: 1.0000e-05 - 2s/epoch - 29ms/step\n",
      "Epoch 31/128\n",
      "79/79 - 2s - loss: 0.3736 - acc: 0.8644 - val_loss: 0.3598 - val_acc: 0.8532 - lr: 1.0000e-05 - 2s/epoch - 29ms/step\n",
      "Epoch 32/128\n",
      "79/79 - 2s - loss: 0.3745 - acc: 0.8642 - val_loss: 0.3597 - val_acc: 0.8533 - lr: 1.0000e-05 - 2s/epoch - 29ms/step\n",
      "Epoch 33/128\n",
      "79/79 - 2s - loss: 0.3733 - acc: 0.8641 - val_loss: 0.3595 - val_acc: 0.8533 - lr: 1.0000e-05 - 2s/epoch - 29ms/step\n",
      "Epoch 34/128\n",
      "79/79 - 2s - loss: 0.3716 - acc: 0.8643 - val_loss: 0.3594 - val_acc: 0.8531 - lr: 1.0000e-05 - 2s/epoch - 29ms/step\n",
      "Epoch 35/128\n",
      "79/79 - 2s - loss: 0.3724 - acc: 0.8640 - val_loss: 0.3593 - val_acc: 0.8532 - lr: 1.0000e-05 - 2s/epoch - 28ms/step\n",
      "Epoch 36/128\n",
      "79/79 - 2s - loss: 0.3727 - acc: 0.8638 - val_loss: 0.3593 - val_acc: 0.8530 - lr: 1.0000e-06 - 2s/epoch - 29ms/step\n",
      "Training time: 83.47423958778381s\n"
     ]
    }
   ],
   "source": [
    "## DESIGN A\n",
    "import time \n",
    "start = time.time()\n",
    "historyA = modelA.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [es, reduce_lr])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")\n",
    "\n",
    "# ## FUNCTIONALITY: INFERENCE TIME\n",
    "# modelC.evaluate(train_generator[1][0][0].reshape(1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "79/79 - 6s - loss: 0.6807 - acc: 0.3529 - val_loss: 0.6502 - val_acc: 0.3232 - lr: 0.0010 - 6s/epoch - 73ms/step\n",
      "Epoch 2/128\n",
      "79/79 - 2s - loss: 0.6280 - acc: 0.3527 - val_loss: 0.6214 - val_acc: 0.4018 - lr: 0.0010 - 2s/epoch - 25ms/step\n",
      "Epoch 3/128\n",
      "79/79 - 2s - loss: 0.5926 - acc: 0.3451 - val_loss: 0.5827 - val_acc: 0.3339 - lr: 0.0010 - 2s/epoch - 25ms/step\n",
      "Epoch 4/128\n",
      "79/79 - 2s - loss: 0.5620 - acc: 0.3403 - val_loss: 0.5623 - val_acc: 0.3590 - lr: 0.0010 - 2s/epoch - 25ms/step\n",
      "Epoch 5/128\n",
      "79/79 - 2s - loss: 0.5329 - acc: 0.3416 - val_loss: 0.5597 - val_acc: 0.4265 - lr: 0.0010 - 2s/epoch - 25ms/step\n",
      "Epoch 6/128\n",
      "79/79 - 2s - loss: 0.5056 - acc: 0.3363 - val_loss: 0.5129 - val_acc: 0.3662 - lr: 0.0010 - 2s/epoch - 25ms/step\n",
      "Epoch 7/128\n",
      "79/79 - 2s - loss: 0.4818 - acc: 0.3315 - val_loss: 0.4766 - val_acc: 0.3231 - lr: 0.0010 - 2s/epoch - 25ms/step\n",
      "Epoch 8/128\n",
      "79/79 - 2s - loss: 0.4633 - acc: 0.3314 - val_loss: 0.4718 - val_acc: 0.3539 - lr: 0.0010 - 2s/epoch - 25ms/step\n",
      "Epoch 9/128\n",
      "79/79 - 2s - loss: 0.4472 - acc: 0.3298 - val_loss: 0.4508 - val_acc: 0.3353 - lr: 0.0010 - 2s/epoch - 26ms/step\n",
      "Epoch 10/128\n",
      "79/79 - 2s - loss: 0.4337 - acc: 0.3317 - val_loss: 0.4453 - val_acc: 0.3749 - lr: 0.0010 - 2s/epoch - 25ms/step\n",
      "Epoch 11/128\n",
      "79/79 - 2s - loss: 0.4231 - acc: 0.3365 - val_loss: 0.4225 - val_acc: 0.3418 - lr: 0.0010 - 2s/epoch - 25ms/step\n",
      "Epoch 12/128\n",
      "79/79 - 2s - loss: 0.4153 - acc: 0.3361 - val_loss: 0.4184 - val_acc: 0.3335 - lr: 1.0000e-04 - 2s/epoch - 25ms/step\n",
      "Epoch 13/128\n",
      "79/79 - 2s - loss: 0.4137 - acc: 0.3332 - val_loss: 0.4163 - val_acc: 0.3295 - lr: 1.0000e-04 - 2s/epoch - 25ms/step\n",
      "Epoch 14/128\n",
      "79/79 - 2s - loss: 0.4125 - acc: 0.3349 - val_loss: 0.4148 - val_acc: 0.3261 - lr: 1.0000e-04 - 2s/epoch - 25ms/step\n",
      "Epoch 15/128\n",
      "79/79 - 2s - loss: 0.4117 - acc: 0.3360 - val_loss: 0.4128 - val_acc: 0.3209 - lr: 1.0000e-04 - 2s/epoch - 25ms/step\n",
      "Epoch 16/128\n",
      "79/79 - 2s - loss: 0.4101 - acc: 0.3351 - val_loss: 0.4125 - val_acc: 0.3237 - lr: 1.0000e-04 - 2s/epoch - 25ms/step\n",
      "Epoch 17/128\n",
      "79/79 - 2s - loss: 0.4099 - acc: 0.3348 - val_loss: 0.4114 - val_acc: 0.3252 - lr: 1.0000e-04 - 2s/epoch - 25ms/step\n",
      "Epoch 18/128\n",
      "79/79 - 2s - loss: 0.4083 - acc: 0.3366 - val_loss: 0.4113 - val_acc: 0.3268 - lr: 1.0000e-05 - 2s/epoch - 25ms/step\n",
      "Epoch 19/128\n",
      "79/79 - 2s - loss: 0.4079 - acc: 0.3361 - val_loss: 0.4108 - val_acc: 0.3243 - lr: 1.0000e-05 - 2s/epoch - 25ms/step\n",
      "Epoch 20/128\n",
      "79/79 - 2s - loss: 0.4077 - acc: 0.3369 - val_loss: 0.4105 - val_acc: 0.3223 - lr: 1.0000e-05 - 2s/epoch - 25ms/step\n",
      "Epoch 21/128\n",
      "79/79 - 2s - loss: 0.4076 - acc: 0.3353 - val_loss: 0.4101 - val_acc: 0.3201 - lr: 1.0000e-05 - 2s/epoch - 25ms/step\n",
      "Epoch 22/128\n",
      "79/79 - 2s - loss: 0.4072 - acc: 0.3343 - val_loss: 0.4101 - val_acc: 0.3217 - lr: 1.0000e-05 - 2s/epoch - 24ms/step\n",
      "Epoch 23/128\n",
      "79/79 - 2s - loss: 0.4079 - acc: 0.3339 - val_loss: 0.4100 - val_acc: 0.3204 - lr: 1.0000e-05 - 2s/epoch - 24ms/step\n",
      "Epoch 24/128\n",
      "79/79 - 2s - loss: 0.4077 - acc: 0.3355 - val_loss: 0.4100 - val_acc: 0.3207 - lr: 1.0000e-06 - 2s/epoch - 25ms/step\n",
      "Epoch 25/128\n",
      "79/79 - 2s - loss: 0.4070 - acc: 0.3338 - val_loss: 0.4101 - val_acc: 0.3208 - lr: 1.0000e-06 - 2s/epoch - 24ms/step\n",
      "Epoch 26/128\n",
      "79/79 - 2s - loss: 0.4066 - acc: 0.3349 - val_loss: 0.4100 - val_acc: 0.3209 - lr: 1.0000e-06 - 2s/epoch - 24ms/step\n",
      "Epoch 27/128\n",
      "79/79 - 2s - loss: 0.4068 - acc: 0.3360 - val_loss: 0.4100 - val_acc: 0.3215 - lr: 1.0000e-06 - 2s/epoch - 24ms/step\n",
      "Epoch 28/128\n",
      "79/79 - 2s - loss: 0.4072 - acc: 0.3349 - val_loss: 0.4099 - val_acc: 0.3208 - lr: 1.0000e-06 - 2s/epoch - 25ms/step\n",
      "Epoch 29/128\n",
      "79/79 - 2s - loss: 0.4070 - acc: 0.3358 - val_loss: 0.4099 - val_acc: 0.3212 - lr: 1.0000e-06 - 2s/epoch - 24ms/step\n",
      "Epoch 30/128\n",
      "79/79 - 2s - loss: 0.4068 - acc: 0.3350 - val_loss: 0.4099 - val_acc: 0.3205 - lr: 1.0000e-07 - 2s/epoch - 25ms/step\n",
      "Epoch 31/128\n",
      "79/79 - 2s - loss: 0.4065 - acc: 0.3346 - val_loss: 0.4099 - val_acc: 0.3203 - lr: 1.0000e-07 - 2s/epoch - 25ms/step\n",
      "Epoch 32/128\n",
      "79/79 - 2s - loss: 0.4072 - acc: 0.3339 - val_loss: 0.4099 - val_acc: 0.3207 - lr: 1.0000e-07 - 2s/epoch - 25ms/step\n",
      "Epoch 33/128\n",
      "79/79 - 2s - loss: 0.4073 - acc: 0.3340 - val_loss: 0.4100 - val_acc: 0.3207 - lr: 1.0000e-07 - 2s/epoch - 24ms/step\n",
      "Epoch 34/128\n",
      "79/79 - 2s - loss: 0.4072 - acc: 0.3352 - val_loss: 0.4100 - val_acc: 0.3209 - lr: 1.0000e-07 - 2s/epoch - 25ms/step\n",
      "Epoch 35/128\n",
      "79/79 - 2s - loss: 0.4074 - acc: 0.3347 - val_loss: 0.4100 - val_acc: 0.3212 - lr: 1.0000e-07 - 2s/epoch - 24ms/step\n",
      "Epoch 36/128\n",
      "79/79 - 2s - loss: 0.4065 - acc: 0.3355 - val_loss: 0.4100 - val_acc: 0.3208 - lr: 1.0000e-08 - 2s/epoch - 25ms/step\n",
      "Epoch 37/128\n",
      "79/79 - 2s - loss: 0.4066 - acc: 0.3342 - val_loss: 0.4100 - val_acc: 0.3210 - lr: 1.0000e-08 - 2s/epoch - 25ms/step\n",
      "Training time: 76.65317058563232s\n"
     ]
    }
   ],
   "source": [
    "## DESIGN B\n",
    "import time \n",
    "start = time.time()\n",
    "historyB = modelB.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [es, reduce_lr])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "79/79 - 10s - loss: 0.4586 - acc: 0.7431 - val_loss: 2.2830 - val_acc: 0.6330 - lr: 1.0000e-04 - 10s/epoch - 122ms/step\n",
      "Epoch 2/128\n",
      "79/79 - 4s - loss: 0.3839 - acc: 0.7005 - val_loss: 5.7365 - val_acc: 0.7122 - lr: 1.0000e-04 - 4s/epoch - 51ms/step\n",
      "Epoch 3/128\n",
      "79/79 - 4s - loss: 0.3749 - acc: 0.7309 - val_loss: 9.0444 - val_acc: 0.7354 - lr: 1.0000e-04 - 4s/epoch - 53ms/step\n",
      "Epoch 4/128\n",
      "79/79 - 4s - loss: 0.3721 - acc: 0.7393 - val_loss: 6.4444 - val_acc: 0.7225 - lr: 1.0000e-04 - 4s/epoch - 51ms/step\n",
      "Epoch 5/128\n",
      "79/79 - 4s - loss: 0.3681 - acc: 0.7460 - val_loss: 4.1656 - val_acc: 0.8519 - lr: 1.0000e-04 - 4s/epoch - 52ms/step\n",
      "Epoch 6/128\n",
      "79/79 - 4s - loss: 0.3669 - acc: 0.7464 - val_loss: 1.6282 - val_acc: 0.6987 - lr: 1.0000e-04 - 4s/epoch - 53ms/step\n",
      "Epoch 7/128\n",
      "79/79 - 4s - loss: 0.3652 - acc: 0.7405 - val_loss: 1.1341 - val_acc: 0.7152 - lr: 1.0000e-04 - 4s/epoch - 51ms/step\n",
      "Epoch 8/128\n",
      "79/79 - 4s - loss: 0.3655 - acc: 0.7731 - val_loss: 0.5423 - val_acc: 0.7761 - lr: 1.0000e-04 - 4s/epoch - 51ms/step\n",
      "Epoch 9/128\n",
      "79/79 - 4s - loss: 0.3607 - acc: 0.7657 - val_loss: 0.4276 - val_acc: 0.8568 - lr: 1.0000e-04 - 4s/epoch - 53ms/step\n",
      "Epoch 10/128\n",
      "79/79 - 4s - loss: 0.3628 - acc: 0.7529 - val_loss: 0.3747 - val_acc: 0.8533 - lr: 1.0000e-04 - 4s/epoch - 51ms/step\n",
      "Epoch 11/128\n",
      "79/79 - 4s - loss: 0.3607 - acc: 0.7650 - val_loss: 0.3686 - val_acc: 0.8385 - lr: 1.0000e-04 - 4s/epoch - 52ms/step\n",
      "Epoch 12/128\n",
      "79/79 - 4s - loss: 0.3598 - acc: 0.7757 - val_loss: 0.3727 - val_acc: 0.7529 - lr: 1.0000e-04 - 4s/epoch - 51ms/step\n",
      "Epoch 13/128\n",
      "79/79 - 4s - loss: 0.3592 - acc: 0.7768 - val_loss: 0.3687 - val_acc: 0.7730 - lr: 1.0000e-04 - 4s/epoch - 51ms/step\n",
      "Epoch 14/128\n",
      "79/79 - 4s - loss: 0.3574 - acc: 0.7780 - val_loss: 0.3674 - val_acc: 0.6547 - lr: 1.0000e-04 - 4s/epoch - 50ms/step\n",
      "Epoch 15/128\n",
      "79/79 - 4s - loss: 0.3567 - acc: 0.7589 - val_loss: 0.3543 - val_acc: 0.6485 - lr: 1.0000e-04 - 4s/epoch - 51ms/step\n",
      "Epoch 16/128\n",
      "79/79 - 4s - loss: 0.3531 - acc: 0.7834 - val_loss: 0.3516 - val_acc: 0.7976 - lr: 1.0000e-05 - 4s/epoch - 53ms/step\n",
      "Epoch 17/128\n",
      "79/79 - 4s - loss: 0.3525 - acc: 0.7891 - val_loss: 0.3507 - val_acc: 0.8077 - lr: 1.0000e-05 - 4s/epoch - 52ms/step\n",
      "Training time: 75.21778845787048s\n"
     ]
    }
   ],
   "source": [
    "## DESIGN C\n",
    "import time \n",
    "start = time.time()\n",
    "historyC = modelC.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [es, reduce_lr])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:5250: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "FLOPS: 494470548\n"
     ]
    }
   ],
   "source": [
    "## Design A\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculae FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsA = get_flops(modelA, batch_size=1)\n",
    "print(f\"FLOPS: {flopsA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPS: 217993206\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculae FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsB = get_flops(modelB, batch_size=1)\n",
    "print(f\"FLOPS: {flopsB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPS: 1629751046\n"
     ]
    }
   ],
   "source": [
    "## Design C\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculae FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsC = get_flops(modelC, batch_size=1)\n",
    "print(f\"FLOPS: {flopsC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONALITY: INFERENCE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sample = train_generator[1][0][0].reshape(1,64,64,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 739ms/step\n",
      "Inference time: 1001.2958ms\n"
     ]
    }
   ],
   "source": [
    "## Design A\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelA.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 537ms/step\n",
      "Inference time: 568.9769ms\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelB.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 900ms/step\n",
      "Inference time: 956.5411ms\n"
     ]
    }
   ],
   "source": [
    "## Design AC\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelC.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORMANCE: COEFFICIENT OF DETERMINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare set of x values and y values for performance constraint\n",
    "X_values,y_values = [],[]\n",
    "for i in range(100):\n",
    "    values = next(val_generator)\n",
    "    # for j in range(values[0].shape[0]):\n",
    "    X_values.append(values[0])\n",
    "    y_values.append(values[1])\n",
    "\n",
    "## create X_values generator\n",
    "gen_X_values_1 = (x for x in X_values)\n",
    "gen_X_values_2 = (x for x in X_values)\n",
    "gen_X_values_3 = (x for x in X_values)\n",
    "y_values = [y  for y_set in y_values for y in y_set]\n",
    "y_values_0 = np.array(y_values)[:,:,:,0]\n",
    "y_values_1 = np.array(y_values)[:,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 10ms/step\n",
      "R^2 for class 0: 0.6145\n",
      "R^2 for class 1: 0.4850\n",
      "Average R^2 score: 0.5497\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_values = np.array(y_values)\n",
    "predictionsA = modelA.predict(gen_X_values_1)\n",
    "\n",
    "y_values_flat_0 = y_values[:, :, :, 0].flatten()\n",
    "predictionsA_flat_0 = predictionsA[:, :, :, 0].flatten()\n",
    "\n",
    "y_values_flat_1 = y_values[:, :, :, 1].flatten()\n",
    "predictionsA_flat_1 = predictionsA[:, :, :, 1].flatten()\n",
    "\n",
    "# Calculate R^2 score for each class\n",
    "r2_score_0 = r2_score(y_values_flat_0, predictionsA_flat_0)\n",
    "r2_score_1 = r2_score(y_values_flat_1, predictionsA_flat_1)\n",
    "\n",
    "print(f'R^2 for class 0: {r2_score_0:.4f}')\n",
    "print(f'R^2 for class 1: {r2_score_1:.4f}')\n",
    "\n",
    "print(f\"Average R^2 score: {(r2_score_0+r2_score_1)/2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 6ms/step\n",
      "R^2 for class 0: 0.5996\n",
      "R^2 for class 1: 0.2402\n",
      "Average R^2 score: 0.4199\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## PERFORMANCE: COEFFICIENT OF DETERMINATION\n",
    "from sklearn.metrics import roc_auc_score\n",
    "predictionsB = modelB.predict(gen_X_values_2)\n",
    "\n",
    "y_values_flat_0 = y_values[:, :, :, 0].flatten()\n",
    "predictionsB_flat_0 = predictionsB[:, :, :, 0].flatten()\n",
    "\n",
    "y_values_flat_1 = y_values[:, :, :, 1].flatten()\n",
    "predictionsB_flat_1 = predictionsB[:, :, :, 1].flatten()\n",
    "\n",
    "# Calculate R^2 score for each class\n",
    "r2_score_0 = r2_score(y_values_flat_0, predictionsB_flat_0)\n",
    "r2_score_1 = r2_score(y_values_flat_1, predictionsB_flat_1)\n",
    "\n",
    "print(f'R^2 for class 0: {r2_score_0:.4f}')\n",
    "print(f'R^2 for class 1: {r2_score_1:.4f}')\n",
    "\n",
    "print(f\"Average R^2 score: {(r2_score_0+r2_score_1)/2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s 12ms/step\n",
      "R^2 for class 0: -0.5446\n",
      "R^2 for class 1: -0.4950\n",
      "Average R^2 score: -0.5198\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## PERFORMANCE: COEFFICIENT OF DETERMINATION\n",
    "from sklearn.metrics import roc_auc_score\n",
    "predictionsC = modelC.predict(gen_X_values_3)\n",
    "\n",
    "y_values_flat_0 = y_values[:, :, :, 0].flatten()\n",
    "predictionsC_flat_0 = predictionsC[:, :, :, 0].flatten()\n",
    "\n",
    "y_values_flat_1 = y_values[:, :, :, 1].flatten()\n",
    "predictionsC_flat_1 = predictionsC[:, :, :, 1].flatten()\n",
    "\n",
    "# Calculate R^2 score for each class\n",
    "r2_score_0 = r2_score(y_values_flat_0, predictionsC_flat_0)\n",
    "r2_score_1 = r2_score(y_values_flat_1, predictionsC_flat_1)\n",
    "\n",
    "print(f'R^2 for class 0: {r2_score_0:.4f}')\n",
    "print(f'R^2 for class 1: {r2_score_1:.4f}')\n",
    "\n",
    "print(f\"Average R^2 score: {(r2_score_0+r2_score_1)/2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFFICIENCY: STORAGE CONSUMPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weight size in megabytes: 99.6718\n"
     ]
    }
   ],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsA = modelA.get_weights()\n",
    "total_sizeA = 0\n",
    "for weight in weightsA:\n",
    "    total_sizeA += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeA*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weight size in megabytes: 13.9723\n"
     ]
    }
   ],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsB = modelB.get_weights()\n",
    "total_sizeB = 0\n",
    "for weight in weightsB:\n",
    "    total_sizeB += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeB*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weight size in megabytes: 623.7215\n"
     ]
    }
   ],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsC = modelC.get_weights()\n",
    "total_sizeC = 0\n",
    "for weight in weightsC:\n",
    "    total_sizeC += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeC*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving final trained and constrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA.save('design_models/designA_v4.h5')\n",
    "modelB.save('design_models/designB_v4.h5')\n",
    "modelC.save('design_models/designC_v4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
