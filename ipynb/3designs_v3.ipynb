{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Flatten, LeakyReLU, ReLU, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, concatenate, Activation\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.backend import clear_session\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop , SGD\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "# from tensorflow.keras.regularizers import L2\n",
    "# from tensorflow.keras import metrics\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from shutil import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 196 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('.\\\\final_data.csv',  names=[\"path\",\"value\"])\n",
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    # # rescale=1./255\n",
    "        # rescale=1.,\n",
    "        # width_shift_range = 0.5,\n",
    "        # height_shift_range = 0.5, \n",
    "        # zoom_range = 0.2,\n",
    "        # shear_range = 0.2,\n",
    "        # horizontal_flip = True,\n",
    "        # vertical_flip = True,\n",
    "    #  channel_shift_range = 64.0,\n",
    "        # brightness_range = (0.7,1.0),\n",
    "        # rotation_range = 45,\n",
    "    )\n",
    "\n",
    "data_generator = generator.flow_from_dataframe(\n",
    "    df, \n",
    "    x_col=\"path\", \n",
    "    y_col=\"value\", \n",
    "    class_mode='raw', \n",
    "    batch_size=198,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = []\n",
    "Ys = []\n",
    "iterations = 4\n",
    "for i in range(iterations):\n",
    "    x,y = next(data_generator)\n",
    "    Xs.extend([np.array(value).astype(int) for value in x])\n",
    "    Ys.extend([np.array(value.replace(\"'\",\"\")[1:-1].split(', ')).astype(float) for value in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXPH = np.max(np.array(Ys)[:,1])\n",
    "MINPH = np.min(np.array(Ys)[:,1])\n",
    "\n",
    "MAXMOISTURE = np.max(np.array(Ys)[:,0])\n",
    "MINMOISTURE = np.min(np.array(Ys)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(image):\n",
    "    image = image.astype(np.uint8)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    binr = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    binr = np.invert(binr)\n",
    "\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    mask = cv2.erode(binr, kernel, iterations=3)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def process_image(image):\n",
    "    rgb_image = image.astype(np.uint8)\n",
    "\n",
    "    rgb_planes = cv2.split(rgb_image)\n",
    "    result_planes = []\n",
    "    for plane in rgb_planes:\n",
    "        processed_image = cv2.medianBlur(plane, 3)\n",
    "        result_planes.append(processed_image)\n",
    "    result = cv2.merge(result_planes)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(image, label, maxPh, minPh, maxMoisture, minMoisture):\n",
    "    mask = get_mask(np.array(image))\n",
    "    shape = mask.shape \n",
    "    array = np.ones(shape=(shape[0], shape[1], 1))\n",
    "\n",
    "    moisture = cv2.bitwise_and(array, array, mask=mask)\n",
    "    ph = cv2.bitwise_and(array, array, mask=mask)\n",
    "\n",
    "    moisture_value = (label[0] - minMoisture) / (maxMoisture - minMoisture)\n",
    "    ph_value = (label[1] - minPh) / (maxPh - minPh)\n",
    "    moisture[moisture > 0] = moisture_value\n",
    "    ph[ph > 0] = ph_value\n",
    "\n",
    "    output = np.stack([moisture, ph], axis=-1)  \n",
    "\n",
    "    return output\n",
    "\n",
    "# def process_label(image, label, maxPh, minPh, maxMoisture, minMoisture):\n",
    "#     # mask = get_mask(np.array(image))\n",
    "#     # shape = mask.shape \n",
    "#     # array = np.ones(shape=(shape[0], shape[1], 1))\n",
    "\n",
    "#     # moisture = cv2.bitwise_and(array, array, mask=mask)\n",
    "#     # ph = cv2.bitwise_and(array, array, mask=mask)\n",
    "\n",
    "#     moisture_value = (label[0] - minMoisture) / (maxMoisture - minMoisture)\n",
    "#     ph_value = (label[1] - minPh) / (maxPh - minPh)\n",
    "#     # moisture[moisture > 0] = moisture_value\n",
    "#     # ph[ph > 0] = ph_value\n",
    "\n",
    "#     output = np.stack([moisture_value, ph_value], axis=-1)  \n",
    "\n",
    "#     return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n"
     ]
    }
   ],
   "source": [
    "Y_values = []\n",
    "X_values = []\n",
    "for i, x in enumerate(Xs):\n",
    "    print(i)\n",
    "    Y_values.append(process_label(Xs[i], Ys[i], MAXPH, MINPH, MAXMOISTURE, MINMOISTURE))\n",
    "    # Y_values.append(process_label(Ys[i], MAXPH, MINPH, MAXMOISTURE, MINMOISTURE))\n",
    "    X_values.append(process_image(Xs[i]))\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_values,Y_values , \n",
    "                                #    random_state=23,  \n",
    "                                   test_size=0.20,  \n",
    "                                   shuffle=True) \n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "        width_shift_range = 0.2,\n",
    "        height_shift_range = 0.2, \n",
    "        zoom_range = 0.2,\n",
    "        # shear_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True,\n",
    "    #  channel_shift_range = 64.0,\n",
    "        # brightness_range = (0.6,1.0),\n",
    "        # rotation_range = 15,\n",
    "    ).flow(x=np.array(X_train), y=y_train, batch_size=16) \n",
    "\n",
    "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    "    ).flow(x=np.array(X_test), y=y_test, batch_size=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "[[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.48717949 0.13888889]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "for i in next(train_generator)[1][0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = \"path\"\n",
    "y_col = \"pH\"\n",
    "batch_size = 16\n",
    "epochs = 1024\n",
    "lr = 1e-5\n",
    "image_size = (IMAGE_SIZE[0],IMAGE_SIZE[1])\n",
    "channels = 3\n",
    "shuffle = True\n",
    "class_mode =\"raw\"\n",
    "color_mode = \"rgb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Designs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of Designs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Convolutional Block\n",
    "def conv_block(inputs, num_filters):\n",
    "\t# Applying the sequence of Convolutional, Batch Normalization\n",
    "\t# and Activation Layers to the input tensor\n",
    "\tx = Sequential([\n",
    "\t\t# Convolutional Layer\n",
    "\t\tConv2D(num_filters, 1, padding='same'),\n",
    "\t\t# Batch Normalization Layer\n",
    "\t\tBatchNormalization(),\n",
    "\t\t# Activation Layer\n",
    "\t\tReLU(),\n",
    "\t\t# Convolutional Layer\n",
    "\t\tConv2D(num_filters, 1, padding='same'),\n",
    "\t\t# Batch Normalization Layer\n",
    "\t\tBatchNormalization(),\n",
    "\t\t# Activation Layer\n",
    "\t\tReLU()\n",
    "\t])(inputs)\n",
    "\n",
    "\t# Returning the output of the Convolutional Block\n",
    "\treturn x\n",
    "def dense_block(units, dropout_rate):\n",
    "    return Sequential([\n",
    "        Dense(units, activation='relu'),\n",
    "    ])\n",
    "# Defining the Unet++ Model\n",
    "def unet_plus_plus_model(hp):\n",
    "\tinputs = Input(shape=image_size+(3,))\n",
    "\thp_filters = hp.Choice('filters',values = [16,32,64])\n",
    "\t# Encoding Path\n",
    "\tx_00 = conv_block(inputs, hp_filters)\n",
    "\tx_10 = conv_block(MaxPooling2D()(x_00), hp_filters*2)\n",
    "\tx_20 = conv_block(MaxPooling2D()(x_10), hp_filters*4)\n",
    "\tx_30 = conv_block(MaxPooling2D()(x_20), hp_filters*8)\n",
    "\tx_40 = conv_block(MaxPooling2D()(x_30), hp_filters*16)\n",
    "\t\n",
    "\thp.Boolean(\"dropouts\", default=False)\n",
    "\thp.Boolean(\"batch_normalization\", default=False)\n",
    "\tflattened = Flatten()(x_40)\n",
    "\tdense = dense_block(hp_filters*hp_filters, 0.2)(flattened)\n",
    "\tif hp.Boolean(\"dropouts\"):\n",
    "\t\tdense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\tif hp.Boolean(\"batch_normalization\"):\n",
    "\t\tdense = tf.keras.layers.BatchNormalization()(dense)\n",
    "\tdense = dense_block(hp_filters*hp_filters, 0.2)(dense)\n",
    "\tif hp.Boolean(\"dropouts\"):\n",
    "\t\tdense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\tif hp.Boolean(\"batch_normalization\"):\n",
    "\t\tdense = tf.keras.layers.BatchNormalization()(dense)\n",
    "\tdense = dense_block(hp_filters*hp_filters, 0.2)(dense);hp.Boolean(\"4th dense\", default=False);\n",
    "\tif hp.Boolean(\"4th dense\"):\n",
    "\t\tdense = dense_block(hp_filters*hp_filters, 0.2)(dense)\n",
    "\t\tif hp.Boolean(\"dropouts\"):\n",
    "\t\t\tdense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\t\tif hp.Boolean(\"batch_normalization\"):\n",
    "\t\t\tdense = tf.keras.layers.BatchNormalization()(dense)\n",
    "\t# dense = dense_block(4096, 0.2)(dense)\n",
    "\t# dense = dense_block(4096, 0.2)(dense)\n",
    "\treshaped = tf.keras.layers.Reshape((x_40.shape[1], x_40.shape[1], (hp_filters*hp_filters)//(x_40.shape[1]*x_40.shape[1])))(dense)  # Reshape to reintroduce spatial dimensions\n",
    "\n",
    "\n",
    "\t# Nested Decoding Path\n",
    "\tx_01 = conv_block(concatenate(\n",
    "\t\t[x_00, UpSampling2D()(x_10)]), hp_filters)\n",
    "\tx_11 = conv_block(concatenate(\n",
    "\t\t[x_10, UpSampling2D()(x_20)]), hp_filters*2)\n",
    "\tx_21 = conv_block(concatenate(\n",
    "\t\t[x_20, UpSampling2D()(x_30)]), hp_filters*4)\n",
    "\tx_31 = conv_block(concatenate(\n",
    "\t\t[x_30, UpSampling2D()(reshaped)]), hp_filters*8)\n",
    "\n",
    "\tx_02 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, UpSampling2D()(x_11)]), hp_filters)\n",
    "\tx_12 = conv_block(concatenate(\n",
    "\t\t[x_10, x_11, UpSampling2D()(x_21)]), hp_filters*2)\n",
    "\tx_22 = conv_block(concatenate(\n",
    "\t\t[x_20, x_21, UpSampling2D()(x_31)]), hp_filters*4)\n",
    "\n",
    "\tx_03 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, x_02, UpSampling2D()(x_12)]), hp_filters)\n",
    "\tx_13 = conv_block(concatenate(\n",
    "\t\t[x_10, x_11, x_12, UpSampling2D()(x_22)]), hp_filters*2)\n",
    "\n",
    "\tx_04 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), hp_filters)\n",
    "\t\n",
    "\toutputs = tf.keras.layers.Conv2D(2, 1, activation='sigmoid')(x_04);print(outputs.shape)\n",
    "\n",
    "\t# Creating the model\n",
    "\tmodel = tf.keras.Model(\n",
    "\t\tinputs=inputs, outputs=outputs, name='Unet_plus_plus');lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5]);model.compile(optimizer= tf.keras.optimizers.Adam(lr=lr), loss= [\"binary_crossentropy\"], metrics=['acc'])\n",
    "\t# Returning the model\n",
    "\treturn model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Dropout, Concatenate, Flatten, Dense, Reshape\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "# def convolution_block(inputs, num_filters, kernel_size=3, padding=\"same\", use_bias=False):\n",
    "#     x = Conv2D(num_filters, kernel_size=kernel_size, padding=padding, use_bias=use_bias, kernel_initializer=HeNormal())(inputs)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "#     return x\n",
    "\n",
    "# def upsample_block(inputs, skip_features, num_filters):\n",
    "#     x = UpSampling2D((2, 2))(inputs)\n",
    "#     x = Conv2D(num_filters, (2, 2), padding=\"same\")(x)\n",
    "#     x = Concatenate()([x, skip_features])\n",
    "#     x = convolution_block(x, num_filters)\n",
    "#     return x\n",
    "\n",
    "# def dense_upsampling_block(inputs, skip_features, num_filters):\n",
    "#     x = UpSampling2D((2, 2))(inputs)\n",
    "#     x = Conv2D(num_filters, (2, 2), padding=\"same\")(x)\n",
    "#     x = Concatenate()([x, skip_features])\n",
    "#     x = convolution_block(x, num_filters)\n",
    "#     return x\n",
    "\n",
    "# def unet_plus_plus_model(hp,input_shape=(64,64,3)):\n",
    "#     inputs = Input(input_shape)\n",
    "\n",
    "#     resnet50 = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "#     resnet50.trainable = True\n",
    "\n",
    "#     # Encoder\n",
    "#     s1 = resnet50.get_layer(\"conv1_relu\").output\n",
    "#     s2 = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "#     s3 = resnet50.get_layer(\"conv3_block4_out\").output\n",
    "#     s4 = resnet50.get_layer(\"conv4_block6_out\").output\n",
    "\n",
    "#     b1 = resnet50.get_layer(\"conv5_block3_out\").output\n",
    "#     # Flatten the bottleneck output\n",
    "#     x = Flatten()(b1);UNITS = hp.Choice('units',values = [256,512,1024,2048]);DROPOUT = hp.Float('dropout',min_value=0.0, max_value=0.5, step=0.1)\n",
    "    \n",
    "#     BATCHNORM = hp.Boolean('batchnorm',default=False)\n",
    "#     # Dense layers between encoder and decoder\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     # Reshape back to spatial dimensions for the decoder\n",
    "#     SIZE = input_shape[0] // 32  # Assuming input size is a multiple of 32\n",
    "#     if BATCHNORM:\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Dense(SIZE * SIZE * 2048, activation='relu')(x)\n",
    "#     x = Reshape((SIZE, SIZE, 2048))(x)\n",
    "\n",
    "#     # Nested U-Net\n",
    "#     d4_2 = dense_upsampling_block(x, s4, 512)\n",
    "#     d3_2 = dense_upsampling_block(d4_2, s3, 256)\n",
    "#     d2_2 = dense_upsampling_block(d3_2, s2, 128)\n",
    "#     d1_2 = dense_upsampling_block(d2_2, s1, 64)\n",
    "\n",
    "#     d4_1 = upsample_block(x, s4, 512)\n",
    "#     d3_1 = upsample_block(d4_1, s3, 256)\n",
    "#     d2_1 = upsample_block(d3_1, s2, 128)\n",
    "#     d1_1 = upsample_block(d2_1, s1, 64)\n",
    "\n",
    "#     outputs = UpSampling2D()(d1_1)\n",
    "#     outputs = Conv2D(2, (1, 1), padding=\"same\", activation=\"sigmoid\")(outputs)\n",
    "#     model = tf.keras.Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "    \n",
    "#     # Hyperparameter choice for optimizer\n",
    "#     optimizer_choice = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    \n",
    "#     # Select optimizer\n",
    "#     if optimizer_choice == 'adam':\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "#     elif optimizer_choice == 'sgd':\n",
    "#         optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "#     elif optimizer_choice == 'rmsprop':\n",
    "#         optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    \n",
    "#     model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "#     return model\n",
    "\n",
    "# # # Define input shape and build model\n",
    "# # input_shape = (256, 256, 3)\n",
    "# # model = build_resnet50_unetpp(input_shape)\n",
    "# # model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# # # Print model summary\n",
    "# # model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Dropout, Concatenate, Flatten, Dense, Reshape\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "# resnet50 = ResNet50(include_top=False, weights='imagenet')\n",
    "# print(resnet50.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the convolutional block\n",
    "# def conv_block(inputs, num_filters):\n",
    "#     x = Sequential([\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU(),\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU()\n",
    "#     ])(inputs)\n",
    "#     return x\n",
    "\n",
    "# # Define the dense block\n",
    "# def dense_block(units, dropout_rate):\n",
    "#     return Sequential([\n",
    "#         Dense(units, activation='relu'),\n",
    "#         Dropout(dropout_rate)\n",
    "#     ])\n",
    "\n",
    "# # Define the Unet++ model\n",
    "# def unet_plus_plus_model(hp, image_size=(224, 224)):\n",
    "#     inputs = Input(shape=image_size + (3,))\n",
    "\n",
    "#     # Load ResNet50 as encoder\n",
    "#     resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "#     encoder_layers = [resnet.get_layer(name).output for name in ['conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block6_out', 'conv5_block3_out']]\n",
    "    \n",
    "#     # Encoding path\n",
    "#     x_00 = encoder_layers[0]\n",
    "#     x_10 = encoder_layers[1]\n",
    "#     x_20 = encoder_layers[2]\n",
    "#     x_30 = encoder_layers[3]\n",
    "#     x_40 = encoder_layers[4]\n",
    "\n",
    "#     flattened = Flatten()(x_40)\n",
    "#     dense = dense_block(2048, 0.2)(flattened)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(2048, 0.2)(dense)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(2048, 0.2)(dense)\n",
    "#     if hp.Boolean(\"4th_dense\"):\n",
    "#         dense = dense_block(2048, 0.2)(dense)\n",
    "#         if hp.Boolean(\"dropouts\"):\n",
    "#             dense = Dropout(0.5)(dense)\n",
    "#         if hp.Boolean(\"batch_normalization\"):\n",
    "#             dense = BatchNormalization()(dense)\n",
    "\n",
    "#     reshaped = Reshape((x_40.shape[1], x_40.shape[2], 32))(dense)\n",
    "\n",
    "#     # Nested decoding path\n",
    "#     x_01 = conv_block(concatenate([x_00, UpSampling2D()(x_10)]), 64)\n",
    "#     x_11 = conv_block(concatenate([x_10, UpSampling2D()(x_20)]), 128)\n",
    "#     x_21 = conv_block(concatenate([x_20, UpSampling2D()(x_30)]), 256)\n",
    "#     x_31 = conv_block(concatenate([x_30, UpSampling2D()(reshaped)]), 512)\n",
    "\n",
    "#     x_02 = conv_block(concatenate([x_00, x_01, UpSampling2D()(x_11)]), 64)\n",
    "#     x_12 = conv_block(concatenate([x_10, x_11, UpSampling2D()(x_21)]), 128)\n",
    "#     x_22 = conv_block(concatenate([x_20, x_21, UpSampling2D()(x_31)]), 256)\n",
    "\n",
    "#     x_03 = conv_block(concatenate([x_00, x_01, x_02, UpSampling2D()(x_12)]), 64)\n",
    "#     x_13 = conv_block(concatenate([x_10, x_11, x_12, UpSampling2D()(x_22)]), 128)\n",
    "\n",
    "#     x_04 = conv_block(concatenate([x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), 64)\n",
    "\n",
    "#     outputs = Conv2D(2, 1, activation='sigmoid')(x_04)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, ReLU, Flatten, Dense, Dropout, concatenate, Reshape\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Define the convolutional block\n",
    "def conv_block(inputs, num_filters):\n",
    "    x = Sequential([\n",
    "        Conv2D(num_filters, 3, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        Conv2D(num_filters, 3, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        ReLU()\n",
    "    ])(inputs)\n",
    "    return x\n",
    "\n",
    "# Define the dense block\n",
    "def dense_block(units, dropout_rate):\n",
    "    return Sequential([\n",
    "        Dense(units, activation='relu'),\n",
    "        Dropout(dropout_rate)\n",
    "    ])\n",
    "\n",
    "# Define the Unet++ model\n",
    "def unet_plus_plus_model(hp, image_size=(32, 32)):\n",
    "    inputs = Input(shape=image_size + (3,))\n",
    "\n",
    "    # Load ResNet50 as encoder\n",
    "    resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    encoder_layers = [resnet.get_layer(name).output for name in ['conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block6_out', 'conv5_block3_out']]\n",
    "    \n",
    "    # Encoding path\n",
    "    x_00 = encoder_layers[0]\n",
    "    x_10 = encoder_layers[1]\n",
    "    x_20 = encoder_layers[2]\n",
    "    x_30 = encoder_layers[3]\n",
    "    x_40 = encoder_layers[4]\n",
    "    print(x_40.shape)\n",
    "    flattened = Flatten()(x_40)\n",
    "    dense = dense_block(2048, 0.2)(flattened)\n",
    "    if hp.Boolean(\"dropouts\"):\n",
    "        dense = Dropout(0.5)(dense)\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        dense = BatchNormalization()(dense)\n",
    "    dense = dense_block(2048, 0.2)(dense)\n",
    "    if hp.Boolean(\"dropouts\"):\n",
    "        dense = Dropout(0.5)(dense)\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        dense = BatchNormalization()(dense)\n",
    "    dense = dense_block(2048, 0.2)(dense)\n",
    "    if hp.Boolean(\"4th_dense\"):\n",
    "        dense = dense_block(2048, 0.2)(dense)\n",
    "        if hp.Boolean(\"dropouts\"):\n",
    "            dense = Dropout(0.5)(dense)\n",
    "        if hp.Boolean(\"batch_normalization\"):\n",
    "            dense = BatchNormalization()(dense)\n",
    "    reshaped = tf.keras.layers.Reshape((x_40.shape[1], x_40.shape[1], 2048))(dense)  # Reshape to reintroduce spatial dimensions\n",
    "\n",
    "    # Nested decoding path\n",
    "    x_01 = conv_block(concatenate([x_00, UpSampling2D()(x_10)]), 64)\n",
    "    x_11 = conv_block(concatenate([x_10, UpSampling2D()(x_20)]), 128)\n",
    "    x_21 = conv_block(concatenate([x_20, UpSampling2D()(x_30)]), 256)\n",
    "    x_31 = conv_block(concatenate([x_30, UpSampling2D()(reshaped)]), 512)\n",
    "\n",
    "    x_02 = conv_block(concatenate([x_00, x_01, UpSampling2D()(x_11)]), 64)\n",
    "    x_12 = conv_block(concatenate([x_10, x_11, UpSampling2D()(x_21)]), 128)\n",
    "    x_22 = conv_block(concatenate([x_20, x_21, UpSampling2D()(x_31)]), 256)\n",
    "\n",
    "    x_03 = conv_block(concatenate([x_00, x_01, x_02, UpSampling2D()(x_12)]), 64)\n",
    "    x_13 = conv_block(concatenate([x_10, x_11, x_12, UpSampling2D()(x_22)]), 128)\n",
    "\n",
    "    x_04 = conv_block(concatenate([x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), 64)\n",
    "    \n",
    "    x_final = UpSampling2D(size=(2, 2))(x_04)\n",
    "\n",
    "    outputs = Conv2D(2, 1, activation='sigmoid')(x_final)\n",
    "    print(outputs.shape)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "    lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the convolutional block\n",
    "# def conv_block(inputs, num_filters):\n",
    "#     x = Sequential([\n",
    "#         Conv2D(num_filters, 1, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU(),\n",
    "#         Conv2D(num_filters, 1, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU()\n",
    "#     ])(inputs)\n",
    "#     return x\n",
    "\n",
    "# # Define the dense block\n",
    "# def dense_block(units, dropout_rate):\n",
    "#     return Sequential([\n",
    "#         Dense(units, activation='relu')\n",
    "#     ])\n",
    "\n",
    "# # Define the Unet++ model\n",
    "# def unet_plus_plus_model(hp):\n",
    "#     inputs = Input(shape=(32, 32, 3))\n",
    "#     hp_filters = hp.Choice('filters', values=[16, 32, 64,])\n",
    "\n",
    "#     # Encoding Path\n",
    "#     x_00 = conv_block(inputs, hp_filters)\n",
    "#     x_10 = conv_block(MaxPooling2D()(x_00), hp_filters * 2)\n",
    "#     x_20 = conv_block(MaxPooling2D()(x_10), hp_filters * 4)\n",
    "#     x_30 = conv_block(MaxPooling2D()(x_20), hp_filters * 8)\n",
    "#     x_40 = conv_block(MaxPooling2D()(x_30), hp_filters * 16)\n",
    "\n",
    "#     # Dense layers and reshape\n",
    "#     flattened = Flatten()(x_40)\n",
    "#     dense = dense_block(hp_filters * hp_filters, 0.2)(flattened)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(hp_filters * hp_filters, 0.2)(dense)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense = Dropout(0.5)(dense)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense = BatchNormalization()(dense)\n",
    "#     dense = dense_block(hp_filters * hp_filters, 0.2)(dense)\n",
    "#     if hp.Boolean(\"4th_dense\"):\n",
    "#         dense = dense_block(hp_filters * hp_filters, 0.2)(dense)\n",
    "#         if hp.Boolean(\"dropouts\"):\n",
    "#             dense = Dropout(0.5)(dense)\n",
    "#         if hp.Boolean(\"batch_normalization\"):\n",
    "#             dense = BatchNormalization()(dense)\n",
    "\n",
    "#     reshaped = tf.keras.layers.Reshape((x_40.shape[1], x_40.shape[1], (hp_filters * hp_filters) // (x_40.shape[1] * x_40.shape[1])))(dense)\n",
    "\n",
    "#     # Nested Decoding Path\n",
    "#     x_01 = conv_block(concatenate([x_00, UpSampling2D()(x_10)]), hp_filters)\n",
    "#     x_11 = conv_block(concatenate([x_10, UpSampling2D()(x_20)]), hp_filters * 2)\n",
    "#     x_21 = conv_block(concatenate([x_20, UpSampling2D()(x_30)]), hp_filters * 4)\n",
    "#     x_31 = conv_block(concatenate([x_30, UpSampling2D()(reshaped)]), hp_filters * 8)\n",
    "\n",
    "#     x_02 = conv_block(concatenate([x_00, x_01, UpSampling2D()(x_11)]), hp_filters)\n",
    "#     x_12 = conv_block(concatenate([x_10, x_11, UpSampling2D()(x_21)]), hp_filters * 2)\n",
    "#     x_22 = conv_block(concatenate([x_20, x_21, UpSampling2D()(x_31)]), hp_filters * 4)\n",
    "\n",
    "#     x_03 = conv_block(concatenate([x_00, x_01, x_02, UpSampling2D()(x_12)]), hp_filters)\n",
    "#     x_13 = conv_block(concatenate([x_10, x_11, x_12, UpSampling2D()(x_22)]), hp_filters * 2)\n",
    "\n",
    "#     x_04 = conv_block(concatenate([x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), hp_filters)\n",
    "\n",
    "#     # Flattening the final output\n",
    "#     flat_output = Flatten()(x_04)\n",
    "\n",
    "#     # Adding Dense layers\n",
    "#     dense_output = Dense(hp_filters * hp_filters, activation='relu')(flat_output)\n",
    "#     if hp.Boolean(\"dropouts\"):\n",
    "#         dense_output = Dropout(0.5)(dense_output)\n",
    "#     if hp.Boolean(\"batch_normalization\"):\n",
    "#         dense_output = BatchNormalization()(dense_output)\n",
    "\n",
    "#     # Final Dense layer with sigmoid activation for binary classification\n",
    "#     outputs = Dense(2, activation='sigmoid')(dense_output)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Flatten, Dense, Dropout, BatchNormalization, ReLU\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "# def conv_block(inputs, num_filters):\n",
    "#     x = Sequential([\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU(),\n",
    "#         Conv2D(num_filters, 3, padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         ReLU()\n",
    "#     ])(inputs)\n",
    "#     return x\n",
    "\n",
    "# def dense_block(units):\n",
    "#     return Sequential([\n",
    "#         Dense(units, activation='relu'),\n",
    "#         Dropout(0.5),\n",
    "#         BatchNormalization()\n",
    "#     ])\n",
    "\n",
    "# def unet_plus_plus_model(hp):\n",
    "#     inputs = Input(shape=(128, 128, 3))\n",
    "\n",
    "#     # Encoder with ResNet50V2\n",
    "#     base_model = ResNet50V2(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "#     base_model.trainable = False  # Freeze the model\n",
    "\n",
    "#     # Collect skip connections\n",
    "#     s1 = base_model.get_layer(\"conv2_block3_out\").output  # 128x128\n",
    "#     s2 = base_model.get_layer(\"conv3_block4_out\").output  # 64x64\n",
    "#     s3 = base_model.get_layer(\"conv4_block6_out\").output  # 32x32\n",
    "#     s4 = base_model.get_layer(\"post_relu\").output         # 16x16\n",
    "\n",
    "#     # Nested Decoding Path\n",
    "#     x_00 = s1\n",
    "#     x_10 = conv_block(s2, hp.Choice('filters_x10', values=[64, 128, 256]))\n",
    "#     x_20 = conv_block(s3, hp.Choice('filters_x20', values=[128, 256, 512]))\n",
    "#     x_30 = conv_block(s4, hp.Choice('filters_x30', values=[256, 512, 1024]))\n",
    "\n",
    "#     # Up-sampling and concatenation\n",
    "#     x_01 = conv_block(concatenate([x_00, UpSampling2D()(x_10)]), hp.Choice('filters_x01', values=[64, 128, 256]))\n",
    "#     x_11 = conv_block(concatenate([x_10, UpSampling2D()(x_20)]), hp.Choice('filters_x11', values=[128, 256, 512]))\n",
    "#     x_21 = conv_block(concatenate([x_20, UpSampling2D()(x_30)]), hp.Choice('filters_x21', values=[256, 512, 1024]))\n",
    "\n",
    "#     x_02 = conv_block(concatenate([x_00, x_01, UpSampling2D()(x_11)]), hp.Choice('filters_x02', values=[64, 128, 256]))\n",
    "#     x_12 = conv_block(concatenate([x_10, x_11, UpSampling2D()(x_21)]), hp.Choice('filters_x12', values=[128, 256, 512]))\n",
    "\n",
    "#     x_03 = conv_block(concatenate([x_00, x_01, x_02, UpSampling2D()(x_12)]), hp.Choice('filters_x03', values=[64, 128, 256]))\n",
    "\n",
    "#     # Final convolution\n",
    "#     outputs = Conv2D(2, 1, activation='sigmoid')(x_03)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name='ResNet50V2_Unet_plus_plus')\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Dropout, Concatenate, Flatten, Dense, Reshape\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "def convolution_block(inputs, num_filters, kernel_size=3, padding=\"same\", use_bias=False):\n",
    "    x = Conv2D(num_filters, kernel_size=kernel_size, padding=padding, use_bias=use_bias, kernel_initializer=HeNormal())(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def upsample_block(inputs, skip_features, num_filters):\n",
    "    x = UpSampling2D((2, 2))(inputs)\n",
    "    x = Conv2D(num_filters, (2, 2), padding=\"same\")(x)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = convolution_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def dense_upsampling_block(inputs, skip_features, num_filters):\n",
    "    x = UpSampling2D((2, 2))(inputs)\n",
    "    x = Conv2D(num_filters, (2, 2), padding=\"same\")(x)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = convolution_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def unet_plus_plus_model(hp,input_shape=(32,32,3)):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    resnet50 = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    resnet50.trainable = True\n",
    "\n",
    "    # Encoder\n",
    "    s1 = resnet50.get_layer(\"conv1_relu\").output\n",
    "    s2 = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "    s3 = resnet50.get_layer(\"conv3_block4_out\").output\n",
    "    s4 = resnet50.get_layer(\"conv4_block6_out\").output\n",
    "\n",
    "    b1 = resnet50.get_layer(\"conv5_block3_out\").output\n",
    "    # Flatten the bottleneck output\n",
    "    x = Flatten()(b1);UNITS = hp.Choice('units',values = [256,512,1024,2048], default=256);DROPOUT = hp.Float('dropout',min_value=0.0, max_value=0.5, step=0.1, default=0.0)\n",
    "    \n",
    "    BATCHNORM = hp.Boolean('batchnorm',default=False)\n",
    "    # Dense layers between encoder and decoder\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    # Reshape back to spatial dimensions for the decoder\n",
    "    SIZE = input_shape[0] // 32  # Assuming input size is a multiple of 32\n",
    "    if BATCHNORM:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Dense(SIZE * SIZE * 2048, activation='relu')(x)\n",
    "    x = Reshape((SIZE, SIZE, 2048))(x)\n",
    "\n",
    "    # Nested U-Net\n",
    "    d4_2 = dense_upsampling_block(x, s4, 512)\n",
    "    d3_2 = dense_upsampling_block(d4_2, s3, 256)\n",
    "    d2_2 = dense_upsampling_block(d3_2, s2, 128)\n",
    "    d1_2 = dense_upsampling_block(d2_2, s1, 64)\n",
    "\n",
    "    d4_1 = upsample_block(x, s4, 512)\n",
    "    d3_1 = upsample_block(d4_1, s3, 256)\n",
    "    d2_1 = upsample_block(d3_1, s2, 128)\n",
    "    d1_1 = upsample_block(d2_1, s1, 64)\n",
    "\n",
    "    outputs = UpSampling2D()(d1_1)\n",
    "    outputs = Conv2D(2, (1, 1), padding=\"same\", activation=\"sigmoid\")(outputs)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "    \n",
    "    lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5],  default=1e-3)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "# # Define input shape and build model\n",
    "# input_shape = (256, 256, 3)\n",
    "# model = build_resnet50_unetpp(input_shape)\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# # Print model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 01m 25s]\n",
      "val_loss: 0.30451881885528564\n",
      "\n",
      "Best val_loss So Far: 0.2590216398239136\n",
      "Total elapsed time: 01h 12m 32s\n",
      "{'units': 256, 'dropout': 0.4, 'batchnorm': False, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "tunerA = kt.BayesianOptimization(unet_plus_plus_model,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 30,\n",
    "                     project_name='design_a',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8)\n",
    "\n",
    "tunerA.search(train_generator, epochs=50, validation_data=val_generator, callbacks=[stop_early, reduce_lr])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsA=tunerA.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "print(best_hpsA.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hpsA.values['filters']=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Unet_plus_plus\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 38, 38, 3)    0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 16, 16, 64)   9472        ['conv1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 16, 16, 64)   256         ['conv1_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 16, 16, 64)   0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 18, 18, 64)   0           ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 8, 8, 64)     0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 8, 8, 64)     4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 8, 8, 256)    16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 8, 8, 256)    0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 8, 8, 64)     16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 8, 8, 256)    0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 8, 8, 64)     16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 8, 8, 256)    0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 4, 4, 128)    32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 4, 4, 512)    131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 4, 4, 512)    0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 4, 4, 512)    0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 4, 4, 512)    0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 4, 4, 512)    0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 2, 2, 256)    131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 2, 2, 1024)   525312      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                                                  'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block1_out[0][0]',       \n",
      "                                                                  'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block2_out[0][0]',       \n",
      "                                                                  'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block3_out[0][0]',       \n",
      "                                                                  'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block4_out[0][0]',       \n",
      "                                                                  'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block5_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block5_out[0][0]',       \n",
      "                                                                  'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block6_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 1, 1, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 1, 1, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 1, 1, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 1, 1, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 2048)         0           ['conv5_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          524544      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 256)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 256)          65792       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 256)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 256)          65792       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 256)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 256)          65792       ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 2048)         526336      ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 1, 2048)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " up_sampling2d_13 (UpSampling2D  (None, 2, 2, 2048)  0           ['reshape_1[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 2, 2, 512)    4194816     ['up_sampling2d_13[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 2, 2, 1536)   0           ['conv2d_25[0][0]',              \n",
      "                                                                  'conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 2, 2, 512)    7077888     ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 2, 2, 512)   2048        ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 2, 2, 512)    0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_14 (UpSampling2D  (None, 4, 4, 512)   0           ['activation_12[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 4, 4, 256)    524544      ['up_sampling2d_14[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 4, 4, 768)    0           ['conv2d_27[0][0]',              \n",
      "                                                                  'conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 4, 4, 256)    1769472     ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 4, 4, 256)   1024        ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 4, 4, 256)    0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_15 (UpSampling2D  (None, 8, 8, 256)   0           ['activation_13[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 8, 8, 128)    131200      ['up_sampling2d_15[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 8, 8, 384)    0           ['conv2d_29[0][0]',              \n",
      "                                                                  'conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 128)    442368      ['concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 8, 8, 128)   512         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_16 (UpSampling2D  (None, 16, 16, 128)  0          ['activation_14[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 16, 16, 64)   32832       ['up_sampling2d_16[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 16, 16, 128)  0           ['conv2d_31[0][0]',              \n",
      "                                                                  'conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 16, 16, 64)   73728       ['concatenate_15[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 64)  256         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_17 (UpSampling2D  (None, 32, 32, 64)  0           ['activation_15[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 32, 32, 2)    130         ['up_sampling2d_17[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 39,086,786\n",
      "Trainable params: 39,031,746\n",
      "Non-trainable params: 55,040\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the model with the best hp.\n",
    "modelA = unet_plus_plus_model(best_hpsA)\n",
    "modelA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 8s 80ms/step - loss: 0.4188 - acc: 0.8958 - val_loss: 1.4745 - val_acc: 0.8799\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.3094 - acc: 0.8981 - val_loss: 0.5191 - val_acc: 0.8838\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2940 - acc: 0.8905 - val_loss: 0.4279 - val_acc: 0.8838\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2897 - acc: 0.8912 - val_loss: 0.4983 - val_acc: 0.8838\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2864 - acc: 0.8954 - val_loss: 0.4421 - val_acc: 0.8838\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2847 - acc: 0.8823 - val_loss: 0.4086 - val_acc: 0.8838\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2836 - acc: 0.8910 - val_loss: 0.4368 - val_acc: 0.8838\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2832 - acc: 0.8910 - val_loss: 0.3524 - val_acc: 0.8838\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2794 - acc: 0.8842 - val_loss: 0.3523 - val_acc: 0.8838\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2795 - acc: 0.8891 - val_loss: 0.3408 - val_acc: 0.8669\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2790 - acc: 0.8843 - val_loss: 0.3000 - val_acc: 0.8822\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2805 - acc: 0.8821 - val_loss: 0.3067 - val_acc: 0.8647\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2785 - acc: 0.8833 - val_loss: 0.2977 - val_acc: 0.8838\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2769 - acc: 0.8881 - val_loss: 0.3066 - val_acc: 0.8838\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2779 - acc: 0.8794 - val_loss: 0.3015 - val_acc: 0.8838\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2760 - acc: 0.8818 - val_loss: 0.3407 - val_acc: 0.8801\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2784 - acc: 0.8802 - val_loss: 0.3183 - val_acc: 0.8838\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 2s 62ms/step - loss: 0.2763 - acc: 0.8846 - val_loss: 0.2969 - val_acc: 0.8839\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2751 - acc: 0.8728 - val_loss: 0.2920 - val_acc: 0.8838\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2750 - acc: 0.8813 - val_loss: 0.2953 - val_acc: 0.8838\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2738 - acc: 0.8836 - val_loss: 0.3147 - val_acc: 0.8718\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.2735 - acc: 0.8851 - val_loss: 0.3236 - val_acc: 0.8838\n",
      "Epoch 23/100\n",
      " 4/40 [==>...........................] - ETA: 1s - loss: 0.2841 - acc: 0.8934"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodelA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\nealb\\AppData\\Local\\miniconda3\\envs\\pd\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelA.fit(train_generator, validation_data=val_generator, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#    Manual Training with y data alteration\n",
    "# '''\n",
    "# ## Variable declaration\n",
    "# epochs = 512\n",
    "# steps_per_epoch = 512 // batch_size + 1\n",
    "# min_lr = 1e-8\n",
    "# factor = 0.1\n",
    "# SCHEDULE_EPOCH = 50\n",
    "# STEPS = 50\n",
    "# MONITOR = \"loss\"\n",
    "# PATIENCE = 10\n",
    "# WEIGHTS = []\n",
    "\n",
    "# historyA = {\"history\": {\"loss\": [], \"mae\": [], \"val_loss\": [], \"val_mae\": []}}\n",
    "# for e in range(epochs):\n",
    "#     for i, (images, y_batch) in enumerate(train_generator):\n",
    "#         new_y_batch = []\n",
    "\n",
    "#         ## Train data y alteration\n",
    "#         for x, img in enumerate(images):\n",
    "#             array = np.ones(image_size + (3,))\n",
    "#             array *= img > 0\n",
    "#             array[array > 0] = y_batch[x]\n",
    "#             new_y_batch.append(array)\n",
    "#         new_y_batch = np.array(new_y_batch)\n",
    "#         loss = modelA.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "\n",
    "#         if i >= steps_per_epoch:  # manually detect the end of the epoch train set\n",
    "#             historyA[\"history\"][\"loss\"].append(loss[0])\n",
    "#             historyA[\"history\"][\"mae\"].append(loss[1])\n",
    "#             break\n",
    "\n",
    "#     for i, (images, y_batch) in enumerate(val_generator):\n",
    "#         new_y_batch = []\n",
    "\n",
    "#         ## Val data y alteration\n",
    "#         for x, img in enumerate(images):\n",
    "#             array = np.ones(image_size + (3,))\n",
    "#             array *= img > 0\n",
    "#             array[array > 0] = y_batch[x]\n",
    "#             new_y_batch.append(array)\n",
    "#         new_y_batch = np.array(new_y_batch)\n",
    "#         val = modelA.test_on_batch(images, new_y_batch)\n",
    "\n",
    "#         if i >= steps_per_epoch:  # manually detect the end of the epoch validation set\n",
    "#             historyA[\"history\"][\"val_loss\"].append(val[0])\n",
    "#             historyA[\"history\"][\"val_mae\"].append(val[1])\n",
    "#             break\n",
    "        \n",
    "#     ## LR Scheduler\n",
    "#     curr_lr = modelA.optimizer.learning_rate\n",
    "#     if e >= SCHEDULE_EPOCH and curr_lr > min_lr:\n",
    "#         K.set_value(modelA.optimizer.learning_rate, curr_lr * factor)\n",
    "#         curr_lr = modelA.optimizer.learning_rate\n",
    "#         SCHEDULE_EPOCH += STEPS\n",
    "\n",
    "#     # ## Early Stopping\n",
    "#     # if e > PATIENCE:\n",
    "#     #     PAST = historyA[\"history\"][MONITOR][-PATIENCE-1:-1]\n",
    "#     #     LATEST = historyA[\"history\"][MONITOR][-1]\n",
    "#     #     for i,P in enumerate(PAST):\n",
    "#     #         if LATEST > P:\n",
    "#     #             PATIENCE -= 1  ## decrement patience if previous monitor is worse than the current value\n",
    "\n",
    "#     # if PATIENCE <= 0:\n",
    "#     #     break  ## if no more patience left, early stop training\n",
    "#     # PATIENCE = 10\n",
    "\n",
    "\n",
    "#     print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}   CURR_LR {:.2E}\".format(\n",
    "#         e + 1, loss[0], loss[1], val[0], val[1], curr_lr.numpy()))\n",
    "#     train_generator.on_epoch_end()  # shuffles the data at the end of each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyA[\"history\"][\"loss\"], color='g',alpha=.5)\n",
    "# plt.plot(historyA[\"history\"][\"val_loss\"], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyA[\"history\"][\"mae\"], color='g',alpha=.5)\n",
    "# plt.plot(historyA[\"history\"][\"val_mae\"], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.suptitle(\"Model A: Nested U-Net\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(modelA(sample[0]))\n",
    "# print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validation\n",
    "# val_sample = next(val_generator)\n",
    "# val_result = modelA.predict(val_sample[0])\n",
    "\n",
    "# for x, i in enumerate(val_result):\n",
    "#     RGB = cv2.cvtColor(val_sample[0][x], cv2.COLOR_HSV2RGB)\n",
    "#     MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "#     ARRAY = np.array(val_result[x]).reshape(image_size) * MASK\n",
    "#     OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "#     print(\"Validation - Average Output:\", np.average(OUTPUT), \"Ground Truth:\", np.average(val_sample[1][x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelC.save('design_models/designA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelA = tf.keras.models.load_model('design_models/designA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shutil import rmtree\n",
    "# # removing directory \n",
    "# rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Conv2DTranspose, Dropout, Flatten, Dense, Reshape\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "def segnet(hp):\n",
    "    hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "    \n",
    "    # Encoding path\n",
    "    inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    \n",
    "    # Encoder\n",
    "    x = Conv2D(hp_filters, (3, 3), padding='same', name='conv1')(inputs)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters, (3, 3), padding='same', name='conv2')(x)\n",
    "    x = BatchNormalization(name='bn2')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(name='pool1')(x)\n",
    "    \n",
    "    x = Conv2D(hp_filters * 2, (3, 3), padding='same', name='conv3')(x)\n",
    "    x = BatchNormalization(name='bn3')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 2, (3, 3), padding='same', name='conv4')(x)\n",
    "    x = BatchNormalization(name='bn4')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(name='pool2')(x)\n",
    "    \n",
    "    x = Conv2D(hp_filters * 4, (3, 3), padding='same', name='conv5')(x)\n",
    "    x = BatchNormalization(name='bn5')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 4, (3, 3), padding='same', name='conv6')(x)\n",
    "    x = BatchNormalization(name='bn6')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 4, (3, 3), padding='same', name='conv7')(x)\n",
    "    x = BatchNormalization(name='bn7')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(name='pool3')(x)\n",
    "    \n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv8')(x)\n",
    "    x = BatchNormalization(name='bn8')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv9')(x)\n",
    "    x = BatchNormalization(name='bn9')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv10')(x)\n",
    "    x = BatchNormalization(name='bn10')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(name='pool4')(x)\n",
    "    \n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv11')(x)\n",
    "    x = BatchNormalization(name='bn11')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv12')(x)\n",
    "    x = BatchNormalization(name='bn12')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters * 8, (3, 3), padding='same', name='conv13')(x)\n",
    "    x = BatchNormalization(name='bn13')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    qwe = MaxPooling2D(name='pool5')(x)\n",
    "    print(x.shape)\n",
    "    x = Flatten()(x);UNITS = hp.Choice('units',values = [256,512,1024,2048], default=256);DROPOUT = hp.Float('dropout',min_value=0.0, max_value=0.5, step=0.1, default=0.0)\n",
    "    \n",
    "    BATCHNORM = hp.Boolean('batchnorm',default=False)\n",
    "    # Dense layers between encoder and decoder\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    # Reshape back to spatial dimensions for the decoder\n",
    "    SIZE = IMAGE_SIZE[0] // 16  # Assuming input size is a multiple of 32\n",
    "    if BATCHNORM:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Dense(SIZE * SIZE * 512, activation='relu')(x)\n",
    "    print((SIZE, SIZE, 512))\n",
    "    x = Reshape((SIZE, SIZE, 512))(x)\n",
    "    \n",
    "    # Decoding path\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters * 8, (3, 3), padding='same', name='deconv1')(x)\n",
    "    x = BatchNormalization(name='bn14')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters * 8, (3, 3), padding='same', name='deconv2')(x)\n",
    "    x = BatchNormalization(name='bn15')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters * 8, (3, 3), padding='same', name='deconv3')(x)\n",
    "    x = BatchNormalization(name='bn16')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters * 4, (3, 3), padding='same', name='deconv4')(x)\n",
    "    x = BatchNormalization(name='bn17')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters * 4, (3, 3), padding='same', name='deconv5')(x)\n",
    "    x = BatchNormalization(name='bn18')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters * 4, (3, 3), padding='same', name='deconv6')(x)\n",
    "    x = BatchNormalization(name='bn19')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters * 2, (3, 3), padding='same', name='deconv7')(x)\n",
    "    x = BatchNormalization(name='bn20')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters * 2, (3, 3), padding='same', name='deconv8')(x)\n",
    "    x = BatchNormalization(name='bn21')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters, (3, 3), padding='same', name='deconv9')(x)\n",
    "    x = BatchNormalization(name='bn22')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters, (3, 3), padding='same', name='deconv10')(x)\n",
    "    x = BatchNormalization(name='bn23')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters, (3, 3), padding='same', name='deconv11')(x)\n",
    "    x = BatchNormalization(name='bn24')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(1, (3, 3), padding='same', name='deconv12')(x)\n",
    "    x = BatchNormalization(name='bn25')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(1, (3, 3), padding='same', name='deconv13')(x)\n",
    "    x = BatchNormalization(name='bn26')(x)\n",
    "    \n",
    "    # Output layer with sigmoid activation for binary segmentation\n",
    "    outputs = Conv2D(2, (1, 1), strides=2, activation='sigmoid')(x)\n",
    "    print(outputs.shape)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs);lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5]);model.compile(optimizer= tf.keras.optimizers.Adam(lr=lr), loss= [\"binary_crossentropy\"], metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Concatenate\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# def segnet(hp, input_shape=(64, 64, 3), num_classes=2):\n",
    "#     # Load ResNet50 with pre-trained weights and without top layers\n",
    "#     resnet_base = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "#     # Get encoder output layers\n",
    "#     encoder_outputs = [\n",
    "#         resnet_base.get_layer(\"conv1_relu\").output,\n",
    "#         resnet_base.get_layer(\"conv2_block3_out\").output,\n",
    "#         resnet_base.get_layer(\"conv3_block4_out\").output,\n",
    "#         resnet_base.get_layer(\"conv4_block6_out\").output,\n",
    "#         resnet_base.get_layer(\"conv5_block3_out\").output\n",
    "#     ]\n",
    "\n",
    "#     # Decoder part of SegNet\n",
    "#     x = encoder_outputs[-1]\n",
    "#     for i in range(4, 0, -1):\n",
    "#         x = UpSampling2D()(x)\n",
    "#         x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Concatenate()([x, encoder_outputs[i - 1]])\n",
    "\n",
    "#     x = UpSampling2D()(x)\n",
    "#     # Output layer\n",
    "#     outputs = Conv2D(2, (1, 1), activation='softmax')(x)\n",
    "#     print(outputs.shape)\n",
    "#     # Create model\n",
    "#     model = Model(inputs=resnet_base.input, outputs=outputs)\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5]);model.compile(optimizer= tf.keras.optimizers.Adam(lr=lr), loss= [\"binary_crossentropy\"], metrics=['acc'])\n",
    "#     return model\n",
    "\n",
    "# # Usage\n",
    "# # model = segnet_with_resnet50()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Conv2DTranspose, Dropout, Flatten, Dense, Reshape\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras import backend as K\n",
    "# import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "\n",
    "# def segnet(hp):\n",
    "#     hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "    \n",
    "#     # Load ResNet50 with pre-trained weights\n",
    "#     resnet_base = ResNet50(include_top=False, weights='imagenet', input_shape=(64,64,3))\n",
    "#     resnet_base.trainable = False\n",
    "\n",
    "#     # Encoder part using ResNet50\n",
    "#     inputs = Input(shape=(64,64,3))\n",
    "#     encoder_outputs = [\n",
    "#         resnet_base.get_layer(\"conv1_relu\").output,\n",
    "#         resnet_base.get_layer(\"conv2_block3_out\").output,\n",
    "#         resnet_base.get_layer(\"conv3_block4_out\").output,\n",
    "#         resnet_base.get_layer(\"conv4_block6_out\").output,\n",
    "#         resnet_base.get_layer(\"conv5_block3_out\").output\n",
    "#     ]\n",
    "\n",
    "#     # Flatten the bottleneck output\n",
    "#     x = Flatten()(encoder_outputs[-1])\n",
    "\n",
    "#     # Dense layers between encoder and decoder\n",
    "#     x = Dense(2048, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(2048, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(2048, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(2048, activation='relu')(x)\n",
    "    \n",
    "#     print(encoder_outputs[-1].shape)\n",
    "    \n",
    "#     # Reshape back to spatial dimensions for the decoder\n",
    "#     SIZE = 2\n",
    "    \n",
    "#     x = Dense(SIZE * SIZE * 2048, activation='relu')(x)\n",
    "#     x = Reshape((SIZE, SIZE, 2048))(x)\n",
    "    \n",
    "#     # Decoder part\n",
    "#     for i in range(4, 0, -1):\n",
    "#         x = UpSampling2D()(x)\n",
    "#         x = Conv2D(hp_filters * 8, (3, 3), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "#         x = Conv2D(hp_filters * 8, (3, 3), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "#         x = Conv2D(hp_filters * 8, (3, 3), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "#         x = Concatenate()([x, encoder_outputs[i - 1]])\n",
    "\n",
    "#     # Output layer\n",
    "#     outputs = Conv2D(2, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "#     # Create and compile model\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 03m 29s]\n",
      "val_loss: 0.3044518232345581\n",
      "\n",
      "Best val_loss So Far: 0.27843520045280457\n",
      "Total elapsed time: 00h 56m 21s\n",
      "{'filters': 16, 'units': 512, 'dropout': 0.4, 'batchnorm': False, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "tunerB = kt.BayesianOptimization(segnet,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 30,\n",
    "                     project_name='design_b',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8)\n",
    "\n",
    "\n",
    "tunerB.search(train_generator, epochs=50, validation_data=val_generator, callbacks=[stop_early, reduce_lr])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsB=tunerB.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "\n",
    "print(best_hpsB.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "# def segnet_resnet(input_shape, num_classes):\n",
    "#     # Define ResNet50 as the backbone\n",
    "#     backbone = ResNet50V2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "#     backbone.trainable = True\n",
    "    \n",
    "#     # # Encoder\n",
    "#     # x = backbone.get_layer(name=\"conv2_block2_out\").output\n",
    "#     # x = backbone.get_layer(name=\"conv3_block3_out\").output\n",
    "#     # x = backbone.get_layer(name=\"conv4_block4_out\").output\n",
    "#     # x = backbone.get_layer(name=\"conv5_block3_out\").output\n",
    "        \n",
    "#     # Encoder\n",
    "#     x = backbone.output\n",
    "#     denseOutputShape = x.shape\n",
    "    \n",
    "#     print(x.shape)\n",
    "    \n",
    "#     x = Flatten()(x);UNITS = 1024;DROPOUT = 0.4\n",
    "    \n",
    "#     BATCHNORM = False\n",
    "#     # Dense layers between encoder and decoder\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     # Reshape back to spatial dimensions for the decoder\n",
    "#     # SIZE = IMAGE_SIZE[0] // 32  # Assuming input size is a multiple of 32\n",
    "#     if BATCHNORM:\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Dense(denseOutputShape[1] * denseOutputShape[2] * denseOutputShape[3], activation='relu')(x)\n",
    "#     print((denseOutputShape[1], denseOutputShape[2], denseOutputShape[3]))\n",
    "#     x = Reshape((denseOutputShape[1], denseOutputShape[2], denseOutputShape[3]))(x)\n",
    "\n",
    "#     # Decoder\n",
    "#     decoder = layers.Conv2DTranspose(1024, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "#     decoder = layers.BatchNormalization()(decoder)\n",
    "#     decoder = layers.Activation('relu')(decoder)\n",
    "    \n",
    "#     decoder = layers.Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same')(decoder)\n",
    "#     decoder = layers.BatchNormalization()(decoder)\n",
    "#     decoder = layers.Activation('relu')(decoder)\n",
    "\n",
    "#     decoder = layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(decoder)\n",
    "#     decoder = layers.BatchNormalization()(decoder)\n",
    "#     decoder = layers.Activation('relu')(decoder)\n",
    "\n",
    "#     decoder = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(decoder)\n",
    "#     decoder = layers.BatchNormalization()(decoder)\n",
    "#     decoder = layers.Activation('relu')(decoder)\n",
    "\n",
    "#     decoder = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(decoder)\n",
    "#     decoder = layers.BatchNormalization()(decoder)\n",
    "#     decoder = layers.Activation('relu')(decoder)\n",
    "\n",
    "\n",
    "#     # Output layer\n",
    "#     output = layers.Conv2D(num_classes, (1, 1), activation='sigmoid', padding='same')(decoder)\n",
    "#     output = layers.Flatten()(output)\n",
    "#     output = layers.Dense(2, activation='sigmoid')(output)\n",
    "\n",
    "#     model = Model(inputs=backbone.input, outputs=output)\n",
    "    \n",
    "#     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "#     return model\n",
    "\n",
    "# # Example usage\n",
    "# input_shape = (64, 64, 3)  # Input shape of your images\n",
    "# num_classes = 2  # Number of segmentation classes\n",
    "# model = segnet_resnet(input_shape, num_classes)\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Conv2DTranspose, Dropout, Flatten, Dense, Reshape\n",
    "\n",
    "# def segnet():\n",
    "#     # Input layer\n",
    "#     inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    \n",
    "#     # Encoding path\n",
    "#     x = Conv2D(64, (3, 3), padding='same', name='conv1')(inputs)\n",
    "#     x = BatchNormalization(name='bn1')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(64, (3, 3), padding='same', name='conv2')(x)\n",
    "#     x = BatchNormalization(name='bn2')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool1')(x)\n",
    "    \n",
    "#     x = Conv2D(128, (3, 3), padding='same', name='conv3')(x)\n",
    "#     x = BatchNormalization(name='bn3')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(128, (3, 3), padding='same', name='conv4')(x)\n",
    "#     x = BatchNormalization(name='bn4')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool2')(x)\n",
    "    \n",
    "#     x = Conv2D(256, (3, 3), padding='same', name='conv5')(x)\n",
    "#     x = BatchNormalization(name='bn5')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(256, (3, 3), padding='same', name='conv6')(x)\n",
    "#     x = BatchNormalization(name='bn6')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(256, (3, 3), padding='same', name='conv7')(x)\n",
    "#     x = BatchNormalization(name='bn7')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool3')(x)\n",
    "    \n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv8')(x)\n",
    "#     x = BatchNormalization(name='bn8')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv9')(x)\n",
    "#     x = BatchNormalization(name='bn9')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv10')(x)\n",
    "#     x = BatchNormalization(name='bn10')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling2D(name='pool4')(x)\n",
    "    \n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv11')(x)\n",
    "#     x = BatchNormalization(name='bn11')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv12')(x)\n",
    "#     x = BatchNormalization(name='bn12')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2D(512, (3, 3), padding='same', name='conv13')(x)\n",
    "#     x = BatchNormalization(name='bn13')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     # Flattening and Dense layers\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "    \n",
    "#     # Reshape back to spatial dimensions for the decoder\n",
    "#     SIZE = IMAGE_SIZE[0] // 16  # Assuming input size is a multiple of 32\n",
    "#     x = Dense(SIZE * SIZE * 512, activation='relu')(x)\n",
    "#     x = Reshape((SIZE, SIZE, 512))(x)\n",
    "    \n",
    "#     # Decoding path\n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv1')(x)\n",
    "#     x = BatchNormalization(name='bn14')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv2')(x)\n",
    "#     x = BatchNormalization(name='bn15')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv3')(x)\n",
    "#     x = BatchNormalization(name='bn16')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv4')(x)\n",
    "#     x = BatchNormalization(name='bn17')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv5')(x)\n",
    "#     x = BatchNormalization(name='bn18')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv6')(x)\n",
    "#     x = BatchNormalization(name='bn19')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(128, (3, 3), padding='same', name='deconv7')(x)\n",
    "#     x = BatchNormalization(name='bn20')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Conv2DTranspose(128, (3, 3), padding='same', name='deconv8')(x)\n",
    "#     x = BatchNormalization(name='bn21')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     x = UpSampling2D()(x)\n",
    "#     x = Conv2DTranspose(64, (3, 3), padding='same', name='deconv9')(x)\n",
    "#     x = BatchNormalization(name='bn22')(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     # Output layer\n",
    "#     output = layers.Conv2D(num_classes, (1, 1), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "#     model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "#     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "#     return model\n",
    "\n",
    "# model = segnet()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " reshape_5 (Reshape)         (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_26 (Conv2D  (None, 1, 1, 512)        2359808   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 1, 1, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_26 (ReLU)             (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_27 (Conv2D  (None, 1, 1, 512)        2359808   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 1, 1, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_27 (ReLU)             (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_28 (Conv2D  (None, 1, 1, 512)        2359808   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 1, 1, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_28 (ReLU)             (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " up_sampling2d_19 (UpSamplin  (None, 2, 2, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_29 (Conv2D  (None, 2, 2, 512)        2359808   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_29 (ReLU)             (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_30 (Conv2D  (None, 2, 2, 512)        2359808   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_30 (ReLU)             (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_31 (Conv2D  (None, 2, 2, 256)        1179904   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 2, 2, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_31 (ReLU)             (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_20 (UpSamplin  (None, 4, 4, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_32 (Conv2D  (None, 4, 4, 256)        590080    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 4, 4, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_32 (ReLU)             (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_33 (Conv2D  (None, 4, 4, 256)        590080    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 4, 4, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_33 (ReLU)             (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_34 (Conv2D  (None, 4, 4, 128)        295040    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_34 (ReLU)             (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " up_sampling2d_21 (UpSamplin  (None, 8, 8, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_35 (Conv2D  (None, 8, 8, 128)        147584    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 8, 8, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_35 (ReLU)             (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_36 (Conv2D  (None, 8, 8, 64)         73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_42 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_36 (ReLU)             (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " up_sampling2d_22 (UpSamplin  (None, 16, 16, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_37 (Conv2D  (None, 16, 16, 64)       36928     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_37 (ReLU)             (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_38 (Conv2D  (None, 32, 32, 2)        1154      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 32, 32, 2)        8         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_38 (ReLU)             (None, 32, 32, 2)         0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 32, 32, 2)         6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,493,264\n",
      "Trainable params: 30,485,836\n",
      "Non-trainable params: 7,428\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "def SegNet(input_shape=(32, 32, 3), num_classes=21):\n",
    "    # Load the VGG16 model with batch normalization\n",
    "    vgg16_bn = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Adjust the input layer if needed\n",
    "    if input_shape[-1] != 3:\n",
    "        x = layers.Conv2D(64, (3, 3), padding='same', name='custom_input_conv')(inputs)\n",
    "    else:\n",
    "        x = inputs\n",
    "\n",
    "    # Using the pretrained VGG16_bn layers\n",
    "    x = vgg16_bn.get_layer('block1_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block1_conv2')(x)\n",
    "    stage1 = x\n",
    "    x = vgg16_bn.get_layer('block1_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block2_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block2_conv2')(x)\n",
    "    stage2 = x\n",
    "    x = vgg16_bn.get_layer('block2_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block3_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block3_conv2')(x)\n",
    "    x = vgg16_bn.get_layer('block3_conv3')(x)\n",
    "    stage3 = x\n",
    "    x = vgg16_bn.get_layer('block3_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block4_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block4_conv2')(x)\n",
    "    x = vgg16_bn.get_layer('block4_conv3')(x)\n",
    "    stage4 = x\n",
    "    x = vgg16_bn.get_layer('block4_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block5_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block5_conv2')(x)\n",
    "    x = vgg16_bn.get_layer('block5_conv3')(x)\n",
    "    stage5 = x\n",
    "    x = vgg16_bn.get_layer('block5_pool')(x)\n",
    "\n",
    "    # Flatten and add dense layer\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dense(1*1*512, activation='relu')(x)  # Adjust to match the new dimensions\n",
    "    x = layers.Reshape((1, 1, 512))(x)  # Adjust to match the new dimensions\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(num_classes, (3, 3), strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    outputs = layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = SegNet(input_shape=(32, 32, 3), num_classes=2)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 32, 32, 64)   1792        ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 32, 32, 64)   36928       ['block1_conv1[1][0]']           \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)     (None, 16, 16, 64)   0           ['block1_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 16, 16, 128)  73856       ['block1_pool[1][0]']            \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 16, 16, 128)  147584      ['block2_conv1[1][0]']           \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)     (None, 8, 8, 128)    0           ['block2_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 8, 8, 256)    295168      ['block2_pool[1][0]']            \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 8, 8, 256)    590080      ['block3_conv1[1][0]']           \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)          (None, 8, 8, 256)    590080      ['block3_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)     (None, 4, 4, 256)    0           ['block3_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)          (None, 4, 4, 512)    1180160     ['block3_pool[1][0]']            \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)          (None, 4, 4, 512)    2359808     ['block4_conv1[1][0]']           \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)          (None, 4, 4, 512)    2359808     ['block4_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)     (None, 2, 2, 512)    0           ['block4_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)          (None, 2, 2, 512)    2359808     ['block4_pool[1][0]']            \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)          (None, 2, 2, 512)    2359808     ['block5_conv1[1][0]']           \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)          (None, 2, 2, 512)    2359808     ['block5_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " block5_pool (MaxPooling2D)     (None, 1, 1, 512)    0           ['block5_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 512)          0           ['block5_pool[1][0]']            \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 1024)         525312      ['flatten_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 512)          524800      ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 1, 1, 512)    0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose_39 (Conv2DTra  (None, 1, 1, 512)   2359808     ['reshape_6[0][0]']              \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_transpose_39[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_39 (ReLU)                (None, 1, 1, 512)    0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_40 (Conv2DTra  (None, 1, 1, 512)   2359808     ['re_lu_39[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_transpose_40[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_40 (ReLU)                (None, 1, 1, 512)    0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_41 (Conv2DTra  (None, 1, 1, 512)   2359808     ['re_lu_40[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 1, 1, 512)   2048        ['conv2d_transpose_41[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_41 (ReLU)                (None, 1, 1, 512)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_23 (UpSampling2D  (None, 2, 2, 512)   0           ['re_lu_41[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 2, 2, 1024)   0           ['up_sampling2d_23[0][0]',       \n",
      "                                                                  'block5_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_42 (Conv2DTra  (None, 2, 2, 512)   4719104     ['concatenate_1[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 2, 2, 512)   2048        ['conv2d_transpose_42[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_42 (ReLU)                (None, 2, 2, 512)    0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_43 (Conv2DTra  (None, 2, 2, 512)   2359808     ['re_lu_42[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 2, 2, 512)   2048        ['conv2d_transpose_43[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_43 (ReLU)                (None, 2, 2, 512)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_44 (Conv2DTra  (None, 2, 2, 256)   1179904     ['re_lu_43[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_transpose_44[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_44 (ReLU)                (None, 2, 2, 256)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_24 (UpSampling2D  (None, 4, 4, 256)   0           ['re_lu_44[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 4, 4, 768)    0           ['up_sampling2d_24[0][0]',       \n",
      "                                                                  'block4_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_45 (Conv2DTra  (None, 4, 4, 256)   1769728     ['concatenate_2[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 4, 4, 256)   1024        ['conv2d_transpose_45[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_45 (ReLU)                (None, 4, 4, 256)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_46 (Conv2DTra  (None, 4, 4, 256)   590080      ['re_lu_45[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 4, 4, 256)   1024        ['conv2d_transpose_46[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_46 (ReLU)                (None, 4, 4, 256)    0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_47 (Conv2DTra  (None, 4, 4, 128)   295040      ['re_lu_46[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 4, 4, 128)   512         ['conv2d_transpose_47[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_47 (ReLU)                (None, 4, 4, 128)    0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_25 (UpSampling2D  (None, 8, 8, 128)   0           ['re_lu_47[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 8, 8, 384)    0           ['up_sampling2d_25[0][0]',       \n",
      "                                                                  'block3_conv3[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_48 (Conv2DTra  (None, 8, 8, 128)   442496      ['concatenate_3[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 128)   512         ['conv2d_transpose_48[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_48 (ReLU)                (None, 8, 8, 128)    0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_49 (Conv2DTra  (None, 8, 8, 64)    73792       ['re_lu_48[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 8, 8, 64)    256         ['conv2d_transpose_49[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_49 (ReLU)                (None, 8, 8, 64)     0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_26 (UpSampling2D  (None, 16, 16, 64)  0           ['re_lu_49[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 16, 16, 192)  0           ['up_sampling2d_26[0][0]',       \n",
      "                                                                  'block2_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_50 (Conv2DTra  (None, 16, 16, 64)  110656      ['concatenate_4[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 16, 16, 64)  256         ['conv2d_transpose_50[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_50 (ReLU)                (None, 16, 16, 64)   0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_27 (UpSampling2D  (None, 32, 32, 64)  0           ['re_lu_50[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 32, 128)  0           ['up_sampling2d_27[0][0]',       \n",
      "                                                                  'block1_conv2[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_51 (Conv2DTra  (None, 32, 32, 64)  73792       ['concatenate_5[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 32, 32, 64)  256         ['conv2d_transpose_51[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_51 (ReLU)                (None, 32, 32, 64)   0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_52 (Conv2DTra  (None, 32, 32, 2)   1154        ['re_lu_51[0][0]']               \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 32, 32, 2)   8           ['conv2d_transpose_52[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_52 (ReLU)                (None, 32, 32, 2)    0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 2)    6           ['re_lu_52[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34,474,896\n",
      "Trainable params: 34,467,340\n",
      "Non-trainable params: 7,556\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "def SegNet(input_shape=(32, 32, 3), num_classes=21):\n",
    "    # Load the VGG16 model with batch normalization\n",
    "    vgg16_bn = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Adjust the input layer if needed\n",
    "    if input_shape[-1] != 3:\n",
    "        x = layers.Conv2D(64, (3, 3), padding='same', name='custom_input_conv')(inputs)\n",
    "    else:\n",
    "        x = inputs\n",
    "\n",
    "    # Using the pretrained VGG16_bn layers\n",
    "    x = vgg16_bn.get_layer('block1_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block1_conv2')(x)\n",
    "    stage1 = x\n",
    "    x = vgg16_bn.get_layer('block1_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block2_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block2_conv2')(x)\n",
    "    stage2 = x\n",
    "    x = vgg16_bn.get_layer('block2_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block3_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block3_conv2')(x)\n",
    "    x = vgg16_bn.get_layer('block3_conv3')(x)\n",
    "    stage3 = x\n",
    "    x = vgg16_bn.get_layer('block3_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block4_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block4_conv2')(x)\n",
    "    x = vgg16_bn.get_layer('block4_conv3')(x)\n",
    "    stage4 = x\n",
    "    x = vgg16_bn.get_layer('block4_pool')(x)\n",
    "\n",
    "    x = vgg16_bn.get_layer('block5_conv1')(x)\n",
    "    x = vgg16_bn.get_layer('block5_conv2')(x)\n",
    "    x = vgg16_bn.get_layer('block5_conv3')(x)\n",
    "    stage5 = x\n",
    "    x = vgg16_bn.get_layer('block5_pool')(x)\n",
    "\n",
    "    # Flatten and add dense layer\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dense(1*1*512, activation='relu')(x)  # Adjust to match the new dimensions\n",
    "    x = layers.Reshape((1, 1, 512))(x)  # Adjust to match the new dimensions\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    # Incorporate stage5\n",
    "    x = layers.Concatenate()([x, stage5])\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(512, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    # Incorporate stage4\n",
    "    x = layers.Concatenate()([x, stage4])\n",
    "    x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(256, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    # Incorporate stage3\n",
    "    x = layers.Concatenate()([x, stage3])\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    # Incorporate stage2\n",
    "    x = layers.Concatenate()([x, stage2])\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "\n",
    "    # Incorporate stage1\n",
    "    x = layers.Concatenate()([x, stage1])\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2DTranspose(num_classes, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    outputs = layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = SegNet(input_shape=(32, 32, 3), num_classes=2)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 5s 63ms/step - loss: 0.6854 - acc: 0.8217 - val_loss: 5.2193 - val_acc: 0.8795\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.6321 - acc: 0.8141 - val_loss: 1.5978 - val_acc: 0.8795\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.6048 - acc: 0.8077 - val_loss: 0.5934 - val_acc: 0.8337\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.5784 - acc: 0.8196 - val_loss: 0.5962 - val_acc: 0.5925\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.5521 - acc: 0.8353 - val_loss: 0.5539 - val_acc: 0.7687\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.5275 - acc: 0.8318 - val_loss: 0.5255 - val_acc: 0.8838\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.5042 - acc: 0.8279 - val_loss: 0.4935 - val_acc: 0.8594\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.4829 - acc: 0.8325 - val_loss: 0.4789 - val_acc: 0.8836\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.4622 - acc: 0.8318 - val_loss: 0.4507 - val_acc: 0.8379\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.4436 - acc: 0.8395 - val_loss: 0.4276 - val_acc: 0.8369\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.4262 - acc: 0.8375 - val_loss: 0.4057 - val_acc: 0.8556\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.4096 - acc: 0.8434 - val_loss: 0.3924 - val_acc: 0.8732\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3958 - acc: 0.8454 - val_loss: 0.3826 - val_acc: 0.8309\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3827 - acc: 0.8480 - val_loss: 0.3739 - val_acc: 0.8681\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3732 - acc: 0.8536 - val_loss: 0.3519 - val_acc: 0.8346\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3606 - acc: 0.8531 - val_loss: 0.3488 - val_acc: 0.8595\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3528 - acc: 0.8589 - val_loss: 0.3322 - val_acc: 0.8474\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3436 - acc: 0.8587 - val_loss: 0.3272 - val_acc: 0.8604\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3370 - acc: 0.8635 - val_loss: 0.3245 - val_acc: 0.8590\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3303 - acc: 0.8628 - val_loss: 0.3166 - val_acc: 0.8612\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3248 - acc: 0.8677 - val_loss: 0.3076 - val_acc: 0.8664\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.3198 - acc: 0.8692 - val_loss: 0.3088 - val_acc: 0.8572\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.3160 - acc: 0.8660 - val_loss: 0.3032 - val_acc: 0.8630\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.3119 - acc: 0.8743 - val_loss: 0.3032 - val_acc: 0.8689\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.3086 - acc: 0.8728 - val_loss: 0.2959 - val_acc: 0.8551\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3055 - acc: 0.8747 - val_loss: 0.2890 - val_acc: 0.8601\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3025 - acc: 0.8734 - val_loss: 0.2884 - val_acc: 0.8670\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.3003 - acc: 0.8787 - val_loss: 0.2900 - val_acc: 0.8574\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2977 - acc: 0.8768 - val_loss: 0.2880 - val_acc: 0.8673\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2953 - acc: 0.8825 - val_loss: 0.2900 - val_acc: 0.8701\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2946 - acc: 0.8805 - val_loss: 0.2817 - val_acc: 0.8617\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2926 - acc: 0.8819 - val_loss: 0.2921 - val_acc: 0.8697\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2916 - acc: 0.8841 - val_loss: 0.2819 - val_acc: 0.8744\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2897 - acc: 0.8839 - val_loss: 0.2787 - val_acc: 0.8676\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2883 - acc: 0.8851 - val_loss: 0.2768 - val_acc: 0.8645\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2876 - acc: 0.8850 - val_loss: 0.2766 - val_acc: 0.8688\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 0.2862 - acc: 0.8858 - val_loss: 0.2765 - val_acc: 0.8695\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2863 - acc: 0.8879 - val_loss: 0.2949 - val_acc: 0.8699\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2860 - acc: 0.8868 - val_loss: 0.2922 - val_acc: 0.8748\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2838 - acc: 0.8920 - val_loss: 0.2727 - val_acc: 0.8708\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2833 - acc: 0.8906 - val_loss: 0.2725 - val_acc: 0.8728\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2830 - acc: 0.8914 - val_loss: 0.2747 - val_acc: 0.8703\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2818 - acc: 0.8924 - val_loss: 0.2712 - val_acc: 0.8736\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2821 - acc: 0.8956 - val_loss: 0.2723 - val_acc: 0.8689\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2811 - acc: 0.9010 - val_loss: 0.2808 - val_acc: 0.8808\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2809 - acc: 0.8987 - val_loss: 0.2711 - val_acc: 0.8795\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2799 - acc: 0.9028 - val_loss: 0.2713 - val_acc: 0.8806\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2797 - acc: 0.9027 - val_loss: 0.2731 - val_acc: 0.8821\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2791 - acc: 0.9036 - val_loss: 0.2762 - val_acc: 0.8758\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2789 - acc: 0.9066 - val_loss: 0.2728 - val_acc: 0.8812\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2783 - acc: 0.9033 - val_loss: 0.2715 - val_acc: 0.8807\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2780 - acc: 0.9072 - val_loss: 0.2672 - val_acc: 0.8810\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2779 - acc: 0.9067 - val_loss: 0.2717 - val_acc: 0.8827\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2773 - acc: 0.9071 - val_loss: 0.2708 - val_acc: 0.8831\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2765 - acc: 0.9083 - val_loss: 0.2688 - val_acc: 0.8817\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2764 - acc: 0.9081 - val_loss: 0.2662 - val_acc: 0.8835\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2777 - acc: 0.9086 - val_loss: 0.2792 - val_acc: 0.8815\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2764 - acc: 0.9071 - val_loss: 0.2692 - val_acc: 0.8830\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2764 - acc: 0.9079 - val_loss: 0.2673 - val_acc: 0.8835\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2756 - acc: 0.9077 - val_loss: 0.2679 - val_acc: 0.8828\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2751 - acc: 0.9084 - val_loss: 0.2690 - val_acc: 0.8839\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2736 - acc: 0.9085 - val_loss: 0.2671 - val_acc: 0.8831\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2751 - acc: 0.9077 - val_loss: 0.2681 - val_acc: 0.8830\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2740 - acc: 0.9086 - val_loss: 0.2695 - val_acc: 0.8822\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2730 - acc: 0.9085 - val_loss: 0.2684 - val_acc: 0.8839\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2741 - acc: 0.9083 - val_loss: 0.2665 - val_acc: 0.8833\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2725 - acc: 0.9089 - val_loss: 0.2705 - val_acc: 0.8838\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2728 - acc: 0.9088 - val_loss: 0.2731 - val_acc: 0.8839\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2725 - acc: 0.9083 - val_loss: 0.2693 - val_acc: 0.8840\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2721 - acc: 0.9085 - val_loss: 0.2649 - val_acc: 0.8824\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2727 - acc: 0.9090 - val_loss: 0.2661 - val_acc: 0.8838\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2723 - acc: 0.9086 - val_loss: 0.2686 - val_acc: 0.8838\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2708 - acc: 0.9085 - val_loss: 0.2641 - val_acc: 0.8841\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2713 - acc: 0.9087 - val_loss: 0.2639 - val_acc: 0.8832\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2719 - acc: 0.9086 - val_loss: 0.2642 - val_acc: 0.8841\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2708 - acc: 0.9090 - val_loss: 0.2667 - val_acc: 0.8836\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2711 - acc: 0.9077 - val_loss: 0.2648 - val_acc: 0.8840\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2709 - acc: 0.9086 - val_loss: 0.2663 - val_acc: 0.8840\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2710 - acc: 0.9087 - val_loss: 0.2690 - val_acc: 0.8841\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2714 - acc: 0.9090 - val_loss: 0.2670 - val_acc: 0.8837\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.2702 - acc: 0.9081 - val_loss: 0.2636 - val_acc: 0.8840\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2691 - acc: 0.9075 - val_loss: 0.2613 - val_acc: 0.8829\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2684 - acc: 0.9093 - val_loss: 0.2679 - val_acc: 0.8834\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2692 - acc: 0.9083 - val_loss: 0.2621 - val_acc: 0.8822\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2693 - acc: 0.9095 - val_loss: 0.2621 - val_acc: 0.8833\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2688 - acc: 0.9074 - val_loss: 0.2626 - val_acc: 0.8834\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2688 - acc: 0.9083 - val_loss: 0.2656 - val_acc: 0.8833\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2704 - acc: 0.9075 - val_loss: 0.2601 - val_acc: 0.8842\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2681 - acc: 0.9082 - val_loss: 0.2671 - val_acc: 0.8833\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2677 - acc: 0.9086 - val_loss: 0.2610 - val_acc: 0.8835\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2664 - acc: 0.9087 - val_loss: 0.2612 - val_acc: 0.8831\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2674 - acc: 0.9088 - val_loss: 0.2607 - val_acc: 0.8836\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2680 - acc: 0.9084 - val_loss: 0.2639 - val_acc: 0.8833\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2675 - acc: 0.9081 - val_loss: 0.2616 - val_acc: 0.8828\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2666 - acc: 0.9080 - val_loss: 0.2656 - val_acc: 0.8834\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2652 - acc: 0.9078 - val_loss: 0.2663 - val_acc: 0.8839\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2649 - acc: 0.9085 - val_loss: 0.2602 - val_acc: 0.8891\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2672 - acc: 0.9080 - val_loss: 0.2623 - val_acc: 0.8849\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2658 - acc: 0.9086 - val_loss: 0.2611 - val_acc: 0.8816\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.2648 - acc: 0.9090 - val_loss: 0.2598 - val_acc: 0.8835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x226e03f5e80>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, validation_data=val_generator, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hpsB.values['filters']=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2, 2, 128)\n",
      "(2, 2, 512)\n",
      "(None, 32, 32, 2)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 32, 32, 16)        448       \n",
      "                                                                 \n",
      " bn1 (BatchNormalization)    (None, 32, 32, 16)        64        \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 32, 32, 16)        2320      \n",
      "                                                                 \n",
      " bn2 (BatchNormalization)    (None, 32, 32, 16)        64        \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " pool1 (MaxPooling2D)        (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 16, 16, 32)        4640      \n",
      "                                                                 \n",
      " bn3 (BatchNormalization)    (None, 16, 16, 32)        128       \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv4 (Conv2D)              (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " bn4 (BatchNormalization)    (None, 16, 16, 32)        128       \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " pool2 (MaxPooling2D)        (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " conv5 (Conv2D)              (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " bn5 (BatchNormalization)    (None, 8, 8, 64)          256       \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv6 (Conv2D)              (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " bn6 (BatchNormalization)    (None, 8, 8, 64)          256       \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv7 (Conv2D)              (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " bn7 (BatchNormalization)    (None, 8, 8, 64)          256       \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " pool3 (MaxPooling2D)        (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " conv8 (Conv2D)              (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " bn8 (BatchNormalization)    (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv9 (Conv2D)              (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " bn9 (BatchNormalization)    (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv10 (Conv2D)             (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " bn10 (BatchNormalization)   (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_34 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " pool4 (MaxPooling2D)        (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv11 (Conv2D)             (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " bn11 (BatchNormalization)   (None, 2, 2, 128)         512       \n",
      "                                                                 \n",
      " activation_35 (Activation)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv12 (Conv2D)             (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " bn12 (BatchNormalization)   (None, 2, 2, 128)         512       \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv13 (Conv2D)             (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " bn13 (BatchNormalization)   (None, 2, 2, 128)         512       \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2048)              1050624   \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " up_sampling2d_5 (UpSampling  (None, 4, 4, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " deconv1 (Conv2DTranspose)   (None, 4, 4, 128)         589952    \n",
      "                                                                 \n",
      " bn14 (BatchNormalization)   (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " deconv2 (Conv2DTranspose)   (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " bn15 (BatchNormalization)   (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " deconv3 (Conv2DTranspose)   (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " bn16 (BatchNormalization)   (None, 4, 4, 128)         512       \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " up_sampling2d_6 (UpSampling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " deconv4 (Conv2DTranspose)   (None, 8, 8, 64)          73792     \n",
      "                                                                 \n",
      " bn17 (BatchNormalization)   (None, 8, 8, 64)          256       \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " deconv5 (Conv2DTranspose)   (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " bn18 (BatchNormalization)   (None, 8, 8, 64)          256       \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " deconv6 (Conv2DTranspose)   (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " bn19 (BatchNormalization)   (None, 8, 8, 64)          256       \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " up_sampling2d_7 (UpSampling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " deconv7 (Conv2DTranspose)   (None, 16, 16, 32)        18464     \n",
      "                                                                 \n",
      " bn20 (BatchNormalization)   (None, 16, 16, 32)        128       \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " deconv8 (Conv2DTranspose)   (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " bn21 (BatchNormalization)   (None, 16, 16, 32)        128       \n",
      "                                                                 \n",
      " activation_45 (Activation)  (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " deconv9 (Conv2DTranspose)   (None, 16, 16, 16)        4624      \n",
      "                                                                 \n",
      " bn22 (BatchNormalization)   (None, 16, 16, 16)        64        \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 32, 32, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " deconv10 (Conv2DTranspose)  (None, 32, 32, 16)        2320      \n",
      "                                                                 \n",
      " bn23 (BatchNormalization)   (None, 32, 32, 16)        64        \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " deconv11 (Conv2DTranspose)  (None, 32, 32, 16)        2320      \n",
      "                                                                 \n",
      " bn24 (BatchNormalization)   (None, 32, 32, 16)        64        \n",
      "                                                                 \n",
      " activation_48 (Activation)  (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " deconv12 (Conv2DTranspose)  (None, 32, 32, 1)         145       \n",
      "                                                                 \n",
      " bn25 (BatchNormalization)   (None, 32, 32, 1)         4         \n",
      "                                                                 \n",
      " activation_49 (Activation)  (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_9 (UpSampling  (None, 64, 64, 1)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " deconv13 (Conv2DTranspose)  (None, 64, 64, 1)         10        \n",
      "                                                                 \n",
      " bn26 (BatchNormalization)   (None, 64, 64, 1)         4         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 2)         4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,098,919\n",
      "Trainable params: 4,095,427\n",
      "Non-trainable params: 3,492\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the best hp.\n",
    "modelB = segnet(best_hpsB)\n",
    "modelB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 512\n",
    "# steps_per_epoch = 512 // batch_size + 1  # we usually consider 1 epoch to be\n",
    "#                                             # the point where the model has seen\n",
    "#                                             # all the training samples at least once\n",
    "# min_lr = 1e-8\n",
    "# factor = 0.1\n",
    "# SCHEDULE_EPOCH = 200\n",
    "# STEPS = 200\n",
    "\n",
    "\n",
    "# historyB = {\"history\":{\"loss\":[],\"mae\":[],\"val_loss\":[],\"val_mae\":[]}}\n",
    "# for e in range(epochs):\n",
    "#     for i, (images, y_batch) in enumerate(train_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#        loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#     #    val = modelB.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyB[\"history\"][\"loss\"].append(loss[0])\n",
    "#             historyB[\"history\"][\"mae\"].append(loss[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             break  \n",
    "#     for i, (images, y_batch) in enumerate(val_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#     #    loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#        val = modelB.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyB[\"history\"][\"val_loss\"].append(val[0])\n",
    "#             historyB[\"history\"][\"val_mae\"].append(val[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             curr_lr = modelB.optimizer.learning_rate\n",
    "#             if e>=SCHEDULE_EPOCH and curr_lr>min_lr: \n",
    "#                K.set_value(modelB.optimizer.learning_rate, curr_lr*factor)\n",
    "#                curr_lr = modelB.optimizer.learning_rate\n",
    "#                SCHEDULE_EPOCH+=STEPS\n",
    "#             # patience = 2\n",
    "#             # if e>patience and curr_lr>min_lr:\n",
    "#                # curr_loss = historyA[\"history\"][\"val_loss\"][-1]\n",
    "#                # for i in historyA[\"history\"][\"val_loss\"][-patience-1:-1]:\n",
    "#                #    if curr_loss >= i: patience-=1\n",
    "#                #    if patience < 1:\n",
    "#                      # K.set_value(modelA.optimizer.learning_rate, curr_lr*factor)\n",
    "#                      # curr_lr = modelA.optimizer.learning_rate\n",
    "#             break  \n",
    "#     print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}   CURR_LR {:.2E}\".format(e+1, loss[0], loss[1],val[0],val[1], curr_lr.numpy()))\n",
    "#     train_generator.on_epoch_end()  # this shuffles the data at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyB[\"history\"][\"loss\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyB[\"history\"][\"val_loss\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyB[\"history\"][\"mae\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyB[\"history\"][\"val_mae\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.suptitle(\"Model B: SegNet\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(historyB[\"history\"][\"loss\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1,\n",
    "#                               patience=10, min_lr=1e-7)\n",
    "# # stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=10)\n",
    "# history = modelB.fit(train_generator, epochs=2,# validation_data=val_generator,\n",
    "#                      callbacks=[red_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n",
    "# result = modelB(sample[0])\n",
    "# for x,i in enumerate(result):\n",
    "#     RGB  = cv2.cvtColor(sample[0][x],cv2.COLOR_HSV2RGB)\n",
    "#     MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "#     ARRAY = np.array(result[5]).reshape(image_size) * MASK\n",
    "#     OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "#     print(np.average(OUTPUT), np.average(sample[1][x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.imshow(result[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(sample[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# RGB  = cv2.cvtColor(sample[0][5],cv2.COLOR_HSV2RGB)\n",
    "# MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "# ARRAY = np.array(result[5]).reshape(image_size) * MASK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVE = np.average(ARRAY)\n",
    "# TRUTH = sample[1][5]\n",
    "# print(AVE)\n",
    "# print(TRUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(ARRAY)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelA.save('design_models/BEST/designA_V3.h5')\n",
    "# modelB.save('design_models/BEST/designB_V3.h5')\n",
    "# modelC.save('design_models/BEST/designC_V3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shutil import rmtree\n",
    "# # removing directory \n",
    "# rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Conv2DTranspose, Dropout, Flatten, Dense, Reshape, AveragePooling2D, Concatenate\n",
    "from tensorflow.keras import layers, initializers\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n",
    "    x = Conv2D(\n",
    "        num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "        kernel_initializer=initializers.HeNormal(),\n",
    "    )(block_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation('relu')(x)\n",
    "\n",
    "def DilatedSpatialPyramidPooling(dspp_input, num_filters):\n",
    "    dims = dspp_input.shape\n",
    "    x = AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "    out_pool = UpSampling2D(\n",
    "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "\n",
    "    out_1 = convolution_block(dspp_input, num_filters=num_filters, kernel_size=1, dilation_rate=1)\n",
    "    out_6 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=6)\n",
    "    out_12 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=12)\n",
    "    out_18 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=18)\n",
    "\n",
    "    x = Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "    output = convolution_block(x, kernel_size=1)\n",
    "    return output\n",
    "\n",
    "def DeeplabV3Plus(hp, image_size=(64, 64)):\n",
    "    model_input = Input(shape=image_size+(3,))\n",
    "    resnet50 = tf.keras.applications.ResNet50(\n",
    "        weights='imagenet', include_top=False, input_tensor=model_input\n",
    "    )\n",
    "    for layer in resnet50.layers:\n",
    "        layer.trainable=True\n",
    "    \n",
    "    hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "    x = resnet50.get_layer(\"conv1_relu\").output\n",
    "    x = DilatedSpatialPyramidPooling(x, hp_filters*2)\n",
    "    print(x.shape)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(hp_filters*hp_filters, activation='relu')(x)\n",
    "\n",
    "    UNITS = hp_filters*hp_filters\n",
    "    hp.Boolean(\"dropouts\", default=False)\n",
    "    hp.Boolean(\"batch_normalization\", default=False)\n",
    "\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    if hp.Boolean(\"dropouts\"):\n",
    "        x = Dropout(0.5)(x)\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    if hp.Boolean(\"dropouts\"):\n",
    "        x = Dropout(0.5)(x)\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    if hp.Boolean(\"4th_dense\", default=False):\n",
    "        x = Dense(UNITS, activation='relu')(x)\n",
    "        if hp.Boolean(\"dropouts\"):\n",
    "            x = Dropout(0.5)(x)\n",
    "        if hp.Boolean(\"batch_normalization\"):\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "    SIZE = image_size[0]\n",
    "    temp = 0\n",
    "    if hp_filters == 16:\n",
    "        temp = hp_filters\n",
    "    if hp_filters == 32:\n",
    "        temp = hp_filters * 2\n",
    "    if hp_filters == 64:\n",
    "        temp = hp_filters * 4\n",
    "    x = Reshape((SIZE // 16, SIZE // 16, temp))(x)\n",
    "\n",
    "    x = UpSampling2D(size=(SIZE // 4 // x.shape[1], SIZE // 4 // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    input_b = convolution_block(input_b, num_filters=hp_filters, kernel_size=1)\n",
    "\n",
    "    x = Concatenate(axis=-1)([x, input_b])\n",
    "    x = convolution_block(x, num_filters=hp_filters*4)\n",
    "    x = convolution_block(x, num_filters=hp_filters*4)\n",
    "    x = UpSampling2D(size=(SIZE // x.shape[1], SIZE // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "    \n",
    "    outputs = Conv2D(2, 1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=model_input, outputs=outputs)\n",
    "    lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=[\"binary_crossentropy\"], metrics=['acc'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeeplabV3Plus(hp, image_size=(32, 32)):\n",
    "    model_input = Input(shape=image_size + (3,))\n",
    "    resnet50 = tf.keras.applications.ResNet50(\n",
    "        weights='imagenet', include_top=False, input_tensor=model_input\n",
    "    )\n",
    "    for layer in resnet50.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "    x = resnet50.get_layer(\"conv1_relu\").output\n",
    "    x = DilatedSpatialPyramidPooling(x, hp_filters * 2)\n",
    "    print(x.shape)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    UNITS = hp.Choice('units', values=[256, 512, 1024, 2048], default=256)\n",
    "    DROPOUT = hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1, default=0.0)\n",
    "    BATCHNORM = hp.Boolean('batchnorm', default=False)\n",
    "\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Dense(UNITS, activation='relu')(x)\n",
    "\n",
    "    SIZE = image_size[0]\n",
    "    temp = 0\n",
    "    if hp_filters == 16:\n",
    "        temp = hp_filters\n",
    "    if hp_filters == 32:\n",
    "        temp = hp_filters * 2\n",
    "    if hp_filters == 64:\n",
    "        temp = hp_filters * 4\n",
    "    x = Reshape((SIZE // 16, SIZE // 16, temp))(x)\n",
    "\n",
    "    x = UpSampling2D(size=(SIZE // 4 // x.shape[1], SIZE // 4 // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    input_b = convolution_block(input_b, num_filters=hp_filters, kernel_size=1)\n",
    "\n",
    "    x = Concatenate(axis=-1)([x, input_b])\n",
    "    x = convolution_block(x, num_filters=hp_filters * 4)\n",
    "    x = convolution_block(x, num_filters=hp_filters * 4)\n",
    "    x = UpSampling2D(size=(SIZE // x.shape[1], SIZE // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "    \n",
    "    outputs = Conv2D(2, 1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=model_input, outputs=outputs)\n",
    "    lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=[\"binary_crossentropy\"], metrics=['acc'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DeeplabV3Plus(hp,image_size=(64, 64)):\n",
    "#     model_input = Input(shape=image_size + (3,))\n",
    "#     resnet50 = tf.keras.applications.ResNet50(\n",
    "#         weights='imagenet', include_top=False, input_tensor=model_input\n",
    "#     )\n",
    "#     for layer in resnet50.layers:\n",
    "#         layer.trainable = True\n",
    "#     hp_filters = hp.Choice('filters', values=[16, 32, 64])\n",
    "    \n",
    "#     x = resnet50.get_layer(\"conv1_relu\").output\n",
    "#     x = resnet50.get_layer(\"conv2_block3_out\").output\n",
    "#     x = DilatedSpatialPyramidPooling(x, hp_filters * 2)\n",
    "#     print(x.shape)\n",
    "#     x = Flatten()(x)\n",
    "    \n",
    "#     UNITS = hp.Choice('units', values=[256, 512, 1024, 2048], default=256)\n",
    "#     DROPOUT = hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1, default=0.0)\n",
    "#     BATCHNORM = hp.Boolean('batchnorm', default=False)\n",
    "\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "#     x = Dense(UNITS, activation='relu')(x)\n",
    "#     x = Dropout(DROPOUT)(x)\n",
    "\n",
    "#     if BATCHNORM:\n",
    "#         x = BatchNormalization()(x)\n",
    "\n",
    "#     SIZE = image_size[0]//4\n",
    "#     temp = 0\n",
    "#     if hp_filters == 16:\n",
    "#         temp = hp_filters\n",
    "#     if hp_filters == 32:\n",
    "#         temp = hp_filters * 2\n",
    "#     if hp_filters == 64:\n",
    "#         temp = hp_filters * 4\n",
    "#     x = Dense((SIZE // 8)* (SIZE // 8)* temp, activation='relu')(x)\n",
    "#     x = Reshape((SIZE // 8, SIZE // 8, temp))(x)\n",
    "\n",
    "#     x = UpSampling2D(size=(SIZE // 4 // x.shape[1], SIZE // 4 // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "#     input_b = resnet50.get_layer(\"conv4_block4_out\").output\n",
    "#     input_b = convolution_block(input_b, num_filters=hp_filters, kernel_size=1)\n",
    "\n",
    "#     x = Concatenate(axis=-1)([x, input_b])\n",
    "#     x = convolution_block(x, num_filters=hp_filters * 4)\n",
    "#     x = convolution_block(x, num_filters=hp_filters * 4)\n",
    "#     x = UpSampling2D(size=( 16, 16), interpolation=\"bilinear\")(x)\n",
    "    \n",
    "#     outputs = Conv2D(2, 1, activation='sigmoid')(x)\n",
    "    \n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model = Model(inputs=model_input, outputs=outputs)\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=[\"binary_crossentropy\"], metrics=['acc'])\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Example usage and model summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, applications\n",
    "\n",
    "# def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n",
    "#     x = layers.Conv2D(\n",
    "#         num_filters,\n",
    "#         kernel_size=kernel_size,\n",
    "#         dilation_rate=dilation_rate,\n",
    "#         padding=padding,\n",
    "#         use_bias=use_bias,\n",
    "#         kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "#     )(block_input)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     return tf.nn.relu(x)\n",
    "\n",
    "# def DilatedSpatialPyramidPooling(dspp_input):\n",
    "#     dims = dspp_input.shape\n",
    "#     x = layers.AveragePooling2D(pool_size=(dims[1], dims[2]))(dspp_input)\n",
    "#     x = convolution_block(x, num_filters=256, kernel_size=1, use_bias=True)\n",
    "#     out_pool = layers.UpSampling2D(\n",
    "#         size=(dims[1] // x.shape[1], dims[2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "#     )(x)\n",
    "\n",
    "#     out_1 = convolution_block(dspp_input, num_filters=256, kernel_size=1, dilation_rate=1)\n",
    "#     out_6 = convolution_block(dspp_input, num_filters=256, kernel_size=3, dilation_rate=6)\n",
    "#     out_12 = convolution_block(dspp_input, num_filters=256, kernel_size=3, dilation_rate=12)\n",
    "#     out_18 = convolution_block(dspp_input, num_filters=256, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "#     x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "#     output = convolution_block(x, num_filters=256, kernel_size=1)\n",
    "#     return output\n",
    "\n",
    "# def DeeplabV3Plus(hp, image_size=(64, 64, 3), num_classes=2):\n",
    "#     base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=image_size)\n",
    "#     base_model_output = base_model.get_layer(\"conv4_block6_2_relu\").output\n",
    "\n",
    "#     x = DilatedSpatialPyramidPooling(base_model_output)\n",
    "\n",
    "#     input_a = layers.UpSampling2D(size=(image_size[0] // 4 // x.shape[1], image_size[1] // 4 // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "#     input_b = base_model.get_layer(\"conv2_block3_2_relu\").output\n",
    "#     input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "#     x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "#     x = convolution_block(x, num_filters=256)\n",
    "#     x = convolution_block(x, num_filters=256)\n",
    "#     x = layers.UpSampling2D(size=(image_size[0] // x.shape[1], image_size[1] // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "#     x = layers.Conv2D(2, (1, 1), padding=\"same\")(x)\n",
    "#     outputs = tf.keras.layers.Activation('sigmoid')(x)\n",
    "\n",
    "#     model = tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "#     lr = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=[\"binary_crossentropy\"], metrics=['acc'])\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16, 16, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tunerC = kt.BayesianOptimization(DeeplabV3Plus,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 30,\n",
    "                     project_name='design_c',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8)\n",
    "\n",
    "tunerC.search(train_generator, epochs=50, validation_data=val_generator, callbacks=[stop_early, reduce_lr])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsC=tunerC.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(best_hpsC.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " model_18 (Functional)          [(None, 16, 16, 64)  23587712    ['input_27[0][0]']               \n",
      "                                , (None, 8, 8, 256)                                               \n",
      "                                , (None, 4, 4, 512)                                               \n",
      "                                , (None, 2, 2, 1024                                               \n",
      "                                ),                                                                \n",
      "                                 (None, 1, 1, 2048)                                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " flatten_13 (Flatten)           (None, 2048)         0           ['model_18[0][4]']               \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 1024)         2098176     ['flatten_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 512)          524800      ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_19 (Reshape)           (None, 1, 1, 512)    0           ['dense_35[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling2d_6 (Gl  (None, 512)         0           ['reshape_19[0][0]']             \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_20 (Reshape)           (None, 1, 1, 512)    0           ['global_average_pooling2d_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 1, 1, 256)    131328      ['reshape_20[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 1, 1, 256)    131328      ['reshape_19[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 1, 1, 256)    1179904     ['reshape_19[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 1, 1, 256)    1179904     ['reshape_19[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 1, 1, 256)    1179904     ['reshape_19[0][0]']             \n",
      "                                                                                                  \n",
      " up_sampling2d_44 (UpSampling2D  (None, 1, 1, 256)   0           ['conv2d_54[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 1, 1, 1280)   0           ['conv2d_55[0][0]',              \n",
      "                                                                  'conv2d_56[0][0]',              \n",
      "                                                                  'conv2d_57[0][0]',              \n",
      "                                                                  'conv2d_58[0][0]',              \n",
      "                                                                  'up_sampling2d_44[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 1, 1, 256)    327936      ['concatenate_20[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_45 (UpSampling2D  (None, 8, 8, 256)   0           ['conv2d_59[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 8, 8, 512)    0           ['up_sampling2d_45[0][0]',       \n",
      "                                                                  'model_18[0][1]']               \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 8, 8, 256)    1179904     ['concatenate_21[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 8, 8, 256)    590080      ['conv2d_60[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_46 (UpSampling2D  (None, 16, 16, 256)  0          ['conv2d_61[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 16, 16, 320)  0           ['up_sampling2d_46[0][0]',       \n",
      "                                                                  'model_18[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 16, 16, 256)  737536      ['concatenate_22[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 16, 16, 256)  590080      ['conv2d_62[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_47 (UpSampling2D  (None, 32, 32, 256)  0          ['conv2d_63[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 32, 32, 2)    514         ['up_sampling2d_47[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33,439,106\n",
      "Trainable params: 33,385,986\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "def DeeplabV3Plus(input_shape=(32, 32, 3), num_classes=21):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder: ResNet50\n",
    "    resnet50 = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    \n",
    "    # Extract layers\n",
    "    layer_names = [\n",
    "        'conv1_relu', 'conv2_block3_out', 'conv3_block4_out',\n",
    "        'conv4_block6_out', 'conv5_block3_out'\n",
    "    ]\n",
    "    layers_output = [resnet50.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    # Define the encoder model\n",
    "    encoder = Model(inputs=resnet50.input, outputs=layers_output)\n",
    "\n",
    "    # Feature extraction\n",
    "    stage1, stage2, stage3, stage4, stage5 = encoder(inputs)\n",
    "    \n",
    "    # Concatenate feature maps from stage5\n",
    "    stage5_concat = layers.Concatenate()(stage5) if isinstance(stage5, list) else stage5\n",
    "\n",
    "    # Flatten and add dense layer\n",
    "    x = layers.Flatten()(stage5_concat)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dense(1 * 1 * 512, activation='relu')(x)\n",
    "    x = layers.Reshape((1, 1, 512))(x)\n",
    "    \n",
    "    # ASPP\n",
    "    def aspp_block(x, out_channels, kernel_size, dilation):\n",
    "        return layers.Conv2D(out_channels, kernel_size, padding='same', dilation_rate=dilation, activation='relu')(x)\n",
    "\n",
    "    pool = layers.GlobalAveragePooling2D()(x)\n",
    "    pool = layers.Reshape((1, 1, 512))(pool)\n",
    "    pool = layers.Conv2D(256, (1, 1), activation='relu')(pool)\n",
    "\n",
    "    # Use K.int_shape to get the static shape as a tuple of integers\n",
    "    shape_before = tf.keras.backend.int_shape(x)\n",
    "    pool = layers.UpSampling2D(size=(shape_before[1], shape_before[2]))(pool)\n",
    "\n",
    "    b1 = aspp_block(x, 256, 1, 1)\n",
    "    b2 = aspp_block(x, 256, 3, 6)\n",
    "    b3 = aspp_block(x, 256, 3, 12)\n",
    "    b4 = aspp_block(x, 256, 3, 18)\n",
    "    \n",
    "    x = layers.Concatenate()([b1, b2, b3, b4, pool])\n",
    "    x = layers.Conv2D(256, (1, 1), activation='relu')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.UpSampling2D(size=(8, 8))(x)\n",
    "    x = layers.Concatenate()([x, stage2])\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "\n",
    "    x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "    x = layers.Concatenate()([x, stage1])\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "\n",
    "    x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "    x = layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, x)\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = DeeplabV3Plus(input_shape=(32, 32, 3), num_classes=2)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 9s 93ms/step - loss: 0.6362 - accuracy: 0.6551 - val_loss: 0.3303 - val_accuracy: 0.8682\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.3052 - accuracy: 0.7385 - val_loss: 0.3229 - val_accuracy: 0.6887\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2991 - accuracy: 0.7481 - val_loss: 0.3275 - val_accuracy: 0.8541\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2948 - accuracy: 0.7359 - val_loss: 0.3410 - val_accuracy: 0.5467\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2962 - accuracy: 0.7559 - val_loss: 0.3169 - val_accuracy: 0.7725\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2917 - accuracy: 0.8139 - val_loss: 0.3380 - val_accuracy: 0.8797\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2917 - accuracy: 0.7064 - val_loss: 0.3698 - val_accuracy: 0.8799\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2902 - accuracy: 0.7691 - val_loss: 0.3884 - val_accuracy: 0.5719\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2889 - accuracy: 0.6916 - val_loss: 0.4118 - val_accuracy: 0.8520\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2886 - accuracy: 0.7201 - val_loss: 0.3843 - val_accuracy: 0.7738\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2908 - accuracy: 0.7695 - val_loss: 0.3644 - val_accuracy: 0.8825\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2905 - accuracy: 0.7364 - val_loss: 0.4103 - val_accuracy: 0.7284\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2881 - accuracy: 0.7752 - val_loss: 0.3864 - val_accuracy: 0.8715\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2866 - accuracy: 0.7767 - val_loss: 0.3942 - val_accuracy: 0.8093\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2894 - accuracy: 0.8160 - val_loss: 0.3197 - val_accuracy: 0.7877\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2863 - accuracy: 0.8108 - val_loss: 0.3428 - val_accuracy: 0.8550\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2865 - accuracy: 0.7882 - val_loss: 0.3725 - val_accuracy: 0.7855\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2852 - accuracy: 0.7775 - val_loss: 0.3630 - val_accuracy: 0.8079\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2839 - accuracy: 0.7557 - val_loss: 0.3475 - val_accuracy: 0.7253\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2860 - accuracy: 0.7271 - val_loss: 0.3423 - val_accuracy: 0.7408\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2852 - accuracy: 0.7368 - val_loss: 0.3457 - val_accuracy: 0.6764\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2838 - accuracy: 0.6743 - val_loss: 0.3143 - val_accuracy: 0.5170\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2837 - accuracy: 0.7365 - val_loss: 0.2857 - val_accuracy: 0.7044\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2827 - accuracy: 0.7935 - val_loss: 0.2913 - val_accuracy: 0.6739\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2847 - accuracy: 0.6692 - val_loss: 0.2930 - val_accuracy: 0.7237\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2845 - accuracy: 0.6933 - val_loss: 0.2895 - val_accuracy: 0.8319\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2836 - accuracy: 0.7078 - val_loss: 0.2750 - val_accuracy: 0.6432\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2836 - accuracy: 0.6285 - val_loss: 0.2743 - val_accuracy: 0.5477\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2839 - accuracy: 0.7022 - val_loss: 0.2736 - val_accuracy: 0.7340\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2812 - accuracy: 0.7067 - val_loss: 0.2693 - val_accuracy: 0.5202\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2806 - accuracy: 0.7427 - val_loss: 0.2744 - val_accuracy: 0.6032\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2818 - accuracy: 0.6891 - val_loss: 0.2700 - val_accuracy: 0.6573\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2818 - accuracy: 0.7073 - val_loss: 0.2685 - val_accuracy: 0.7321\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2805 - accuracy: 0.7229 - val_loss: 0.2683 - val_accuracy: 0.6982\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2816 - accuracy: 0.7220 - val_loss: 0.2666 - val_accuracy: 0.7526\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2803 - accuracy: 0.7835 - val_loss: 0.2685 - val_accuracy: 0.7331\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2828 - accuracy: 0.6840 - val_loss: 0.2727 - val_accuracy: 0.4196\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2819 - accuracy: 0.7411 - val_loss: 0.2705 - val_accuracy: 0.7613\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2806 - accuracy: 0.7198 - val_loss: 0.2686 - val_accuracy: 0.6804\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2808 - accuracy: 0.7173 - val_loss: 0.2706 - val_accuracy: 0.7878\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2806 - accuracy: 0.7307 - val_loss: 0.2704 - val_accuracy: 0.5949\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2795 - accuracy: 0.6802 - val_loss: 0.2738 - val_accuracy: 0.8589\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2796 - accuracy: 0.7578 - val_loss: 0.2761 - val_accuracy: 0.7565\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2813 - accuracy: 0.7331 - val_loss: 0.2668 - val_accuracy: 0.5920\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2809 - accuracy: 0.7232 - val_loss: 0.2738 - val_accuracy: 0.6181\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2805 - accuracy: 0.7605 - val_loss: 0.2696 - val_accuracy: 0.6131\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2822 - accuracy: 0.7539 - val_loss: 0.2696 - val_accuracy: 0.6556\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2801 - accuracy: 0.6862 - val_loss: 0.2713 - val_accuracy: 0.6283\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2805 - accuracy: 0.6969 - val_loss: 0.2673 - val_accuracy: 0.8360\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2789 - accuracy: 0.7599 - val_loss: 0.2734 - val_accuracy: 0.7131\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2777 - accuracy: 0.7674 - val_loss: 0.2683 - val_accuracy: 0.5378\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2779 - accuracy: 0.7306 - val_loss: 0.2718 - val_accuracy: 0.8472\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2776 - accuracy: 0.7786 - val_loss: 0.2724 - val_accuracy: 0.6717\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2766 - accuracy: 0.7495 - val_loss: 0.2666 - val_accuracy: 0.8257\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2765 - accuracy: 0.6635 - val_loss: 0.2737 - val_accuracy: 0.7717\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2777 - accuracy: 0.7790 - val_loss: 0.2677 - val_accuracy: 0.8051\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2775 - accuracy: 0.7902 - val_loss: 0.2696 - val_accuracy: 0.7539\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2773 - accuracy: 0.7277 - val_loss: 0.2751 - val_accuracy: 0.7423\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2775 - accuracy: 0.7787 - val_loss: 0.2822 - val_accuracy: 0.5384\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2776 - accuracy: 0.7259 - val_loss: 0.2662 - val_accuracy: 0.5787\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2757 - accuracy: 0.7642 - val_loss: 0.2913 - val_accuracy: 0.7562\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2767 - accuracy: 0.7412 - val_loss: 0.2699 - val_accuracy: 0.6119\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2764 - accuracy: 0.7359 - val_loss: 0.2683 - val_accuracy: 0.6767\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2767 - accuracy: 0.7358 - val_loss: 0.2729 - val_accuracy: 0.6127\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2758 - accuracy: 0.7349 - val_loss: 0.2795 - val_accuracy: 0.5649\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 0.2753 - accuracy: 0.6647 - val_loss: 0.2750 - val_accuracy: 0.6185\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 0.2765 - accuracy: 0.7455 - val_loss: 0.2660 - val_accuracy: 0.7140\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2781 - accuracy: 0.7179 - val_loss: 0.2741 - val_accuracy: 0.6870\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2778 - accuracy: 0.7685 - val_loss: 0.2783 - val_accuracy: 0.8244\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 0.2774 - accuracy: 0.7559 - val_loss: 0.2691 - val_accuracy: 0.8027\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2764 - accuracy: 0.7451 - val_loss: 0.2681 - val_accuracy: 0.8182\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2764 - accuracy: 0.7825 - val_loss: 0.2740 - val_accuracy: 0.7709\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2757 - accuracy: 0.7560 - val_loss: 0.2731 - val_accuracy: 0.7114\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2748 - accuracy: 0.7255 - val_loss: 0.2896 - val_accuracy: 0.5396\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2747 - accuracy: 0.6879 - val_loss: 0.2729 - val_accuracy: 0.5919\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2746 - accuracy: 0.7144 - val_loss: 0.2697 - val_accuracy: 0.7510\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2753 - accuracy: 0.7239 - val_loss: 0.2844 - val_accuracy: 0.6690\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2745 - accuracy: 0.6681 - val_loss: 0.2685 - val_accuracy: 0.4365\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 0.2739 - accuracy: 0.6813 - val_loss: 0.2650 - val_accuracy: 0.7443\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2754 - accuracy: 0.6787 - val_loss: 0.2684 - val_accuracy: 0.7599\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2745 - accuracy: 0.7078 - val_loss: 0.2694 - val_accuracy: 0.7498\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2739 - accuracy: 0.7451 - val_loss: 0.2655 - val_accuracy: 0.6585\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2748 - accuracy: 0.7298 - val_loss: 0.2663 - val_accuracy: 0.7892\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 0.2745 - accuracy: 0.6896 - val_loss: 0.2633 - val_accuracy: 0.6850\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2740 - accuracy: 0.6815 - val_loss: 0.2814 - val_accuracy: 0.7927\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2784 - accuracy: 0.7160 - val_loss: 0.2690 - val_accuracy: 0.6114\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2770 - accuracy: 0.6765 - val_loss: 0.2749 - val_accuracy: 0.7420\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2737 - accuracy: 0.6336 - val_loss: 0.2637 - val_accuracy: 0.6333\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2743 - accuracy: 0.6373 - val_loss: 0.2699 - val_accuracy: 0.5244\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2730 - accuracy: 0.6829 - val_loss: 0.2719 - val_accuracy: 0.7112\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2748 - accuracy: 0.6849 - val_loss: 0.2737 - val_accuracy: 0.6133\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2725 - accuracy: 0.6667 - val_loss: 0.2658 - val_accuracy: 0.5682\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2719 - accuracy: 0.6603 - val_loss: 0.2640 - val_accuracy: 0.5441\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2738 - accuracy: 0.6222 - val_loss: 0.2760 - val_accuracy: 0.5338\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2727 - accuracy: 0.6745 - val_loss: 0.2659 - val_accuracy: 0.7869\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2741 - accuracy: 0.6871 - val_loss: 0.2771 - val_accuracy: 0.7610\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.2727 - accuracy: 0.6473 - val_loss: 0.2684 - val_accuracy: 0.5964\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2734 - accuracy: 0.6992 - val_loss: 0.2730 - val_accuracy: 0.5693\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.2720 - accuracy: 0.6622 - val_loss: 0.2631 - val_accuracy: 0.6560\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.2723 - accuracy: 0.6850 - val_loss: 0.2662 - val_accuracy: 0.5733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22724a9b4c0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, validation_data=val_generator, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hpsC.values['filters']=64\n",
    "# best_hpsC.values['learning_rate']=1e-6\n",
    "# Build the model with the best hp.\n",
    "modelC = DeeplabV3Plus(best_hpsC)\n",
    "modelC.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 512\n",
    "# steps_per_epoch = 512 // batch_size + 1  # we usually consider 1 epoch to be\n",
    "#                                             # the point where the model has seen\n",
    "#                                             # all the training samples at least once\n",
    "# min_lr = 1e-8\n",
    "# factor = 0.1\n",
    "# SCHEDULE_EPOCH = 200\n",
    "# STEPS = 200\n",
    "\n",
    "# historyC = {\"history\":{\"loss\":[],\"mae\":[],\"val_loss\":[],\"val_mae\":[]}}\n",
    "# for e in range(epochs):\n",
    "#     for i, (images, y_batch) in enumerate(train_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#        loss = modelC.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#     #    val = modelB.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyC[\"history\"][\"loss\"].append(loss[0])\n",
    "#             historyC[\"history\"][\"mae\"].append(loss[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             break  \n",
    "#     for i, (images, y_batch) in enumerate(val_generator):\n",
    "#        new_y_batch = []\n",
    "#        for x,img in enumerate(images):\n",
    "#         array = np.ones(image_size+(3,))\n",
    "#         array *= img>0\n",
    "#         array[array>0] = y_batch[x]\n",
    "#         new_y_batch.append(array)\n",
    "#        new_y_batch = np.array(new_y_batch)\n",
    "#     #    loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "#        val = modelC.test_on_batch(images, new_y_batch)\n",
    "#        if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "#             historyC[\"history\"][\"val_loss\"].append(val[0])\n",
    "#             historyC[\"history\"][\"val_mae\"].append(val[1])\n",
    "#             # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "#             curr_lr = modelC.optimizer.learning_rate\n",
    "#             if e>=SCHEDULE_EPOCH and curr_lr>min_lr: \n",
    "#                K.set_value(modelC.optimizer.learning_rate, curr_lr*factor)\n",
    "#                curr_lr = modelC.optimizer.learning_rate\n",
    "#                SCHEDULE_EPOCH+=STEPS\n",
    "#             # patience = 2\n",
    "#             # if e>patience and curr_lr>min_lr:\n",
    "#                # curr_loss = historyA[\"history\"][\"val_loss\"][-1]\n",
    "#                # for i in historyA[\"history\"][\"val_loss\"][-patience-1:-1]:\n",
    "#                #    if curr_loss >= i: patience-=1\n",
    "#                #    if patience < 1:\n",
    "#                      # K.set_value(modelA.optimizer.learning_rate, curr_lr*factor)\n",
    "#                      # curr_lr = modelA.optimizer.learning_rate\n",
    "#             break  \n",
    "#     print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}   CURR_LR {:.2E}\".format(e+1, loss[0], loss[1],val[0],val[1], curr_lr.numpy()))\n",
    "#     train_generator.on_epoch_end()  # this shuffles the data at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyC[\"history\"][\"loss\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyC[\"history\"][\"val_loss\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyC[\"history\"][\"mae\"][10:], color='g',alpha=.5)\n",
    "# plt.plot(historyC[\"history\"][\"val_mae\"][10:], color='r',alpha=.7)\n",
    "# plt.legend([\"Train\",\"Val\"])\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "\n",
    "# plt.suptitle(\"Model C: DeepLabV3+\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(train_generator)\n",
    "# result = modelC(sample[0])\n",
    "# for x,i in enumerate(result):\n",
    "#     RGB  = cv2.cvtColor(sample[0][x],cv2.COLOR_HSV2RGB)\n",
    "#     MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "#     ARRAY = np.array(result[5]).reshape(image_size) * MASK\n",
    "#     OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "#     print(np.average(OUTPUT), np.average(sample[1][x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelC.save('design_models/designC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shutil import rmtree\n",
    "# # removing directory \n",
    "# rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANUFACTURABILITY: TRAINING TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 128\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1,\n",
    "                              patience=6, min_lr=1e-9)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=16, mode=\"min\", restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DESIGN A\n",
    "import time \n",
    "start = time.time()\n",
    "historyA = modelA.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [es, reduce_lr])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")\n",
    "\n",
    "# ## FUNCTIONALITY: INFERENCE TIME\n",
    "# modelC.evaluate(train_generator[1][0][0].reshape(1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DESIGN B\n",
    "import time \n",
    "start = time.time()\n",
    "historyB = modelB.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [es, reduce_lr])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DESIGN C\n",
    "import time \n",
    "start = time.time()\n",
    "historyC = modelC.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [es, reduce_lr])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design A\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculae FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsA = get_flops(modelA, batch_size=1)\n",
    "print(f\"FLOPS: {flopsA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design B\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculae FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsB = get_flops(modelB, batch_size=1)\n",
    "print(f\"FLOPS: {flopsB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design C\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculae FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsC = get_flops(modelC, batch_size=1)\n",
    "print(f\"FLOPS: {flopsC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONALITY: INFERENCE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sample = train_generator[1][0][0].reshape(1,32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design A\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelA.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design B\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelB.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design AC\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelC.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORMANCE: COEFFICIENT OF DETERMINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare set of x values and y values for performance constraint\n",
    "X_values,y_values = [],[]\n",
    "for i in range(100):\n",
    "    values = next(val_generator)\n",
    "    # for j in range(values[0].shape[0]):\n",
    "    X_values.append(values[0])\n",
    "    y_values.append(values[1])\n",
    "\n",
    "## create X_values generator\n",
    "gen_X_values_1 = (x for x in X_values)\n",
    "gen_X_values_2 = (x for x in X_values)\n",
    "gen_X_values_3 = (x for x in X_values)\n",
    "y_values = [y  for y_set in y_values for y in y_set]\n",
    "y_values_0 = np.array(y_values)[:,:,:,0]\n",
    "y_values_1 = np.array(y_values)[:,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_values = np.array(y_values)\n",
    "predictionsA = modelA.predict(gen_X_values_1)\n",
    "\n",
    "y_values_flat_0 = y_values[:, :, :, 0].flatten()\n",
    "predictionsA_flat_0 = predictionsA[:, :, :, 0].flatten()\n",
    "\n",
    "y_values_flat_1 = y_values[:, :, :, 1].flatten()\n",
    "predictionsA_flat_1 = predictionsA[:, :, :, 1].flatten()\n",
    "\n",
    "# Calculate R^2 score for each class\n",
    "r2_score_0 = r2_score(y_values_flat_0, predictionsA_flat_0)\n",
    "r2_score_1 = r2_score(y_values_flat_1, predictionsA_flat_1)\n",
    "\n",
    "print(f'R^2 for class 0: {r2_score_0:.4f}')\n",
    "print(f'R^2 for class 1: {r2_score_1:.4f}')\n",
    "\n",
    "print(f\"Average R^2 score: {(r2_score_0+r2_score_1)/2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design B\n",
    "## PERFORMANCE: COEFFICIENT OF DETERMINATION\n",
    "from sklearn.metrics import roc_auc_score\n",
    "predictionsB = modelB.predict(gen_X_values_2)\n",
    "\n",
    "y_values_flat_0 = y_values[:, :, :, 0].flatten()\n",
    "predictionsB_flat_0 = predictionsB[:, :, :, 0].flatten()\n",
    "\n",
    "y_values_flat_1 = y_values[:, :, :, 1].flatten()\n",
    "predictionsB_flat_1 = predictionsB[:, :, :, 1].flatten()\n",
    "\n",
    "# Calculate R^2 score for each class\n",
    "r2_score_0 = r2_score(y_values_flat_0, predictionsB_flat_0)\n",
    "r2_score_1 = r2_score(y_values_flat_1, predictionsB_flat_1)\n",
    "\n",
    "print(f'R^2 for class 0: {r2_score_0:.4f}')\n",
    "print(f'R^2 for class 1: {r2_score_1:.4f}')\n",
    "\n",
    "print(f\"Average R^2 score: {(r2_score_0+r2_score_1)/2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design B\n",
    "## PERFORMANCE: COEFFICIENT OF DETERMINATION\n",
    "from sklearn.metrics import roc_auc_score\n",
    "predictionsC = modelC.predict(gen_X_values_3)\n",
    "\n",
    "y_values_flat_0 = y_values[:, :, :, 0].flatten()\n",
    "predictionsC_flat_0 = predictionsC[:, :, :, 0].flatten()\n",
    "\n",
    "y_values_flat_1 = y_values[:, :, :, 1].flatten()\n",
    "predictionsC_flat_1 = predictionsC[:, :, :, 1].flatten()\n",
    "\n",
    "# Calculate R^2 score for each class\n",
    "r2_score_0 = r2_score(y_values_flat_0, predictionsC_flat_0)\n",
    "r2_score_1 = r2_score(y_values_flat_1, predictionsC_flat_1)\n",
    "\n",
    "print(f'R^2 for class 0: {r2_score_0:.4f}')\n",
    "print(f'R^2 for class 1: {r2_score_1:.4f}')\n",
    "\n",
    "print(f\"Average R^2 score: {(r2_score_0+r2_score_1)/2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFFICIENCY: STORAGE CONSUMPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsA = modelA.get_weights()\n",
    "total_sizeA = 0\n",
    "for weight in weightsA:\n",
    "    total_sizeA += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeA*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsB = modelB.get_weights()\n",
    "total_sizeB = 0\n",
    "for weight in weightsB:\n",
    "    total_sizeB += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeB*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsC = modelC.get_weights()\n",
    "total_sizeC = 0\n",
    "for weight in weightsC:\n",
    "    total_sizeC += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeC*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving final trained and constrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA.save('design_models/designA_v5.h5')\n",
    "modelB.save('design_models/designB_v5.h5')\n",
    "modelC.save('design_models/designC_v5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
