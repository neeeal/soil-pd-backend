{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Flatten, LeakyReLU, ReLU, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, concatenate, Activation\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.backend import clear_session\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop , SGD\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "# from tensorflow.keras.regularizers import L2\n",
    "# from tensorflow.keras import metrics\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from shutil import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>close_seg_\\6\\6K--22-_jpg.rf.663142c881758efb4d...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>close_seg_\\6\\6k--75-_jpg.rf.42aa04547265d5890c...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>close_seg_\\6\\6F--23-_jpg.rf.338da0e01de039e7b7...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>close_seg_\\6\\6F--44-_jpg.rf.1521f3dad55c9c68fd...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>close_seg_\\6\\6k--13-_jpg.rf.47db8c03884b63f6b6...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  pH\n",
       "0  close_seg_\\6\\6K--22-_jpg.rf.663142c881758efb4d...   6\n",
       "1  close_seg_\\6\\6k--75-_jpg.rf.42aa04547265d5890c...   6\n",
       "2  close_seg_\\6\\6F--23-_jpg.rf.338da0e01de039e7b7...   6\n",
       "3  close_seg_\\6\\6F--44-_jpg.rf.1521f3dad55c9c68fd...   6\n",
       "4  close_seg_\\6\\6k--13-_jpg.rf.47db8c03884b63f6b6...   6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"df//train_df.csv\")\n",
    "val_df = pd.read_csv(\"df//val_df.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(listdir(\"jpgData//6\")))\n",
    "# print(len(listdir(\"jpgData//7\")))\n",
    "# print(len(listdir(\"jpgData//8\")))\n",
    "# print(len(listdir(\"jpgData//9\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = \"path\"\n",
    "y_col = \"pH\"\n",
    "batch_size = 32\n",
    "epochs = 1024\n",
    "lr = 1e-5\n",
    "image_size = (128,128)\n",
    "channels = 3\n",
    "shuffle = True\n",
    "class_mode =\"raw\"\n",
    "color_mode = \"rgb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 512 validated image filenames.\n",
      "Found 131 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                     width_shift_range = 0.2,\n",
    "                                     height_shift_range = 0.2, \n",
    "                                     zoom_range = 0.2,\n",
    "                                     shear_range = 0.2,\n",
    "                                     horizontal_flip = True,\n",
    "                                     vertical_flip = True,\n",
    "                                    #  channel_shift_range = 64.0,\n",
    "                                     brightness_range = (0.5,1.0),\n",
    "                                     rotation_range = 45,\n",
    "                                     )\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\n",
    "                                              x_col=x_col, y_col=y_col, has_ext=True, \n",
    "                                              class_mode=class_mode, target_size=image_size, \n",
    "                                              batch_size=batch_size, color_mode = color_mode,\n",
    "                                              shuffle = shuffle)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_generator = val_datagen.flow_from_dataframe(dataframe=val_df,\n",
    "                                              x_col=x_col, y_col=y_col, has_ext=True, \n",
    "                                              class_mode=class_mode, target_size=image_size, \n",
    "                                              batch_size=batch_size,  color_mode = color_mode,\n",
    "                                              shuffle = shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design A: Nested U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Convolutional Block\n",
    "def conv_block(inputs, num_filters):\n",
    "\t# Applying the sequence of Convolutional, Batch Normalization\n",
    "\t# and Activation Layers to the input tensor\n",
    "\tx = Sequential([\n",
    "\t\t# Convolutional Layer\n",
    "\t\tConv2D(num_filters, 1, padding='same'),\n",
    "\t\t# Batch Normalization Layer\n",
    "\t\tBatchNormalization(),\n",
    "\t\t# Activation Layer\n",
    "\t\tReLU(),\n",
    "\t\t# Convolutional Layer\n",
    "\t\tConv2D(num_filters, 1, padding='same'),\n",
    "\t\t# Batch Normalization Layer\n",
    "\t\tBatchNormalization(),\n",
    "\t\t# Activation Layer\n",
    "\t\tReLU()\n",
    "\t])(inputs)\n",
    "\n",
    "\t# Returning the output of the Convolutional Block\n",
    "\treturn x\n",
    "\n",
    "# Defining the Unet++ Model\n",
    "def unet_plus_plus_model(input_shape=(image_size[0], image_size[1], channels), num_classes=1, deep_supervision=True):\n",
    "\tinputs = Input(shape=input_shape)\n",
    "\n",
    "\t# Encoding Path\n",
    "\tx_00 = conv_block(inputs, 64)\n",
    "\tx_10 = conv_block(MaxPooling2D()(x_00), 128)\n",
    "\tx_20 = conv_block(MaxPooling2D()(x_10), 256)\n",
    "\tx_30 = conv_block(MaxPooling2D()(x_20), 512)\n",
    "\tx_40 = conv_block(MaxPooling2D()(x_30), 1024)\n",
    "\n",
    "\t# Nested Decoding Path\n",
    "\tx_01 = conv_block(concatenate(\n",
    "\t\t[x_00, UpSampling2D()(x_10)]), 64)\n",
    "\tx_11 = conv_block(concatenate(\n",
    "\t\t[x_10, UpSampling2D()(x_20)]), 128)\n",
    "\tx_21 = conv_block(concatenate(\n",
    "\t\t[x_20, UpSampling2D()(x_30)]), 256)\n",
    "\tx_31 = conv_block(concatenate(\n",
    "\t\t[x_30, UpSampling2D()(x_40)]), 512)\n",
    "\n",
    "\tx_02 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, UpSampling2D()(x_11)]), 64)\n",
    "\tx_12 = conv_block(concatenate(\n",
    "\t\t[x_10, x_11, UpSampling2D()(x_21)]), 128)\n",
    "\tx_22 = conv_block(concatenate(\n",
    "\t\t[x_20, x_21, UpSampling2D()(x_31)]), 256)\n",
    "\n",
    "\tx_03 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, x_02, UpSampling2D()(x_12)]), 64)\n",
    "\tx_13 = conv_block(concatenate(\n",
    "\t\t[x_10, x_11, x_12, UpSampling2D()(x_22)]), 128)\n",
    "\n",
    "\tx_04 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), 64)\n",
    "\n",
    "\t# Deep Supervision Path\n",
    "\t# If deep supervision is enabled, then the model will output the segmentation maps\n",
    "\t# at each stage of the decoding path\n",
    "\tif deep_supervision:\n",
    "\t\toutputs = [\n",
    "\t\t\tConv2D(num_classes, 1)(x_01),\n",
    "\t\t\tConv2D(num_classes, 1)(x_02),\n",
    "\t\t\tConv2D(num_classes, 1)(x_03),\n",
    "\t\t\tConv2D(num_classes, 1)(x_04)\n",
    "\t\t]\n",
    "\t\t# Concatenating the segmentation maps\n",
    "\t\toutputs = concatenate(outputs, axis=0)\n",
    "\n",
    "\t# If deep supervision is disabled, then the model will output the final segmentation map\n",
    "\t# which is the segmentation map at the end of the decoding path\n",
    "\telse:\n",
    "\t\tflatten  = layers.Flatten()(x_04)\n",
    "\t\tdense = Dense(1024, activation='relu')(flatten)\n",
    "\t\t# dense = Dense(512, activation='relu')(dense)\n",
    "\t\toutputs = Dense(1, activation='linear')(dense)\n",
    "\n",
    "\t# Creating the model\n",
    "\tmodel = tf.keras.Model(\n",
    "\t\tinputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "\n",
    "\t# Returning the model\n",
    "\treturn model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Unet_plus_plus\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 32, 32, 64)   4928        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 16, 16, 64)   0           ['sequential[0][0]']             \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 16, 16, 128)  25856       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 128)   0           ['sequential_1[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 8, 8, 256)    100864      ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 256)   0           ['sequential_2[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)      (None, 4, 4, 512)    398336      ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 2, 2, 512)   0           ['sequential_3[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_4 (Sequential)      (None, 2, 2, 1024)   1583104     ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 4, 4, 1024)  0           ['sequential_4[0][0]']           \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 8, 8, 512)   0           ['sequential_3[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 4, 4, 1536)   0           ['sequential_3[0][0]',           \n",
      "                                                                  'up_sampling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 256)  0          ['sequential_2[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 8, 8, 768)    0           ['sequential_2[0][0]',           \n",
      "                                                                  'up_sampling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " sequential_8 (Sequential)      (None, 4, 4, 512)    1053696     ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 32, 32, 128)  0           ['sequential_1[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 16, 16, 384)  0           ['sequential_1[0][0]',           \n",
      "                                                                  'up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " sequential_7 (Sequential)      (None, 8, 8, 256)    264704      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_6 (UpSampling2D)  (None, 8, 8, 512)   0           ['sequential_8[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 192)  0           ['sequential[0][0]',             \n",
      "                                                                  'up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)      (None, 16, 16, 128)  66816       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_5 (UpSampling2D)  (None, 16, 16, 256)  0          ['sequential_7[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 8, 8, 1024)   0           ['sequential_2[0][0]',           \n",
      "                                                                  'sequential_7[0][0]',           \n",
      "                                                                  'up_sampling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " sequential_5 (Sequential)      (None, 32, 32, 64)   17024       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " up_sampling2d_4 (UpSampling2D)  (None, 32, 32, 128)  0          ['sequential_6[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 16, 16, 512)  0           ['sequential_1[0][0]',           \n",
      "                                                                  'sequential_6[0][0]',           \n",
      "                                                                  'up_sampling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " sequential_11 (Sequential)     (None, 8, 8, 256)    330240      ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 32, 32, 256)  0           ['sequential[0][0]',             \n",
      "                                                                  'sequential_5[0][0]',           \n",
      "                                                                  'up_sampling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " sequential_10 (Sequential)     (None, 16, 16, 128)  83200       ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_8 (UpSampling2D)  (None, 16, 16, 256)  0          ['sequential_11[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_9 (Sequential)      (None, 32, 32, 64)   21120       ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_7 (UpSampling2D)  (None, 32, 32, 128)  0          ['sequential_10[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 16, 16, 640)  0           ['sequential_1[0][0]',           \n",
      "                                                                  'sequential_6[0][0]',           \n",
      "                                                                  'sequential_10[0][0]',          \n",
      "                                                                  'up_sampling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 32, 32, 320)  0           ['sequential[0][0]',             \n",
      "                                                                  'sequential_5[0][0]',           \n",
      "                                                                  'sequential_9[0][0]',           \n",
      "                                                                  'up_sampling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " sequential_13 (Sequential)     (None, 16, 16, 128)  99584       ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_12 (Sequential)     (None, 32, 32, 64)   25216       ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_9 (UpSampling2D)  (None, 32, 32, 128)  0          ['sequential_13[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 384)  0           ['sequential[0][0]',             \n",
      "                                                                  'sequential_5[0][0]',           \n",
      "                                                                  'sequential_9[0][0]',           \n",
      "                                                                  'sequential_12[0][0]',          \n",
      "                                                                  'up_sampling2d_9[0][0]']        \n",
      "                                                                                                  \n",
      " sequential_14 (Sequential)     (None, 32, 32, 64)   29312       ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 65536)        0           ['sequential_14[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         67109888    ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            1025        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 71,214,913\n",
      "Trainable params: 71,200,321\n",
      "Non-trainable params: 14,592\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating the model\n",
    "modelA = unet_plus_plus_model(input_shape=(\n",
    "\timage_size[0], image_size[1], channels), deep_supervision=False)\n",
    "modelA.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mean_squared_error', metrics=['mae'])\n",
    "modelA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# historyA = modelA.fit(train_generator, epochs=10, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Graphing Result of training\n",
    "# plt.figure(figsize=(15,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyA.history['mae'], color ='r')\n",
    "# plt.plot(historyA.history['val_mae'])\n",
    "# plt.title('mae')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mae')\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyA.history['loss'], color ='r')\n",
    "# plt.plot(historyA.history['val_loss'])\n",
    "# plt.title('loss')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mean squared error')\n",
    "\n",
    "# # displaying the figure\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Best model saved\n",
    "# train_loss = historyA.history['loss']\n",
    "# train_mae = historyA.history['mae']\n",
    "# val_loss = historyA.history['val_loss']\n",
    "# val_mae = historyA.history['val_mae']\n",
    "\n",
    "# best_epoch = np.argmin(np.array(val_loss))\n",
    "\n",
    "# print(\"Best Epoch: \", best_epoch)\n",
    "# print(\"Final Train Loss: {:.4f}\".format(train_loss[best_epoch]))\n",
    "# print(\"Final Validation Loss: {:.4f}\".format(val_loss[best_epoch]))\n",
    "# print(\"Final Train MAE: {:.2f}\".format(train_mae[best_epoch]*100))\n",
    "# print(\"Final Test MAE: {:.2f}\".format(val_mae[best_epoch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design B: SegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segnet(input_shape):\n",
    "\n",
    "    # Encoding layer\n",
    "    img_input = Input(shape= input_shape)\n",
    "    x = Conv2D(64, (3, 3), padding='same', name='conv1',strides= (1,1))(img_input)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', name='conv2')(x)\n",
    "    x = BatchNormalization(name='bn2')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), padding='same', name='conv3')(x)\n",
    "    x = BatchNormalization(name='bn3')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', name='conv4')(x)\n",
    "    x = BatchNormalization(name='bn4')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='conv5')(x)\n",
    "    x = BatchNormalization(name='bn5')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='conv6')(x)\n",
    "    x = BatchNormalization(name='bn6')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='conv7')(x)\n",
    "    x = BatchNormalization(name='bn7')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv8')(x)\n",
    "    x = BatchNormalization(name='bn8')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv9')(x)\n",
    "    x = BatchNormalization(name='bn9')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv10')(x)\n",
    "    x = BatchNormalization(name='bn10')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    \n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv11')(x)\n",
    "    x = BatchNormalization(name='bn11')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv12')(x)\n",
    "    x = BatchNormalization(name='bn12')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='conv13')(x)\n",
    "    x = BatchNormalization(name='bn13')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Dense(1024, activation = 'relu', name='fc1')(x)\n",
    "    x = Dense(1024, activation = 'relu', name='fc2')(x)\n",
    "    # Decoding Layer \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv1')(x)\n",
    "    x = BatchNormalization(name='bn14')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv2')(x)\n",
    "    x = BatchNormalization(name='bn15')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv3')(x)\n",
    "    x = BatchNormalization(name='bn16')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv4')(x)\n",
    "    x = BatchNormalization(name='bn17')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(512, (3, 3), padding='same', name='deconv5')(x)\n",
    "    x = BatchNormalization(name='bn18')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv6')(x)\n",
    "    x = BatchNormalization(name='bn19')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv7')(x)\n",
    "    x = BatchNormalization(name='bn20')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(256, (3, 3), padding='same', name='deconv8')(x)\n",
    "    x = BatchNormalization(name='bn21')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(128, (3, 3), padding='same', name='deconv9')(x)\n",
    "    x = BatchNormalization(name='bn22')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(128, (3, 3), padding='same', name='deconv10')(x)\n",
    "    x = BatchNormalization(name='bn23')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), padding='same', name='deconv11')(x)\n",
    "    x = BatchNormalization(name='bn24')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), padding='same', name='deconv12')(x)\n",
    "    x = BatchNormalization(name='bn25')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(1, (3, 3), padding='same', name='deconv13')(x)\n",
    "    x = BatchNormalization(name='bn26')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    # pred = Reshape((192,256))(x)\n",
    "    x = Flatten()(x)\n",
    "    x=Dense(1024, activation='relu')(x)\n",
    "    otuput=Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=img_input, outputs=otuput)\n",
    "    \n",
    "    model.compile(optimizer= tf.keras.optimizers.Adam(lr=lr), loss= [\"mse\"]\n",
    "                  , metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " bn1 (BatchNormalization)    (None, 32, 32, 64)        256       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " bn2 (BatchNormalization)    (None, 32, 32, 64)        256       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " bn3 (BatchNormalization)    (None, 16, 16, 128)       512       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv4 (Conv2D)              (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " bn4 (BatchNormalization)    (None, 16, 16, 128)       512       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv5 (Conv2D)              (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " bn5 (BatchNormalization)    (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv6 (Conv2D)              (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn6 (BatchNormalization)    (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv7 (Conv2D)              (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn7 (BatchNormalization)    (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 4, 4, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv8 (Conv2D)              (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " bn8 (BatchNormalization)    (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv9 (Conv2D)              (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn9 (BatchNormalization)    (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv10 (Conv2D)             (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn10 (BatchNormalization)   (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 2, 2, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv11 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn11 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv12 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn12 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv13 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn13 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 1, 1, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 1, 1, 1024)        525312    \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 1, 1, 1024)        1049600   \n",
      "                                                                 \n",
      " up_sampling2d_10 (UpSamplin  (None, 2, 2, 1024)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv1 (Conv2DTranspose)   (None, 2, 2, 512)         4719104   \n",
      "                                                                 \n",
      " bn14 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " deconv2 (Conv2DTranspose)   (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn15 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " deconv3 (Conv2DTranspose)   (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn16 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " up_sampling2d_11 (UpSamplin  (None, 4, 4, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv4 (Conv2DTranspose)   (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn17 (BatchNormalization)   (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " deconv5 (Conv2DTranspose)   (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn18 (BatchNormalization)   (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " deconv6 (Conv2DTranspose)   (None, 4, 4, 256)         1179904   \n",
      "                                                                 \n",
      " bn19 (BatchNormalization)   (None, 4, 4, 256)         1024      \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_12 (UpSamplin  (None, 8, 8, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv7 (Conv2DTranspose)   (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn20 (BatchNormalization)   (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " deconv8 (Conv2DTranspose)   (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn21 (BatchNormalization)   (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " deconv9 (Conv2DTranspose)   (None, 8, 8, 128)         295040    \n",
      "                                                                 \n",
      " bn22 (BatchNormalization)   (None, 8, 8, 128)         512       \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " up_sampling2d_13 (UpSamplin  (None, 16, 16, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv10 (Conv2DTranspose)  (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " bn23 (BatchNormalization)   (None, 16, 16, 128)       512       \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " deconv11 (Conv2DTranspose)  (None, 16, 16, 64)        73792     \n",
      "                                                                 \n",
      " bn24 (BatchNormalization)   (None, 16, 16, 64)        256       \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " up_sampling2d_14 (UpSamplin  (None, 32, 32, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv12 (Conv2DTranspose)  (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " bn25 (BatchNormalization)   (None, 32, 32, 64)        256       \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " deconv13 (Conv2DTranspose)  (None, 32, 32, 1)         577       \n",
      "                                                                 \n",
      " bn26 (BatchNormalization)   (None, 32, 32, 1)         4         \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,444,294\n",
      "Trainable params: 34,428,420\n",
      "Non-trainable params: 15,874\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelB = segnet(input_shape=image_size+(3,))\n",
    "modelB.summary()\n",
    "\n",
    "\n",
    "# model.save(savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = modelB.fit(train_generator, epochs= 10, validation_data= val_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelB = segnet(input_shape=image_size+(3,))\n",
    "# modelB.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mse', metrics=['mae'])\n",
    "# modelB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## MANUFACTURABILITY: TRAINING TIME\n",
    "# import time \n",
    "# start = time.time()\n",
    "# historyB = modelB.fit(train_generator, epochs=10, validation_data=val_generator, verbose=0)\n",
    "# stop = time.time()\n",
    "# print(f\"Training time: {stop - start}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Graphing Result of training\n",
    "# plt.figure(figsize=(15,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyB.history['mae'], color ='r')\n",
    "# plt.plot(historyB.history['val_mae'])\n",
    "# plt.title('mae')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mae')\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyB.history['loss'], color ='r')\n",
    "# plt.plot(historyB.history['val_loss'])\n",
    "# plt.title('loss')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mean squared error')\n",
    "\n",
    "# # displaying the figure\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Best model saved\n",
    "# train_loss = historyB.history['loss']\n",
    "# train_mae = historyB.history['mae']\n",
    "# val_loss = historyB.history['val_loss']\n",
    "# val_mae = historyB.history['val_mae']\n",
    "\n",
    "# best_epoch = np.argmin(np.array(val_loss))\n",
    "\n",
    "# print(\"Best Epoch: \", best_epoch)\n",
    "# print(\"Final Train Loss: {:.4f}\".format(train_loss[best_epoch]))\n",
    "# print(\"Final Validation Loss: {:.4f}\".format(val_loss[best_epoch]))\n",
    "# print(\"Final Train MAE: {:.2f}\".format(train_mae[best_epoch]*100))\n",
    "# print(\"Final Test MAE: {:.2f}\".format(val_mae[best_epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8, 7, 9, 8, 9, 9, 6, 9, 6, 7, 9, 8, 7, 8, 6, 7, 6, 9, 9, 7, 8,\n",
       "       6, 7, 8, 6, 9, 9, 7, 7, 6, 6], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design C: DeepLabv3+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 38, 38, 3)    0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 16, 16, 64)   9472        ['conv1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 16, 16, 64)   256         ['conv1_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 16, 16, 64)   0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 18, 18, 64)   0           ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 8, 8, 64)     0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 8, 8, 64)     4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 8, 8, 256)    16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 8, 8, 256)    0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 8, 8, 64)     16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 8, 8, 256)    0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 8, 8, 64)     16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 8, 8, 256)    0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 4, 4, 128)    32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 4, 4, 512)    131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 4, 4, 512)    0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 4, 4, 512)    0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 4, 4, 512)    0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 4, 4, 512)    0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 2, 2, 256)    131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 2, 2, 1024)   525312      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                                                  'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block1_out[0][0]',       \n",
      "                                                                  'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block2_out[0][0]',       \n",
      "                                                                  'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block3_out[0][0]',       \n",
      "                                                                  'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block4_out[0][0]',       \n",
      "                                                                  'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block5_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 1, 1, 256)   0           ['conv4_block6_2_relu[0][0]']    \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 1, 1, 256)    65792       ['average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 1, 1, 256)   1024        ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 2, 2, 256)    65536       ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 2, 2, 256)    589824      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 2, 2, 256)    589824      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 2, 2, 256)    589824      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_18 (TFOpLambda)     (None, 1, 1, 256)    0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_6 (UpSampling2D)  (None, 2, 2, 256)   0           ['tf.nn.relu_18[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_19 (TFOpLambda)     (None, 2, 2, 256)    0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " tf.nn.relu_20 (TFOpLambda)     (None, 2, 2, 256)    0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " tf.nn.relu_21 (TFOpLambda)     (None, 2, 2, 256)    0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " tf.nn.relu_22 (TFOpLambda)     (None, 2, 2, 256)    0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 2, 2, 1280)   0           ['up_sampling2d_6[0][0]',        \n",
      "                                                                  'tf.nn.relu_19[0][0]',          \n",
      "                                                                  'tf.nn.relu_20[0][0]',          \n",
      "                                                                  'tf.nn.relu_21[0][0]',          \n",
      "                                                                  'tf.nn.relu_22[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 2, 2, 256)    327680      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 8, 8, 48)     3072        ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_23 (TFOpLambda)     (None, 2, 2, 256)    0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 8, 8, 48)    192         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_7 (UpSampling2D)  (None, 8, 8, 256)   0           ['tf.nn.relu_23[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_24 (TFOpLambda)     (None, 8, 8, 48)     0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 8, 8, 304)    0           ['up_sampling2d_7[0][0]',        \n",
      "                                                                  'tf.nn.relu_24[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 8, 8, 256)    700416      ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.nn.relu_25 (TFOpLambda)     (None, 8, 8, 256)    0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 8, 8, 256)    589824      ['tf.nn.relu_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.nn.relu_26 (TFOpLambda)     (None, 8, 8, 256)    0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_8 (UpSampling2D)  (None, 32, 32, 256)  0          ['tf.nn.relu_26[0][0]']          \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 256)         0           ['up_sampling2d_8[0][0]']        \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1024)         263168      ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            1025        ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,116,289\n",
      "Trainable params: 12,083,553\n",
      "Non-trainable params: 32,736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def convolution_block(\n",
    "    block_input,\n",
    "    num_filters=256,\n",
    "    kernel_size=3,\n",
    "    dilation_rate=1,\n",
    "    padding=\"same\",\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding=\"same\",\n",
    "        use_bias=use_bias,\n",
    "        kernel_initializer=keras.initializers.HeNormal(),\n",
    "    )(block_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def DilatedSpatialPyramidPooling(dspp_input):\n",
    "    dims = dspp_input.shape\n",
    "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "    out_pool = layers.UpSampling2D(\n",
    "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "\n",
    "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
    "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
    "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
    "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "    output = convolution_block(x, kernel_size=1)\n",
    "    return output\n",
    "\n",
    "def DeeplabV3Plus(image_size):\n",
    "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    resnet50 = keras.applications.ResNet50(\n",
    "        weights=None, include_top=False, input_tensor=model_input\n",
    "    )\n",
    "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
    "    x = DilatedSpatialPyramidPooling(x)\n",
    "\n",
    "    input_a = layers.UpSampling2D(\n",
    "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "    x = convolution_block(x)\n",
    "    x = convolution_block(x)\n",
    "    x = layers.UpSampling2D(\n",
    "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x= Dense(1024, activation=\"relu\",)(x)\n",
    "    model_output = layers.Dense(1, activation='linear')(x)\n",
    "    return keras.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "\n",
    "modelC = DeeplabV3Plus(image_size=image_size[0])\n",
    "modelC.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelC.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mse', metrics=['mae'])\n",
    "\n",
    "# historyC = modelC.fit(train_generator, epochs=10, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Graphing Result of training\n",
    "# plt.figure(figsize=(15,4))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.plot(historyC.history['mae'], color ='r')\n",
    "# plt.plot(historyC.history['val_mae'])\n",
    "# plt.title('mae')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mae')\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.plot(historyC.history['loss'], color ='r')\n",
    "# plt.plot(historyC.history['val_loss'])\n",
    "# plt.title('loss')\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('mean squared error')\n",
    "\n",
    "# # displaying the figure\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Best model saved\n",
    "# train_loss = historyC.history['loss']\n",
    "# train_mae = historyC.history['mae']\n",
    "# val_loss = historyC.history['val_loss']\n",
    "# val_mae = historyC.history['val_mae']\n",
    "\n",
    "# best_epoch = np.argmin(np.array(val_loss))\n",
    "\n",
    "# print(\"Best Epoch: \", best_epoch)\n",
    "# print(\"Final Train Loss: {:.4f}\".format(train_loss[best_epoch]))\n",
    "# print(\"Final Validation Loss: {:.4f}\".format(val_loss[best_epoch]))\n",
    "# print(\"Final Train MAE: {:.2f}\".format(train_mae[best_epoch]*100))\n",
    "# print(\"Final Test MAE: {:.2f}\".format(val_mae[best_epoch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Designs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of Designs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Convolutional Block\n",
    "def conv_block(inputs, num_filters):\n",
    "\t# Applying the sequence of Convolutional, Batch Normalization\n",
    "\t# and Activation Layers to the input tensor\n",
    "\tx = Sequential([\n",
    "\t\t# Convolutional Layer\n",
    "\t\tConv2D(num_filters, 1, padding='same'),\n",
    "\t\t# Batch Normalization Layer\n",
    "\t\tBatchNormalization(),\n",
    "\t\t# Activation Layer\n",
    "\t\tReLU(),\n",
    "\t\t# Convolutional Layer\n",
    "\t\tConv2D(num_filters, 1, padding='same'),\n",
    "\t\t# Batch Normalization Layer\n",
    "\t\tBatchNormalization(),\n",
    "\t\t# Activation Layer\n",
    "\t\tReLU()\n",
    "\t])(inputs)\n",
    "\n",
    "\t# Returning the output of the Convolutional Block\n",
    "\treturn x\n",
    "\n",
    "# Defining the Unet++ Model\n",
    "def unet_plus_plus_model(hp):\n",
    "\tinputs = Input(shape=image_size+(3,))\n",
    "\thp_filters = hp.Choice('filters',values = [16,32,64,128])\n",
    "\t# Encoding Path\n",
    "\tx_00 = conv_block(inputs, hp_filters)\n",
    "\tx_10 = conv_block(MaxPooling2D()(x_00), hp_filters*2)\n",
    "\tx_20 = conv_block(MaxPooling2D()(x_10), hp_filters*4)\n",
    "\tx_30 = conv_block(MaxPooling2D()(x_20), hp_filters*8)\n",
    "\tx_40 = conv_block(MaxPooling2D()(x_30), hp_filters*16)\n",
    "\n",
    "\t# Nested Decoding Path\n",
    "\tx_01 = conv_block(concatenate(\n",
    "\t\t[x_00, UpSampling2D()(x_10)]), hp_filters)\n",
    "\tx_11 = conv_block(concatenate(\n",
    "\t\t[x_10, UpSampling2D()(x_20)]), hp_filters*2)\n",
    "\tx_21 = conv_block(concatenate(\n",
    "\t\t[x_20, UpSampling2D()(x_30)]), hp_filters*4)\n",
    "\tx_31 = conv_block(concatenate(\n",
    "\t\t[x_30, UpSampling2D()(x_40)]), hp_filters*8)\n",
    "\n",
    "\tx_02 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, UpSampling2D()(x_11)]), hp_filters)\n",
    "\tx_12 = conv_block(concatenate(\n",
    "\t\t[x_10, x_11, UpSampling2D()(x_21)]), hp_filters*2)\n",
    "\tx_22 = conv_block(concatenate(\n",
    "\t\t[x_20, x_21, UpSampling2D()(x_31)]), hp_filters*4)\n",
    "\n",
    "\tx_03 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, x_02, UpSampling2D()(x_12)]), hp_filters)\n",
    "\tx_13 = conv_block(concatenate(\n",
    "\t\t[x_10, x_11, x_12, UpSampling2D()(x_22)]), hp_filters*2)\n",
    "\n",
    "\tx_04 = conv_block(concatenate(\n",
    "\t\t[x_00, x_01, x_02, x_03, UpSampling2D()(x_13)]), hp_filters)\n",
    "\t\n",
    "\toutputs = tf.keras.layers.Conv2D(1, 1, activation='linear')(x_04)\n",
    "\t# # Deep Supervision Path\n",
    "\t# # If deep supervision is enabled, then the model will output the segmentation maps\n",
    "\t# # at each stage of the decoding path\n",
    "\t# if deep_supervision:\n",
    "\t# \toutputs = [\n",
    "\t# \t\tConv2D(num_classes, 1)(x_01),\n",
    "\t# \t\tConv2D(num_classes, 1)(x_02),\n",
    "\t# \t\tConv2D(num_classes, 1)(x_03),\n",
    "\t# \t\tConv2D(num_classes, 1)(x_04)\n",
    "\t# \t]\n",
    "\t# \t# Concatenating the segmentation maps\n",
    "\t# \toutputs = concatenate(outputs, axis=0)\n",
    "\n",
    "\t# # If deep supervision is disabled, then the model will output the final segmentation map\n",
    "\t# # which is the segmentation map at the end of the decoding path\n",
    "\t# else:\n",
    "\t# flatten  = layers.Flatten()(x_04)\n",
    "\t# hp_units = hp.Choice('units',values = [512,1024])\n",
    "\t# dense = Dense(hp_units, activation='relu')(flatten)\n",
    "\t# if hp.Boolean(\"2nd dense\"):\n",
    "\t# \tdense = Dense(hp_units, activation='relu')(dense)\n",
    "\t# outputs = Dense(1, activation='linear')(dense)\n",
    "\n",
    "\t# Creating the model\n",
    "\tmodel = tf.keras.Model(\n",
    "\t\tinputs=inputs, outputs=outputs, name='Unet_plus_plus')\n",
    "\t\n",
    "\thp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "\tmodel.compile(optimizer=tf.keras.optimizers.Adam(hp_learning_rate), loss='mean_squared_error', metrics=['mae'])\n",
    "\t# Returning the model\n",
    "\treturn model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\design_a\\tuner0.json\n",
      "{'filters': 16, 'learning_rate': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "tunerA = kt.BayesianOptimization(unet_plus_plus_model,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 1,\n",
    "                     project_name='design_a',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tunerA.search(train_generator, epochs=100, validation_data=val_generator, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsA=tunerA.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "print(best_hpsA.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hpsA.values['filters']=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Unet_plus_plus\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential_60 (Sequential)     (None, 128, 128, 64  4928        ['input_5[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_16 (MaxPooling2D  (None, 64, 64, 64)  0           ['sequential_60[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " sequential_61 (Sequential)     (None, 64, 64, 128)  25856       ['max_pooling2d_16[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_17 (MaxPooling2D  (None, 32, 32, 128)  0          ['sequential_61[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " sequential_62 (Sequential)     (None, 32, 32, 256)  100864      ['max_pooling2d_17[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_18 (MaxPooling2D  (None, 16, 16, 256)  0          ['sequential_62[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " sequential_63 (Sequential)     (None, 16, 16, 512)  398336      ['max_pooling2d_18[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_19 (MaxPooling2D  (None, 8, 8, 512)   0           ['sequential_63[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " sequential_64 (Sequential)     (None, 8, 8, 1024)   1583104     ['max_pooling2d_19[0][0]']       \n",
      "                                                                                                  \n",
      " up_sampling2d_43 (UpSampling2D  (None, 16, 16, 1024  0          ['sequential_64[0][0]']          \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_42 (UpSampling2D  (None, 32, 32, 512)  0          ['sequential_63[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_43 (Concatenate)   (None, 16, 16, 1536  0           ['sequential_63[0][0]',          \n",
      "                                )                                 'up_sampling2d_43[0][0]']       \n",
      "                                                                                                  \n",
      " up_sampling2d_41 (UpSampling2D  (None, 64, 64, 256)  0          ['sequential_62[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_42 (Concatenate)   (None, 32, 32, 768)  0           ['sequential_62[0][0]',          \n",
      "                                                                  'up_sampling2d_42[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_68 (Sequential)     (None, 16, 16, 512)  1053696     ['concatenate_43[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_40 (UpSampling2D  (None, 128, 128, 12  0          ['sequential_61[0][0]']          \n",
      " )                              8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_41 (Concatenate)   (None, 64, 64, 384)  0           ['sequential_61[0][0]',          \n",
      "                                                                  'up_sampling2d_41[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_67 (Sequential)     (None, 32, 32, 256)  264704      ['concatenate_42[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_46 (UpSampling2D  (None, 32, 32, 512)  0          ['sequential_68[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_40 (Concatenate)   (None, 128, 128, 19  0           ['sequential_60[0][0]',          \n",
      "                                2)                                'up_sampling2d_40[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_66 (Sequential)     (None, 64, 64, 128)  66816       ['concatenate_41[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_45 (UpSampling2D  (None, 64, 64, 256)  0          ['sequential_67[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_46 (Concatenate)   (None, 32, 32, 1024  0           ['sequential_62[0][0]',          \n",
      "                                )                                 'sequential_67[0][0]',          \n",
      "                                                                  'up_sampling2d_46[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_65 (Sequential)     (None, 128, 128, 64  17024       ['concatenate_40[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_44 (UpSampling2D  (None, 128, 128, 12  0          ['sequential_66[0][0]']          \n",
      " )                              8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_45 (Concatenate)   (None, 64, 64, 512)  0           ['sequential_61[0][0]',          \n",
      "                                                                  'sequential_66[0][0]',          \n",
      "                                                                  'up_sampling2d_45[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_71 (Sequential)     (None, 32, 32, 256)  330240      ['concatenate_46[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_44 (Concatenate)   (None, 128, 128, 25  0           ['sequential_60[0][0]',          \n",
      "                                6)                                'sequential_65[0][0]',          \n",
      "                                                                  'up_sampling2d_44[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_70 (Sequential)     (None, 64, 64, 128)  83200       ['concatenate_45[0][0]']         \n",
      "                                                                                                  \n",
      " up_sampling2d_48 (UpSampling2D  (None, 64, 64, 256)  0          ['sequential_71[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " sequential_69 (Sequential)     (None, 128, 128, 64  21120       ['concatenate_44[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_47 (UpSampling2D  (None, 128, 128, 12  0          ['sequential_70[0][0]']          \n",
      " )                              8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_48 (Concatenate)   (None, 64, 64, 640)  0           ['sequential_61[0][0]',          \n",
      "                                                                  'sequential_66[0][0]',          \n",
      "                                                                  'sequential_70[0][0]',          \n",
      "                                                                  'up_sampling2d_48[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_47 (Concatenate)   (None, 128, 128, 32  0           ['sequential_60[0][0]',          \n",
      "                                0)                                'sequential_65[0][0]',          \n",
      "                                                                  'sequential_69[0][0]',          \n",
      "                                                                  'up_sampling2d_47[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_73 (Sequential)     (None, 64, 64, 128)  99584       ['concatenate_48[0][0]']         \n",
      "                                                                                                  \n",
      " sequential_72 (Sequential)     (None, 128, 128, 64  25216       ['concatenate_47[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_49 (UpSampling2D  (None, 128, 128, 12  0          ['sequential_73[0][0]']          \n",
      " )                              8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_49 (Concatenate)   (None, 128, 128, 38  0           ['sequential_60[0][0]',          \n",
      "                                4)                                'sequential_65[0][0]',          \n",
      "                                                                  'sequential_69[0][0]',          \n",
      "                                                                  'sequential_72[0][0]',          \n",
      "                                                                  'up_sampling2d_49[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_74 (Sequential)     (None, 128, 128, 64  29312       ['concatenate_49[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_154 (Conv2D)            (None, 128, 128, 1)  65          ['sequential_74[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,104,065\n",
      "Trainable params: 4,089,473\n",
      "Non-trainable params: 14,592\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the model with the best hp.\n",
    "modelA = unet_plus_plus_model(best_hpsA)\n",
    "modelA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "16/16 [==============================] - 7s 426ms/step - loss: 46.9311 - mae: 6.7121 - val_loss: 54.9211 - val_mae: 7.3266\n",
      "Epoch 2/8\n",
      "16/16 [==============================] - 7s 426ms/step - loss: 45.8900 - mae: 6.6341 - val_loss: 54.3349 - val_mae: 7.2865\n",
      "Epoch 3/8\n",
      "16/16 [==============================] - 7s 424ms/step - loss: 45.0557 - mae: 6.5724 - val_loss: 53.5190 - val_mae: 7.2302\n",
      "Epoch 4/8\n",
      "16/16 [==============================] - 7s 424ms/step - loss: 44.3943 - mae: 6.5218 - val_loss: 52.7288 - val_mae: 7.1753\n",
      "Epoch 5/8\n",
      "16/16 [==============================] - 7s 425ms/step - loss: 43.5824 - mae: 6.4613 - val_loss: 52.0868 - val_mae: 7.1304\n",
      "Epoch 6/8\n",
      "16/16 [==============================] - 7s 423ms/step - loss: 42.8871 - mae: 6.4076 - val_loss: 51.5027 - val_mae: 7.0893\n",
      "Epoch 7/8\n",
      "16/16 [==============================] - 7s 426ms/step - loss: 42.2458 - mae: 6.3594 - val_loss: 50.9457 - val_mae: 7.0497\n",
      "Epoch 8/8\n",
      "16/16 [==============================] - 7s 424ms/step - loss: 41.6599 - mae: 6.3133 - val_loss: 50.4220 - val_mae: 7.0123\n"
     ]
    }
   ],
   "source": [
    "# red_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "#                               patience=10, min_lr=1e-7)\n",
    "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "historyA = modelA.fit(train_generator, epochs=8, validation_data=val_generator,\n",
    "                     callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 LOSS: 26.563084 MAE: 4.258342 VAL_LOSS: 31.845999 VAL_MAE: 4.341275\n",
      "EPOCH: 2 LOSS: 29.248995 MAE: 4.514007 VAL_LOSS: 33.694996 VAL_MAE: 4.544348\n",
      "EPOCH: 3 LOSS: 24.764332 MAE: 4.143941 VAL_LOSS: 30.149715 VAL_MAE: 4.318193\n",
      "EPOCH: 4 LOSS: 25.670479 MAE: 4.249720 VAL_LOSS: 29.858786 VAL_MAE: 4.308772\n",
      "EPOCH: 5 LOSS: 23.337196 MAE: 4.083032 VAL_LOSS: 32.043995 VAL_MAE: 4.490649\n",
      "EPOCH: 6 LOSS: 23.988148 MAE: 4.164299 VAL_LOSS: 32.701298 VAL_MAE: 4.581276\n",
      "EPOCH: 7 LOSS: 22.661530 MAE: 4.039544 VAL_LOSS: 35.303753 VAL_MAE: 4.815928\n",
      "EPOCH: 8 LOSS: 21.669712 MAE: 3.958996 VAL_LOSS: 29.601875 VAL_MAE: 4.436086\n",
      "EPOCH: 9 LOSS: 18.625494 MAE: 3.684499 VAL_LOSS: 36.656345 VAL_MAE: 5.124908\n",
      "EPOCH: 10 LOSS: 20.530819 MAE: 3.879291 VAL_LOSS: 25.673042 VAL_MAE: 4.307412\n",
      "EPOCH: 11 LOSS: 22.030304 MAE: 4.015653 VAL_LOSS: 25.396744 VAL_MAE: 4.380093\n",
      "EPOCH: 12 LOSS: 20.785172 MAE: 3.896466 VAL_LOSS: 20.486225 VAL_MAE: 4.021935\n",
      "EPOCH: 13 LOSS: 19.086576 MAE: 3.692009 VAL_LOSS: 21.591873 VAL_MAE: 4.149312\n",
      "EPOCH: 14 LOSS: 17.864346 MAE: 3.632332 VAL_LOSS: 18.357428 VAL_MAE: 4.000600\n",
      "EPOCH: 15 LOSS: 19.036873 MAE: 3.694053 VAL_LOSS: 21.873631 VAL_MAE: 4.235597\n",
      "EPOCH: 16 LOSS: 19.294781 MAE: 3.711574 VAL_LOSS: 22.387062 VAL_MAE: 4.302604\n",
      "EPOCH: 17 LOSS: 17.832260 MAE: 3.588257 VAL_LOSS: 20.944153 VAL_MAE: 4.140948\n",
      "EPOCH: 18 LOSS: 16.888252 MAE: 3.511724 VAL_LOSS: 20.060131 VAL_MAE: 4.058009\n",
      "EPOCH: 19 LOSS: 17.052168 MAE: 3.502557 VAL_LOSS: 11.074939 VAL_MAE: 3.196023\n",
      "EPOCH: 20 LOSS: 17.533388 MAE: 3.518867 VAL_LOSS: 20.269001 VAL_MAE: 4.085864\n",
      "EPOCH: 21 LOSS: 19.060509 MAE: 3.637526 VAL_LOSS: 20.070667 VAL_MAE: 4.078187\n",
      "EPOCH: 22 LOSS: 17.066195 MAE: 3.483600 VAL_LOSS: 19.656563 VAL_MAE: 3.997407\n",
      "EPOCH: 23 LOSS: 16.386576 MAE: 3.405992 VAL_LOSS: 17.087490 VAL_MAE: 3.715161\n",
      "EPOCH: 24 LOSS: 17.101494 MAE: 3.527304 VAL_LOSS: 14.473527 VAL_MAE: 3.432711\n",
      "EPOCH: 25 LOSS: 16.631569 MAE: 3.448626 VAL_LOSS: 19.063671 VAL_MAE: 3.696230\n",
      "EPOCH: 26 LOSS: 16.096310 MAE: 3.363494 VAL_LOSS: 17.974659 VAL_MAE: 3.595494\n",
      "EPOCH: 27 LOSS: 15.816487 MAE: 3.374839 VAL_LOSS: 17.379803 VAL_MAE: 3.489937\n",
      "EPOCH: 28 LOSS: 16.419849 MAE: 3.421762 VAL_LOSS: 18.522402 VAL_MAE: 3.597417\n",
      "EPOCH: 29 LOSS: 18.988468 MAE: 3.714826 VAL_LOSS: 20.093691 VAL_MAE: 3.774330\n",
      "EPOCH: 30 LOSS: 13.425940 MAE: 3.080349 VAL_LOSS: 17.592270 VAL_MAE: 3.383713\n",
      "EPOCH: 31 LOSS: 16.915857 MAE: 3.493385 VAL_LOSS: 17.192362 VAL_MAE: 3.340107\n",
      "EPOCH: 32 LOSS: 13.920409 MAE: 3.173340 VAL_LOSS: 17.606281 VAL_MAE: 3.435350\n",
      "EPOCH: 33 LOSS: 16.238846 MAE: 3.408597 VAL_LOSS: 20.194117 VAL_MAE: 3.760475\n",
      "EPOCH: 34 LOSS: 15.350111 MAE: 3.301518 VAL_LOSS: 23.941397 VAL_MAE: 4.047884\n",
      "EPOCH: 35 LOSS: 15.945683 MAE: 3.333712 VAL_LOSS: 19.402931 VAL_MAE: 3.686900\n",
      "EPOCH: 36 LOSS: 14.339390 MAE: 3.170527 VAL_LOSS: 19.036427 VAL_MAE: 3.642472\n",
      "EPOCH: 37 LOSS: 16.347404 MAE: 3.409089 VAL_LOSS: 19.546173 VAL_MAE: 3.690795\n",
      "EPOCH: 38 LOSS: 16.247334 MAE: 3.361820 VAL_LOSS: 18.116308 VAL_MAE: 3.570857\n",
      "EPOCH: 39 LOSS: 14.015274 MAE: 3.130002 VAL_LOSS: 19.000610 VAL_MAE: 3.491701\n",
      "EPOCH: 40 LOSS: 17.820414 MAE: 3.537807 VAL_LOSS: 17.319527 VAL_MAE: 3.557087\n",
      "EPOCH: 41 LOSS: 15.273354 MAE: 3.295312 VAL_LOSS: 17.978001 VAL_MAE: 3.613551\n",
      "EPOCH: 42 LOSS: 14.776998 MAE: 3.191882 VAL_LOSS: 19.667528 VAL_MAE: 3.805152\n",
      "EPOCH: 43 LOSS: 14.785692 MAE: 3.201622 VAL_LOSS: 18.688911 VAL_MAE: 3.720425\n",
      "EPOCH: 44 LOSS: 14.478523 MAE: 3.159398 VAL_LOSS: 19.003876 VAL_MAE: 3.816185\n",
      "EPOCH: 45 LOSS: 14.666191 MAE: 3.205342 VAL_LOSS: 17.015902 VAL_MAE: 3.478177\n",
      "EPOCH: 46 LOSS: 13.854235 MAE: 3.094689 VAL_LOSS: 17.369751 VAL_MAE: 3.515990\n",
      "EPOCH: 47 LOSS: 15.885031 MAE: 3.354540 VAL_LOSS: 17.148451 VAL_MAE: 3.527912\n",
      "EPOCH: 48 LOSS: 14.550019 MAE: 3.177585 VAL_LOSS: 16.464409 VAL_MAE: 3.451907\n",
      "EPOCH: 49 LOSS: 14.026826 MAE: 3.119545 VAL_LOSS: 13.771385 VAL_MAE: 3.275831\n",
      "EPOCH: 50 LOSS: 14.813049 MAE: 3.234615 VAL_LOSS: 16.205467 VAL_MAE: 3.455592\n",
      "EPOCH: 51 LOSS: 13.906481 MAE: 3.123471 VAL_LOSS: 16.545595 VAL_MAE: 3.461566\n",
      "EPOCH: 52 LOSS: 12.666763 MAE: 2.982327 VAL_LOSS: 15.032701 VAL_MAE: 3.273236\n",
      "EPOCH: 53 LOSS: 12.474041 MAE: 2.946580 VAL_LOSS: 16.705627 VAL_MAE: 3.507973\n",
      "EPOCH: 54 LOSS: 13.368802 MAE: 3.041062 VAL_LOSS: 13.144398 VAL_MAE: 3.167601\n",
      "EPOCH: 55 LOSS: 12.215664 MAE: 2.893318 VAL_LOSS: 16.252148 VAL_MAE: 3.429963\n",
      "EPOCH: 56 LOSS: 14.238941 MAE: 3.143149 VAL_LOSS: 16.323807 VAL_MAE: 3.466527\n",
      "EPOCH: 57 LOSS: 14.548519 MAE: 3.211924 VAL_LOSS: 14.234055 VAL_MAE: 3.209697\n",
      "EPOCH: 58 LOSS: 13.255952 MAE: 3.033103 VAL_LOSS: 13.776093 VAL_MAE: 3.167026\n",
      "EPOCH: 59 LOSS: 13.843392 MAE: 3.116102 VAL_LOSS: 10.920009 VAL_MAE: 2.759036\n",
      "EPOCH: 60 LOSS: 14.626362 MAE: 3.191205 VAL_LOSS: 14.616678 VAL_MAE: 3.282418\n",
      "EPOCH: 61 LOSS: 14.297167 MAE: 3.143680 VAL_LOSS: 13.898033 VAL_MAE: 3.148877\n",
      "EPOCH: 62 LOSS: 13.294952 MAE: 3.038500 VAL_LOSS: 15.171591 VAL_MAE: 3.276493\n",
      "EPOCH: 63 LOSS: 14.226364 MAE: 3.113962 VAL_LOSS: 14.825195 VAL_MAE: 3.281576\n",
      "EPOCH: 64 LOSS: 14.527827 MAE: 3.184705 VAL_LOSS: 16.937811 VAL_MAE: 3.553314\n",
      "EPOCH: 65 LOSS: 13.930750 MAE: 3.101382 VAL_LOSS: 13.710451 VAL_MAE: 3.129360\n",
      "EPOCH: 66 LOSS: 13.369064 MAE: 3.018257 VAL_LOSS: 14.426550 VAL_MAE: 3.215582\n",
      "EPOCH: 67 LOSS: 10.832793 MAE: 2.669295 VAL_LOSS: 14.698895 VAL_MAE: 3.257947\n",
      "EPOCH: 68 LOSS: 12.158866 MAE: 2.898438 VAL_LOSS: 13.034593 VAL_MAE: 3.047027\n",
      "EPOCH: 69 LOSS: 11.881964 MAE: 2.815217 VAL_LOSS: 13.773944 VAL_MAE: 3.132936\n",
      "EPOCH: 70 LOSS: 12.205572 MAE: 2.908686 VAL_LOSS: 14.278180 VAL_MAE: 3.184871\n",
      "EPOCH: 71 LOSS: 12.681602 MAE: 2.950328 VAL_LOSS: 14.551003 VAL_MAE: 3.199262\n",
      "EPOCH: 72 LOSS: 13.520978 MAE: 3.034880 VAL_LOSS: 13.321976 VAL_MAE: 3.072354\n",
      "EPOCH: 73 LOSS: 12.069980 MAE: 2.891642 VAL_LOSS: 13.485882 VAL_MAE: 3.082656\n",
      "EPOCH: 74 LOSS: 12.852739 MAE: 2.907434 VAL_LOSS: 12.870503 VAL_MAE: 3.092801\n",
      "EPOCH: 75 LOSS: 11.986578 MAE: 2.857316 VAL_LOSS: 14.283819 VAL_MAE: 3.200420\n",
      "EPOCH: 76 LOSS: 10.355171 MAE: 2.621499 VAL_LOSS: 14.424683 VAL_MAE: 3.179720\n",
      "EPOCH: 77 LOSS: 12.265174 MAE: 2.867260 VAL_LOSS: 13.846645 VAL_MAE: 3.127548\n",
      "EPOCH: 78 LOSS: 11.457737 MAE: 2.807167 VAL_LOSS: 12.069273 VAL_MAE: 2.930476\n",
      "EPOCH: 79 LOSS: 12.281416 MAE: 2.857012 VAL_LOSS: 14.638810 VAL_MAE: 3.292513\n",
      "EPOCH: 80 LOSS: 11.496801 MAE: 2.782298 VAL_LOSS: 12.316447 VAL_MAE: 2.947512\n",
      "EPOCH: 81 LOSS: 11.576464 MAE: 2.754703 VAL_LOSS: 14.965679 VAL_MAE: 3.254074\n",
      "EPOCH: 82 LOSS: 11.223644 MAE: 2.786946 VAL_LOSS: 14.702117 VAL_MAE: 3.227142\n",
      "EPOCH: 83 LOSS: 12.440228 MAE: 2.922736 VAL_LOSS: 13.449884 VAL_MAE: 3.122167\n",
      "EPOCH: 84 LOSS: 10.460427 MAE: 2.646319 VAL_LOSS: 12.944829 VAL_MAE: 3.173572\n",
      "EPOCH: 85 LOSS: 12.579848 MAE: 2.957061 VAL_LOSS: 14.530382 VAL_MAE: 3.225154\n",
      "EPOCH: 86 LOSS: 12.720814 MAE: 2.944371 VAL_LOSS: 12.882120 VAL_MAE: 3.003950\n",
      "EPOCH: 87 LOSS: 12.446917 MAE: 2.898854 VAL_LOSS: 13.975954 VAL_MAE: 3.099520\n",
      "EPOCH: 88 LOSS: 12.342397 MAE: 2.889940 VAL_LOSS: 13.058809 VAL_MAE: 3.056418\n",
      "EPOCH: 89 LOSS: 11.983481 MAE: 2.831128 VAL_LOSS: 11.886765 VAL_MAE: 2.900478\n",
      "EPOCH: 90 LOSS: 13.123820 MAE: 3.004975 VAL_LOSS: 13.252506 VAL_MAE: 3.040027\n",
      "EPOCH: 91 LOSS: 12.273378 MAE: 2.891433 VAL_LOSS: 14.125203 VAL_MAE: 3.188815\n",
      "EPOCH: 92 LOSS: 12.107975 MAE: 2.848433 VAL_LOSS: 13.292478 VAL_MAE: 3.066412\n",
      "EPOCH: 93 LOSS: 12.596011 MAE: 2.914034 VAL_LOSS: 12.342807 VAL_MAE: 2.952436\n",
      "EPOCH: 94 LOSS: 11.837254 MAE: 2.838763 VAL_LOSS: 7.576010 VAL_MAE: 2.342764\n",
      "EPOCH: 95 LOSS: 12.046265 MAE: 2.855767 VAL_LOSS: 12.273136 VAL_MAE: 2.924773\n",
      "EPOCH: 96 LOSS: 13.296089 MAE: 3.008357 VAL_LOSS: 11.796947 VAL_MAE: 2.855725\n",
      "EPOCH: 97 LOSS: 13.651052 MAE: 3.082248 VAL_LOSS: 12.149509 VAL_MAE: 2.914545\n",
      "EPOCH: 98 LOSS: 12.212243 MAE: 2.909269 VAL_LOSS: 12.429493 VAL_MAE: 2.960349\n",
      "EPOCH: 99 LOSS: 13.039705 MAE: 3.008127 VAL_LOSS: 16.002054 VAL_MAE: 3.299370\n",
      "EPOCH: 100 LOSS: 12.968112 MAE: 2.972987 VAL_LOSS: 12.656549 VAL_MAE: 2.972430\n",
      "EPOCH: 101 LOSS: 11.143067 MAE: 2.737980 VAL_LOSS: 12.307158 VAL_MAE: 2.936075\n",
      "EPOCH: 102 LOSS: 11.952099 MAE: 2.829799 VAL_LOSS: 13.192858 VAL_MAE: 3.052009\n",
      "EPOCH: 103 LOSS: 11.166225 MAE: 2.784029 VAL_LOSS: 13.088578 VAL_MAE: 3.032926\n",
      "EPOCH: 104 LOSS: 10.514372 MAE: 2.683267 VAL_LOSS: 13.701047 VAL_MAE: 3.031110\n",
      "EPOCH: 105 LOSS: 10.975922 MAE: 2.679694 VAL_LOSS: 12.313602 VAL_MAE: 2.937998\n",
      "EPOCH: 106 LOSS: 12.941005 MAE: 2.985528 VAL_LOSS: 13.724218 VAL_MAE: 3.117103\n",
      "EPOCH: 107 LOSS: 11.023379 MAE: 2.695414 VAL_LOSS: 11.218204 VAL_MAE: 2.794557\n",
      "EPOCH: 108 LOSS: 11.351536 MAE: 2.751903 VAL_LOSS: 12.466436 VAL_MAE: 2.937085\n",
      "EPOCH: 109 LOSS: 11.260134 MAE: 2.741812 VAL_LOSS: 14.441776 VAL_MAE: 3.133920\n",
      "EPOCH: 110 LOSS: 10.031010 MAE: 2.590290 VAL_LOSS: 11.832805 VAL_MAE: 2.845008\n",
      "EPOCH: 111 LOSS: 11.848082 MAE: 2.810236 VAL_LOSS: 12.235019 VAL_MAE: 2.911607\n",
      "EPOCH: 112 LOSS: 10.547981 MAE: 2.672467 VAL_LOSS: 11.119411 VAL_MAE: 2.785664\n",
      "EPOCH: 113 LOSS: 11.444496 MAE: 2.771224 VAL_LOSS: 12.888577 VAL_MAE: 3.012128\n",
      "EPOCH: 114 LOSS: 11.274076 MAE: 2.744883 VAL_LOSS: 10.238563 VAL_MAE: 2.736490\n",
      "EPOCH: 115 LOSS: 9.556053 MAE: 2.467049 VAL_LOSS: 13.740625 VAL_MAE: 3.122609\n",
      "EPOCH: 116 LOSS: 12.240777 MAE: 2.870254 VAL_LOSS: 11.985575 VAL_MAE: 2.898399\n",
      "EPOCH: 117 LOSS: 12.247763 MAE: 2.905941 VAL_LOSS: 11.198613 VAL_MAE: 2.780245\n",
      "EPOCH: 118 LOSS: 11.670586 MAE: 2.821414 VAL_LOSS: 12.302315 VAL_MAE: 2.925543\n",
      "EPOCH: 119 LOSS: 10.312377 MAE: 2.578362 VAL_LOSS: 13.492484 VAL_MAE: 3.061898\n",
      "EPOCH: 120 LOSS: 12.093492 MAE: 2.827224 VAL_LOSS: 12.147011 VAL_MAE: 2.884429\n",
      "EPOCH: 121 LOSS: 10.009551 MAE: 2.528817 VAL_LOSS: 11.749210 VAL_MAE: 2.863271\n",
      "EPOCH: 122 LOSS: 10.227192 MAE: 2.562399 VAL_LOSS: 11.117941 VAL_MAE: 2.755149\n",
      "EPOCH: 123 LOSS: 10.233059 MAE: 2.541844 VAL_LOSS: 11.894471 VAL_MAE: 2.869272\n",
      "EPOCH: 124 LOSS: 12.252298 MAE: 2.878813 VAL_LOSS: 15.274857 VAL_MAE: 3.263801\n",
      "EPOCH: 125 LOSS: 10.109594 MAE: 2.600294 VAL_LOSS: 11.802849 VAL_MAE: 2.830120\n",
      "EPOCH: 126 LOSS: 10.820405 MAE: 2.678623 VAL_LOSS: 10.554085 VAL_MAE: 2.676939\n",
      "EPOCH: 127 LOSS: 10.925373 MAE: 2.676400 VAL_LOSS: 12.095031 VAL_MAE: 2.893888\n",
      "EPOCH: 128 LOSS: 10.802780 MAE: 2.644248 VAL_LOSS: 10.651548 VAL_MAE: 2.674673\n",
      "EPOCH: 129 LOSS: 10.652785 MAE: 2.644703 VAL_LOSS: 12.649226 VAL_MAE: 2.758593\n",
      "EPOCH: 130 LOSS: 10.478548 MAE: 2.638428 VAL_LOSS: 11.449223 VAL_MAE: 2.837655\n",
      "EPOCH: 131 LOSS: 9.814386 MAE: 2.438436 VAL_LOSS: 12.450949 VAL_MAE: 2.918283\n",
      "EPOCH: 132 LOSS: 12.501854 MAE: 2.887160 VAL_LOSS: 10.805214 VAL_MAE: 2.712235\n",
      "EPOCH: 133 LOSS: 11.005966 MAE: 2.723526 VAL_LOSS: 11.618521 VAL_MAE: 2.831991\n",
      "EPOCH: 134 LOSS: 9.736472 MAE: 2.524312 VAL_LOSS: 12.121445 VAL_MAE: 2.903011\n",
      "EPOCH: 135 LOSS: 12.464766 MAE: 2.898095 VAL_LOSS: 10.728882 VAL_MAE: 2.702205\n",
      "EPOCH: 136 LOSS: 11.852139 MAE: 2.814530 VAL_LOSS: 11.139364 VAL_MAE: 2.763133\n",
      "EPOCH: 137 LOSS: 10.719167 MAE: 2.623528 VAL_LOSS: 11.795904 VAL_MAE: 2.848940\n",
      "EPOCH: 138 LOSS: 9.605399 MAE: 2.493576 VAL_LOSS: 10.412835 VAL_MAE: 2.666981\n",
      "EPOCH: 139 LOSS: 10.145996 MAE: 2.526856 VAL_LOSS: 9.540684 VAL_MAE: 2.523788\n",
      "EPOCH: 140 LOSS: 9.085688 MAE: 2.374882 VAL_LOSS: 12.752687 VAL_MAE: 2.986532\n",
      "EPOCH: 141 LOSS: 11.495363 MAE: 2.740606 VAL_LOSS: 10.736053 VAL_MAE: 2.705342\n",
      "EPOCH: 142 LOSS: 9.484674 MAE: 2.467487 VAL_LOSS: 12.084476 VAL_MAE: 2.881404\n",
      "EPOCH: 143 LOSS: 9.031545 MAE: 2.392665 VAL_LOSS: 10.591557 VAL_MAE: 2.651292\n",
      "EPOCH: 144 LOSS: 10.085267 MAE: 2.575208 VAL_LOSS: 11.207479 VAL_MAE: 2.660511\n",
      "EPOCH: 145 LOSS: 11.366728 MAE: 2.754622 VAL_LOSS: 10.544199 VAL_MAE: 2.640633\n",
      "EPOCH: 146 LOSS: 10.999034 MAE: 2.722309 VAL_LOSS: 13.136371 VAL_MAE: 3.042726\n",
      "EPOCH: 147 LOSS: 10.775260 MAE: 2.658751 VAL_LOSS: 10.227396 VAL_MAE: 2.588326\n",
      "EPOCH: 148 LOSS: 11.125004 MAE: 2.721218 VAL_LOSS: 10.366518 VAL_MAE: 2.599992\n",
      "EPOCH: 149 LOSS: 10.260691 MAE: 2.567435 VAL_LOSS: 18.730562 VAL_MAE: 3.819972\n",
      "EPOCH: 150 LOSS: 10.988013 MAE: 2.663224 VAL_LOSS: 10.878611 VAL_MAE: 2.738093\n",
      "EPOCH: 151 LOSS: 10.470453 MAE: 2.598378 VAL_LOSS: 10.543926 VAL_MAE: 2.694546\n",
      "EPOCH: 152 LOSS: 9.440343 MAE: 2.423818 VAL_LOSS: 10.555313 VAL_MAE: 2.662527\n",
      "EPOCH: 153 LOSS: 10.386649 MAE: 2.623733 VAL_LOSS: 10.528069 VAL_MAE: 2.655043\n",
      "EPOCH: 154 LOSS: 10.667091 MAE: 2.616070 VAL_LOSS: 6.961203 VAL_MAE: 2.014888\n",
      "EPOCH: 155 LOSS: 10.522610 MAE: 2.626265 VAL_LOSS: 11.146841 VAL_MAE: 2.750506\n",
      "EPOCH: 156 LOSS: 9.666007 MAE: 2.501367 VAL_LOSS: 10.823862 VAL_MAE: 2.704552\n",
      "EPOCH: 157 LOSS: 9.688335 MAE: 2.461218 VAL_LOSS: 10.909626 VAL_MAE: 2.714906\n",
      "EPOCH: 158 LOSS: 9.952812 MAE: 2.582185 VAL_LOSS: 10.504213 VAL_MAE: 2.633126\n",
      "EPOCH: 159 LOSS: 8.881940 MAE: 2.354979 VAL_LOSS: 7.554303 VAL_MAE: 2.290204\n",
      "EPOCH: 160 LOSS: 9.708467 MAE: 2.433634 VAL_LOSS: 12.341716 VAL_MAE: 2.905139\n",
      "EPOCH: 161 LOSS: 10.811945 MAE: 2.668199 VAL_LOSS: 11.596009 VAL_MAE: 2.778361\n",
      "EPOCH: 162 LOSS: 9.918311 MAE: 2.539444 VAL_LOSS: 9.661287 VAL_MAE: 2.534713\n",
      "EPOCH: 163 LOSS: 9.947077 MAE: 2.501298 VAL_LOSS: 10.068284 VAL_MAE: 2.585597\n",
      "EPOCH: 164 LOSS: 9.520775 MAE: 2.448979 VAL_LOSS: 8.101663 VAL_MAE: 2.195474\n",
      "EPOCH: 165 LOSS: 9.005721 MAE: 2.391679 VAL_LOSS: 10.378395 VAL_MAE: 2.652231\n",
      "EPOCH: 166 LOSS: 11.512442 MAE: 2.753293 VAL_LOSS: 10.090516 VAL_MAE: 2.609905\n",
      "EPOCH: 167 LOSS: 8.340532 MAE: 2.223332 VAL_LOSS: 10.034552 VAL_MAE: 2.563566\n",
      "EPOCH: 168 LOSS: 8.873596 MAE: 2.348410 VAL_LOSS: 10.547439 VAL_MAE: 2.659147\n",
      "EPOCH: 169 LOSS: 12.084591 MAE: 2.835730 VAL_LOSS: 9.033671 VAL_MAE: 2.529927\n",
      "EPOCH: 170 LOSS: 8.804723 MAE: 2.364506 VAL_LOSS: 10.589787 VAL_MAE: 2.685916\n",
      "EPOCH: 171 LOSS: 9.820226 MAE: 2.464974 VAL_LOSS: 10.112494 VAL_MAE: 2.589880\n",
      "EPOCH: 172 LOSS: 9.877270 MAE: 2.508684 VAL_LOSS: 10.202725 VAL_MAE: 2.565285\n",
      "EPOCH: 173 LOSS: 10.799480 MAE: 2.679894 VAL_LOSS: 10.138012 VAL_MAE: 2.621667\n",
      "EPOCH: 174 LOSS: 9.097499 MAE: 2.419440 VAL_LOSS: 11.327682 VAL_MAE: 2.919173\n",
      "EPOCH: 175 LOSS: 9.384581 MAE: 2.387278 VAL_LOSS: 11.119010 VAL_MAE: 2.742962\n",
      "EPOCH: 176 LOSS: 9.352160 MAE: 2.407570 VAL_LOSS: 9.592518 VAL_MAE: 2.538838\n",
      "EPOCH: 177 LOSS: 9.574208 MAE: 2.398072 VAL_LOSS: 10.481712 VAL_MAE: 2.665636\n",
      "EPOCH: 178 LOSS: 11.152718 MAE: 2.661705 VAL_LOSS: 10.269648 VAL_MAE: 2.602185\n",
      "EPOCH: 179 LOSS: 10.448505 MAE: 2.595687 VAL_LOSS: 12.054214 VAL_MAE: 2.938322\n",
      "EPOCH: 180 LOSS: 9.587282 MAE: 2.485550 VAL_LOSS: 9.344046 VAL_MAE: 2.495896\n",
      "EPOCH: 181 LOSS: 9.719714 MAE: 2.467234 VAL_LOSS: 9.857049 VAL_MAE: 2.501110\n",
      "EPOCH: 182 LOSS: 10.479077 MAE: 2.615389 VAL_LOSS: 10.571589 VAL_MAE: 2.670549\n",
      "EPOCH: 183 LOSS: 10.793508 MAE: 2.652121 VAL_LOSS: 9.931243 VAL_MAE: 2.564455\n",
      "EPOCH: 184 LOSS: 9.299778 MAE: 2.360620 VAL_LOSS: 9.573867 VAL_MAE: 2.536571\n",
      "EPOCH: 185 LOSS: 10.665834 MAE: 2.607646 VAL_LOSS: 10.824659 VAL_MAE: 2.673403\n",
      "EPOCH: 186 LOSS: 9.544382 MAE: 2.438213 VAL_LOSS: 9.845699 VAL_MAE: 2.537638\n",
      "EPOCH: 187 LOSS: 9.252686 MAE: 2.398724 VAL_LOSS: 9.008834 VAL_MAE: 2.402427\n",
      "EPOCH: 188 LOSS: 8.402849 MAE: 2.282132 VAL_LOSS: 10.788012 VAL_MAE: 2.687815\n",
      "EPOCH: 189 LOSS: 9.652966 MAE: 2.464428 VAL_LOSS: 8.498916 VAL_MAE: 2.254267\n",
      "EPOCH: 190 LOSS: 10.327595 MAE: 2.571948 VAL_LOSS: 9.188801 VAL_MAE: 2.423384\n",
      "EPOCH: 191 LOSS: 9.377810 MAE: 2.419702 VAL_LOSS: 10.467249 VAL_MAE: 2.671084\n",
      "EPOCH: 192 LOSS: 9.004575 MAE: 2.327710 VAL_LOSS: 9.266531 VAL_MAE: 2.469244\n",
      "EPOCH: 193 LOSS: 9.260848 MAE: 2.389094 VAL_LOSS: 11.590601 VAL_MAE: 2.817073\n",
      "EPOCH: 194 LOSS: 9.696794 MAE: 2.509426 VAL_LOSS: 14.644188 VAL_MAE: 3.258734\n",
      "EPOCH: 195 LOSS: 8.797756 MAE: 2.301541 VAL_LOSS: 9.863961 VAL_MAE: 2.568944\n",
      "EPOCH: 196 LOSS: 10.070978 MAE: 2.552010 VAL_LOSS: 9.437208 VAL_MAE: 2.437316\n",
      "EPOCH: 197 LOSS: 8.991045 MAE: 2.355211 VAL_LOSS: 9.016058 VAL_MAE: 2.397282\n",
      "EPOCH: 198 LOSS: 9.534506 MAE: 2.481796 VAL_LOSS: 9.832634 VAL_MAE: 2.568266\n",
      "EPOCH: 199 LOSS: 9.574236 MAE: 2.469375 VAL_LOSS: 6.975178 VAL_MAE: 2.070694\n",
      "EPOCH: 200 LOSS: 9.268963 MAE: 2.360205 VAL_LOSS: 10.318832 VAL_MAE: 2.642943\n",
      "EPOCH: 201 LOSS: 8.496101 MAE: 2.269902 VAL_LOSS: 9.534836 VAL_MAE: 2.519650\n",
      "EPOCH: 202 LOSS: 9.355701 MAE: 2.427107 VAL_LOSS: 10.563944 VAL_MAE: 2.649626\n",
      "EPOCH: 203 LOSS: 8.328323 MAE: 2.254776 VAL_LOSS: 9.941879 VAL_MAE: 2.542746\n",
      "EPOCH: 204 LOSS: 9.153339 MAE: 2.354634 VAL_LOSS: 12.289387 VAL_MAE: 2.856829\n",
      "EPOCH: 205 LOSS: 9.935045 MAE: 2.531365 VAL_LOSS: 8.863656 VAL_MAE: 2.366662\n",
      "EPOCH: 206 LOSS: 8.673911 MAE: 2.277653 VAL_LOSS: 9.293681 VAL_MAE: 2.461207\n",
      "EPOCH: 207 LOSS: 9.205452 MAE: 2.386372 VAL_LOSS: 9.978354 VAL_MAE: 2.558465\n",
      "EPOCH: 208 LOSS: 8.638133 MAE: 2.289954 VAL_LOSS: 8.952244 VAL_MAE: 2.356305\n",
      "EPOCH: 209 LOSS: 10.081467 MAE: 2.538616 VAL_LOSS: 13.250337 VAL_MAE: 3.036088\n",
      "EPOCH: 210 LOSS: 9.811771 MAE: 2.526815 VAL_LOSS: 9.961931 VAL_MAE: 2.538785\n",
      "EPOCH: 211 LOSS: 9.218935 MAE: 2.398841 VAL_LOSS: 9.498074 VAL_MAE: 2.480733\n",
      "EPOCH: 212 LOSS: 8.259655 MAE: 2.237653 VAL_LOSS: 9.083475 VAL_MAE: 2.403895\n",
      "EPOCH: 213 LOSS: 8.523447 MAE: 2.311882 VAL_LOSS: 10.619043 VAL_MAE: 2.684153\n",
      "EPOCH: 214 LOSS: 10.153029 MAE: 2.584400 VAL_LOSS: 5.050013 VAL_MAE: 1.716408\n",
      "EPOCH: 215 LOSS: 7.747709 MAE: 2.160689 VAL_LOSS: 9.388227 VAL_MAE: 2.489091\n",
      "EPOCH: 216 LOSS: 9.065426 MAE: 2.392015 VAL_LOSS: 8.224917 VAL_MAE: 2.251201\n",
      "EPOCH: 217 LOSS: 9.392337 MAE: 2.427464 VAL_LOSS: 9.067261 VAL_MAE: 2.402474\n",
      "EPOCH: 218 LOSS: 8.331303 MAE: 2.260747 VAL_LOSS: 9.651001 VAL_MAE: 2.536598\n",
      "EPOCH: 219 LOSS: 10.109450 MAE: 2.546418 VAL_LOSS: 10.340500 VAL_MAE: 2.674647\n",
      "EPOCH: 220 LOSS: 9.108438 MAE: 2.393257 VAL_LOSS: 9.734908 VAL_MAE: 2.544994\n",
      "EPOCH: 221 LOSS: 8.876490 MAE: 2.343922 VAL_LOSS: 10.828892 VAL_MAE: 2.699412\n",
      "EPOCH: 222 LOSS: 9.886422 MAE: 2.527513 VAL_LOSS: 9.532741 VAL_MAE: 2.468493\n",
      "EPOCH: 223 LOSS: 8.164822 MAE: 2.216827 VAL_LOSS: 10.719338 VAL_MAE: 2.678286\n",
      "EPOCH: 224 LOSS: 8.156988 MAE: 2.214300 VAL_LOSS: 9.371343 VAL_MAE: 2.452466\n",
      "EPOCH: 225 LOSS: 9.454628 MAE: 2.461699 VAL_LOSS: 9.738493 VAL_MAE: 2.494510\n",
      "EPOCH: 226 LOSS: 8.506388 MAE: 2.245971 VAL_LOSS: 10.209462 VAL_MAE: 2.539145\n",
      "EPOCH: 227 LOSS: 9.625016 MAE: 2.485507 VAL_LOSS: 9.809649 VAL_MAE: 2.485181\n",
      "EPOCH: 228 LOSS: 9.130877 MAE: 2.336580 VAL_LOSS: 10.003057 VAL_MAE: 2.557590\n",
      "EPOCH: 229 LOSS: 8.758547 MAE: 2.294528 VAL_LOSS: 8.146741 VAL_MAE: 2.209232\n",
      "EPOCH: 230 LOSS: 8.640336 MAE: 2.289523 VAL_LOSS: 10.311333 VAL_MAE: 2.596626\n",
      "EPOCH: 231 LOSS: 8.706099 MAE: 2.279417 VAL_LOSS: 9.934221 VAL_MAE: 2.518516\n",
      "EPOCH: 232 LOSS: 9.396134 MAE: 2.409718 VAL_LOSS: 9.457514 VAL_MAE: 2.467315\n",
      "EPOCH: 233 LOSS: 10.332064 MAE: 2.619430 VAL_LOSS: 9.718111 VAL_MAE: 2.513958\n",
      "EPOCH: 234 LOSS: 9.295082 MAE: 2.378540 VAL_LOSS: 9.620056 VAL_MAE: 2.461046\n",
      "EPOCH: 235 LOSS: 8.993289 MAE: 2.377011 VAL_LOSS: 8.485549 VAL_MAE: 2.297822\n",
      "EPOCH: 236 LOSS: 8.364469 MAE: 2.222599 VAL_LOSS: 8.957452 VAL_MAE: 2.360580\n",
      "EPOCH: 237 LOSS: 8.666271 MAE: 2.242670 VAL_LOSS: 9.232211 VAL_MAE: 2.407632\n",
      "EPOCH: 238 LOSS: 8.819399 MAE: 2.329672 VAL_LOSS: 8.982488 VAL_MAE: 2.388120\n",
      "EPOCH: 239 LOSS: 8.958237 MAE: 2.380296 VAL_LOSS: 10.292949 VAL_MAE: 2.785978\n",
      "EPOCH: 240 LOSS: 8.676265 MAE: 2.296515 VAL_LOSS: 9.177515 VAL_MAE: 2.428862\n",
      "EPOCH: 241 LOSS: 8.470989 MAE: 2.267773 VAL_LOSS: 9.383995 VAL_MAE: 2.444736\n",
      "EPOCH: 242 LOSS: 9.473295 MAE: 2.428619 VAL_LOSS: 9.009661 VAL_MAE: 2.361846\n",
      "EPOCH: 243 LOSS: 7.603625 MAE: 2.139423 VAL_LOSS: 9.864915 VAL_MAE: 2.568646\n",
      "EPOCH: 244 LOSS: 10.349680 MAE: 2.618900 VAL_LOSS: 10.368310 VAL_MAE: 2.667931\n",
      "EPOCH: 245 LOSS: 9.002519 MAE: 2.365063 VAL_LOSS: 9.696710 VAL_MAE: 2.493701\n",
      "EPOCH: 246 LOSS: 7.781844 MAE: 2.161736 VAL_LOSS: 9.211618 VAL_MAE: 2.356925\n",
      "EPOCH: 247 LOSS: 7.967770 MAE: 2.134915 VAL_LOSS: 8.632818 VAL_MAE: 2.313580\n",
      "EPOCH: 248 LOSS: 9.491343 MAE: 2.490001 VAL_LOSS: 8.451506 VAL_MAE: 2.284758\n",
      "EPOCH: 249 LOSS: 9.258766 MAE: 2.378889 VAL_LOSS: 8.483640 VAL_MAE: 2.284243\n",
      "EPOCH: 250 LOSS: 10.136942 MAE: 2.552291 VAL_LOSS: 8.821482 VAL_MAE: 2.337569\n",
      "EPOCH: 251 LOSS: 9.686913 MAE: 2.454987 VAL_LOSS: 9.199374 VAL_MAE: 2.382504\n",
      "EPOCH: 252 LOSS: 8.778086 MAE: 2.289506 VAL_LOSS: 8.549790 VAL_MAE: 2.291033\n",
      "EPOCH: 253 LOSS: 8.474062 MAE: 2.262242 VAL_LOSS: 9.550430 VAL_MAE: 2.511599\n",
      "EPOCH: 254 LOSS: 8.956699 MAE: 2.356451 VAL_LOSS: 7.189213 VAL_MAE: 2.086872\n",
      "EPOCH: 255 LOSS: 9.502669 MAE: 2.436393 VAL_LOSS: 8.783469 VAL_MAE: 2.344085\n",
      "EPOCH: 256 LOSS: 9.559135 MAE: 2.490524 VAL_LOSS: 8.798208 VAL_MAE: 2.388741\n",
      "EPOCH: 257 LOSS: 8.484292 MAE: 2.268164 VAL_LOSS: 9.604708 VAL_MAE: 2.497922\n",
      "EPOCH: 258 LOSS: 9.037354 MAE: 2.333114 VAL_LOSS: 9.118524 VAL_MAE: 2.384532\n",
      "EPOCH: 259 LOSS: 8.451126 MAE: 2.242352 VAL_LOSS: 10.089843 VAL_MAE: 2.556102\n",
      "EPOCH: 260 LOSS: 9.161860 MAE: 2.365718 VAL_LOSS: 8.907904 VAL_MAE: 2.373539\n",
      "EPOCH: 261 LOSS: 8.963434 MAE: 2.400433 VAL_LOSS: 7.878016 VAL_MAE: 2.188891\n",
      "EPOCH: 262 LOSS: 9.452265 MAE: 2.445467 VAL_LOSS: 8.871307 VAL_MAE: 2.355484\n",
      "EPOCH: 263 LOSS: 8.091788 MAE: 2.207388 VAL_LOSS: 7.846117 VAL_MAE: 2.166152\n",
      "EPOCH: 264 LOSS: 8.013398 MAE: 2.164068 VAL_LOSS: 7.581495 VAL_MAE: 2.135622\n",
      "EPOCH: 265 LOSS: 7.406946 MAE: 2.021681 VAL_LOSS: 9.870808 VAL_MAE: 2.551020\n",
      "EPOCH: 266 LOSS: 7.964062 MAE: 2.159645 VAL_LOSS: 9.389914 VAL_MAE: 2.458999\n",
      "EPOCH: 267 LOSS: 8.632583 MAE: 2.270331 VAL_LOSS: 8.612993 VAL_MAE: 2.270522\n",
      "EPOCH: 268 LOSS: 7.171191 MAE: 1.991982 VAL_LOSS: 9.121929 VAL_MAE: 2.407478\n",
      "EPOCH: 269 LOSS: 9.549117 MAE: 2.405013 VAL_LOSS: 5.422668 VAL_MAE: 1.726538\n",
      "EPOCH: 270 LOSS: 8.911764 MAE: 2.386522 VAL_LOSS: 9.569173 VAL_MAE: 2.481209\n",
      "EPOCH: 271 LOSS: 9.056455 MAE: 2.348195 VAL_LOSS: 9.638292 VAL_MAE: 2.460280\n",
      "EPOCH: 272 LOSS: 7.609318 MAE: 2.085829 VAL_LOSS: 8.642191 VAL_MAE: 2.291948\n",
      "EPOCH: 273 LOSS: 8.073040 MAE: 2.195023 VAL_LOSS: 10.402843 VAL_MAE: 2.638141\n",
      "EPOCH: 274 LOSS: 8.160073 MAE: 2.193051 VAL_LOSS: 8.328358 VAL_MAE: 2.216328\n",
      "EPOCH: 275 LOSS: 8.721535 MAE: 2.289028 VAL_LOSS: 9.801466 VAL_MAE: 2.512492\n",
      "EPOCH: 276 LOSS: 7.590306 MAE: 2.091220 VAL_LOSS: 8.757925 VAL_MAE: 2.351697\n",
      "EPOCH: 277 LOSS: 8.577155 MAE: 2.267361 VAL_LOSS: 9.335335 VAL_MAE: 2.471720\n",
      "EPOCH: 278 LOSS: 8.763446 MAE: 2.321224 VAL_LOSS: 8.234631 VAL_MAE: 2.261532\n",
      "EPOCH: 279 LOSS: 8.257104 MAE: 2.247488 VAL_LOSS: 7.675805 VAL_MAE: 2.225187\n",
      "EPOCH: 280 LOSS: 9.829906 MAE: 2.512402 VAL_LOSS: 9.052435 VAL_MAE: 2.413559\n",
      "EPOCH: 281 LOSS: 8.348629 MAE: 2.239488 VAL_LOSS: 8.565470 VAL_MAE: 2.322342\n",
      "EPOCH: 282 LOSS: 7.753305 MAE: 2.163091 VAL_LOSS: 8.981735 VAL_MAE: 2.367451\n",
      "EPOCH: 283 LOSS: 9.251558 MAE: 2.393932 VAL_LOSS: 9.751648 VAL_MAE: 2.498213\n",
      "EPOCH: 284 LOSS: 8.003326 MAE: 2.128866 VAL_LOSS: 8.000626 VAL_MAE: 2.138123\n",
      "EPOCH: 285 LOSS: 7.552588 MAE: 2.061610 VAL_LOSS: 9.172410 VAL_MAE: 2.417318\n",
      "EPOCH: 286 LOSS: 8.665225 MAE: 2.262778 VAL_LOSS: 10.390517 VAL_MAE: 2.636552\n",
      "EPOCH: 287 LOSS: 8.623186 MAE: 2.215984 VAL_LOSS: 8.173176 VAL_MAE: 2.224109\n",
      "EPOCH: 288 LOSS: 7.942869 MAE: 2.136572 VAL_LOSS: 9.020828 VAL_MAE: 2.382187\n",
      "EPOCH: 289 LOSS: 7.463739 MAE: 2.056588 VAL_LOSS: 11.804423 VAL_MAE: 2.810436\n",
      "EPOCH: 290 LOSS: 7.889266 MAE: 2.153022 VAL_LOSS: 9.655982 VAL_MAE: 2.514900\n",
      "EPOCH: 291 LOSS: 8.219944 MAE: 2.216873 VAL_LOSS: 8.219893 VAL_MAE: 2.236653\n",
      "EPOCH: 292 LOSS: 8.144308 MAE: 2.213387 VAL_LOSS: 8.549211 VAL_MAE: 2.279897\n",
      "EPOCH: 293 LOSS: 8.171046 MAE: 2.141434 VAL_LOSS: 8.145045 VAL_MAE: 2.237896\n",
      "EPOCH: 294 LOSS: 7.930029 MAE: 2.188706 VAL_LOSS: 5.706454 VAL_MAE: 1.778827\n",
      "EPOCH: 295 LOSS: 10.197870 MAE: 2.591020 VAL_LOSS: 8.442599 VAL_MAE: 2.289880\n",
      "EPOCH: 296 LOSS: 8.218634 MAE: 2.209330 VAL_LOSS: 8.761155 VAL_MAE: 2.336130\n",
      "EPOCH: 297 LOSS: 7.722974 MAE: 2.108989 VAL_LOSS: 10.093552 VAL_MAE: 2.581650\n",
      "EPOCH: 298 LOSS: 8.000062 MAE: 2.119068 VAL_LOSS: 8.605748 VAL_MAE: 2.287548\n",
      "EPOCH: 299 LOSS: 7.487695 MAE: 2.075630 VAL_LOSS: 5.450758 VAL_MAE: 1.733642\n",
      "EPOCH: 300 LOSS: 8.677827 MAE: 2.314072 VAL_LOSS: 8.406975 VAL_MAE: 2.253344\n",
      "EPOCH: 301 LOSS: 7.827734 MAE: 2.146210 VAL_LOSS: 7.847592 VAL_MAE: 2.169964\n",
      "EPOCH: 302 LOSS: 8.394583 MAE: 2.248816 VAL_LOSS: 8.778654 VAL_MAE: 2.353467\n",
      "EPOCH: 303 LOSS: 8.024410 MAE: 2.227139 VAL_LOSS: 8.097329 VAL_MAE: 2.216227\n",
      "EPOCH: 304 LOSS: 7.887528 MAE: 2.122388 VAL_LOSS: 10.092088 VAL_MAE: 2.603373\n",
      "EPOCH: 305 LOSS: 8.303288 MAE: 2.201871 VAL_LOSS: 7.755683 VAL_MAE: 2.174649\n",
      "EPOCH: 306 LOSS: 8.704037 MAE: 2.340956 VAL_LOSS: 8.176407 VAL_MAE: 2.210803\n",
      "EPOCH: 307 LOSS: 7.586835 MAE: 2.059738 VAL_LOSS: 9.245173 VAL_MAE: 2.406748\n",
      "EPOCH: 308 LOSS: 10.353010 MAE: 2.638222 VAL_LOSS: 8.574925 VAL_MAE: 2.291531\n",
      "EPOCH: 309 LOSS: 10.779126 MAE: 2.680208 VAL_LOSS: 5.803045 VAL_MAE: 1.817591\n",
      "EPOCH: 310 LOSS: 9.033055 MAE: 2.372210 VAL_LOSS: 8.348840 VAL_MAE: 2.260399\n",
      "EPOCH: 311 LOSS: 8.280393 MAE: 2.224241 VAL_LOSS: 8.489069 VAL_MAE: 2.292717\n",
      "EPOCH: 312 LOSS: 8.206727 MAE: 2.241346 VAL_LOSS: 9.082355 VAL_MAE: 2.388415\n",
      "EPOCH: 313 LOSS: 8.025991 MAE: 2.172369 VAL_LOSS: 9.330614 VAL_MAE: 2.433183\n",
      "EPOCH: 314 LOSS: 8.261338 MAE: 2.248616 VAL_LOSS: 4.580986 VAL_MAE: 1.548434\n",
      "EPOCH: 315 LOSS: 8.070188 MAE: 2.130522 VAL_LOSS: 7.906521 VAL_MAE: 2.222886\n",
      "EPOCH: 316 LOSS: 8.473441 MAE: 2.230503 VAL_LOSS: 9.035103 VAL_MAE: 2.400250\n",
      "EPOCH: 317 LOSS: 7.876581 MAE: 2.125242 VAL_LOSS: 8.544180 VAL_MAE: 2.263821\n",
      "EPOCH: 318 LOSS: 7.103355 MAE: 2.015022 VAL_LOSS: 8.749083 VAL_MAE: 2.323755\n",
      "EPOCH: 319 LOSS: 8.511743 MAE: 2.281092 VAL_LOSS: 9.327469 VAL_MAE: 2.478996\n",
      "EPOCH: 320 LOSS: 7.129567 MAE: 2.034290 VAL_LOSS: 8.800261 VAL_MAE: 2.355405\n",
      "EPOCH: 321 LOSS: 8.927906 MAE: 2.332947 VAL_LOSS: 8.523408 VAL_MAE: 2.288902\n",
      "EPOCH: 322 LOSS: 8.801407 MAE: 2.311427 VAL_LOSS: 8.171636 VAL_MAE: 2.218295\n",
      "EPOCH: 323 LOSS: 6.972497 MAE: 1.994126 VAL_LOSS: 7.942480 VAL_MAE: 2.165999\n",
      "EPOCH: 324 LOSS: 8.024508 MAE: 2.161378 VAL_LOSS: 9.747798 VAL_MAE: 2.509787\n",
      "EPOCH: 325 LOSS: 7.291471 MAE: 2.019278 VAL_LOSS: 9.229095 VAL_MAE: 2.442804\n",
      "EPOCH: 326 LOSS: 8.268847 MAE: 2.231089 VAL_LOSS: 9.226517 VAL_MAE: 2.376870\n",
      "EPOCH: 327 LOSS: 7.569611 MAE: 2.080709 VAL_LOSS: 8.049432 VAL_MAE: 2.201831\n",
      "EPOCH: 328 LOSS: 7.638979 MAE: 2.108445 VAL_LOSS: 8.045319 VAL_MAE: 2.192732\n",
      "EPOCH: 329 LOSS: 7.171586 MAE: 2.028254 VAL_LOSS: 6.752550 VAL_MAE: 2.035674\n",
      "EPOCH: 330 LOSS: 8.042004 MAE: 2.201531 VAL_LOSS: 7.591205 VAL_MAE: 2.147193\n",
      "EPOCH: 331 LOSS: 7.435202 MAE: 2.044889 VAL_LOSS: 8.378779 VAL_MAE: 2.296070\n",
      "EPOCH: 332 LOSS: 8.341272 MAE: 2.247851 VAL_LOSS: 8.768331 VAL_MAE: 2.295959\n",
      "EPOCH: 333 LOSS: 7.216094 MAE: 2.015403 VAL_LOSS: 8.167366 VAL_MAE: 2.231047\n",
      "EPOCH: 334 LOSS: 7.541371 MAE: 2.086944 VAL_LOSS: 6.086979 VAL_MAE: 1.869478\n",
      "EPOCH: 335 LOSS: 9.583053 MAE: 2.448954 VAL_LOSS: 7.901418 VAL_MAE: 2.187853\n",
      "EPOCH: 336 LOSS: 7.356527 MAE: 2.057232 VAL_LOSS: 8.420205 VAL_MAE: 2.290418\n",
      "EPOCH: 337 LOSS: 7.221228 MAE: 2.047330 VAL_LOSS: 8.026714 VAL_MAE: 2.182758\n",
      "EPOCH: 338 LOSS: 7.414272 MAE: 2.049279 VAL_LOSS: 8.955308 VAL_MAE: 2.328504\n",
      "EPOCH: 339 LOSS: 8.125695 MAE: 2.175234 VAL_LOSS: 7.086086 VAL_MAE: 2.092834\n",
      "EPOCH: 340 LOSS: 8.057081 MAE: 2.158449 VAL_LOSS: 8.372260 VAL_MAE: 2.232545\n",
      "EPOCH: 341 LOSS: 9.677340 MAE: 2.505898 VAL_LOSS: 7.948651 VAL_MAE: 2.208032\n",
      "EPOCH: 342 LOSS: 8.373856 MAE: 2.217898 VAL_LOSS: 8.962704 VAL_MAE: 2.347288\n",
      "EPOCH: 343 LOSS: 8.179172 MAE: 2.217648 VAL_LOSS: 8.150693 VAL_MAE: 2.232709\n",
      "EPOCH: 344 LOSS: 6.885844 MAE: 1.948633 VAL_LOSS: 7.588469 VAL_MAE: 2.211746\n",
      "EPOCH: 345 LOSS: 8.413086 MAE: 2.205880 VAL_LOSS: 8.679022 VAL_MAE: 2.352991\n",
      "EPOCH: 346 LOSS: 8.110417 MAE: 2.201176 VAL_LOSS: 7.808495 VAL_MAE: 2.177957\n",
      "EPOCH: 347 LOSS: 7.586483 MAE: 2.072872 VAL_LOSS: 8.040923 VAL_MAE: 2.174548\n",
      "EPOCH: 348 LOSS: 7.477518 MAE: 2.083778 VAL_LOSS: 8.438462 VAL_MAE: 2.263489\n",
      "EPOCH: 349 LOSS: 7.630223 MAE: 2.084299 VAL_LOSS: 10.384555 VAL_MAE: 2.492234\n",
      "EPOCH: 350 LOSS: 7.940924 MAE: 2.157928 VAL_LOSS: 8.041868 VAL_MAE: 2.206287\n",
      "EPOCH: 351 LOSS: 8.692087 MAE: 2.283665 VAL_LOSS: 8.348018 VAL_MAE: 2.250338\n",
      "EPOCH: 352 LOSS: 8.394783 MAE: 2.259639 VAL_LOSS: 9.192697 VAL_MAE: 2.383312\n",
      "EPOCH: 353 LOSS: 8.147171 MAE: 2.186231 VAL_LOSS: 8.583181 VAL_MAE: 2.291724\n",
      "EPOCH: 354 LOSS: 7.976398 MAE: 2.159672 VAL_LOSS: 8.802386 VAL_MAE: 2.382435\n",
      "EPOCH: 355 LOSS: 7.051779 MAE: 1.996527 VAL_LOSS: 8.654154 VAL_MAE: 2.284827\n",
      "EPOCH: 356 LOSS: 7.881898 MAE: 2.149575 VAL_LOSS: 8.635986 VAL_MAE: 2.298733\n",
      "EPOCH: 357 LOSS: 7.523874 MAE: 2.055532 VAL_LOSS: 8.512126 VAL_MAE: 2.298469\n",
      "EPOCH: 358 LOSS: 8.668419 MAE: 2.306020 VAL_LOSS: 8.409699 VAL_MAE: 2.272871\n",
      "EPOCH: 359 LOSS: 7.765612 MAE: 2.085669 VAL_LOSS: 9.245860 VAL_MAE: 2.355902\n",
      "EPOCH: 360 LOSS: 7.208143 MAE: 1.980581 VAL_LOSS: 7.666085 VAL_MAE: 2.147008\n",
      "EPOCH: 361 LOSS: 7.840154 MAE: 2.150307 VAL_LOSS: 8.358806 VAL_MAE: 2.254879\n",
      "EPOCH: 362 LOSS: 8.913380 MAE: 2.360655 VAL_LOSS: 8.668155 VAL_MAE: 2.305081\n",
      "EPOCH: 363 LOSS: 9.021229 MAE: 2.363924 VAL_LOSS: 8.708439 VAL_MAE: 2.297755\n",
      "EPOCH: 364 LOSS: 8.311347 MAE: 2.217055 VAL_LOSS: 8.213058 VAL_MAE: 2.196840\n",
      "EPOCH: 365 LOSS: 7.979842 MAE: 2.150587 VAL_LOSS: 8.104499 VAL_MAE: 2.200646\n",
      "EPOCH: 366 LOSS: 7.031647 MAE: 1.960410 VAL_LOSS: 7.437257 VAL_MAE: 2.126687\n",
      "EPOCH: 367 LOSS: 7.965640 MAE: 2.147237 VAL_LOSS: 7.650290 VAL_MAE: 2.122403\n",
      "EPOCH: 368 LOSS: 8.450699 MAE: 2.305679 VAL_LOSS: 8.136911 VAL_MAE: 2.228556\n",
      "EPOCH: 369 LOSS: 8.856952 MAE: 2.313534 VAL_LOSS: 7.792339 VAL_MAE: 2.104823\n",
      "EPOCH: 370 LOSS: 7.647138 MAE: 2.041679 VAL_LOSS: 8.380403 VAL_MAE: 2.250961\n",
      "EPOCH: 371 LOSS: 7.699879 MAE: 2.161232 VAL_LOSS: 6.713848 VAL_MAE: 1.962964\n",
      "EPOCH: 372 LOSS: 7.753238 MAE: 2.129483 VAL_LOSS: 8.307646 VAL_MAE: 2.287911\n",
      "EPOCH: 373 LOSS: 8.458933 MAE: 2.244516 VAL_LOSS: 8.787464 VAL_MAE: 2.344431\n",
      "EPOCH: 374 LOSS: 8.192675 MAE: 2.206402 VAL_LOSS: 8.128074 VAL_MAE: 2.217657\n",
      "EPOCH: 375 LOSS: 7.539141 MAE: 2.063030 VAL_LOSS: 8.058990 VAL_MAE: 2.221019\n",
      "EPOCH: 376 LOSS: 7.141434 MAE: 1.975128 VAL_LOSS: 8.254781 VAL_MAE: 2.239163\n",
      "EPOCH: 377 LOSS: 7.667125 MAE: 2.131894 VAL_LOSS: 8.688206 VAL_MAE: 2.339995\n",
      "EPOCH: 378 LOSS: 7.769265 MAE: 2.129887 VAL_LOSS: 8.349995 VAL_MAE: 2.276672\n",
      "EPOCH: 379 LOSS: 7.206146 MAE: 2.001245 VAL_LOSS: 6.528111 VAL_MAE: 1.949993\n",
      "EPOCH: 380 LOSS: 6.967085 MAE: 1.953319 VAL_LOSS: 7.551506 VAL_MAE: 2.107179\n",
      "EPOCH: 381 LOSS: 6.798834 MAE: 1.911523 VAL_LOSS: 7.769547 VAL_MAE: 2.156779\n",
      "EPOCH: 382 LOSS: 7.771420 MAE: 2.136803 VAL_LOSS: 8.111979 VAL_MAE: 2.266477\n",
      "EPOCH: 383 LOSS: 7.438738 MAE: 2.041283 VAL_LOSS: 8.181828 VAL_MAE: 2.220377\n",
      "EPOCH: 384 LOSS: 7.224366 MAE: 2.017103 VAL_LOSS: 9.759689 VAL_MAE: 2.478745\n",
      "EPOCH: 385 LOSS: 7.220147 MAE: 2.002739 VAL_LOSS: 9.244252 VAL_MAE: 2.419907\n",
      "EPOCH: 386 LOSS: 8.232047 MAE: 2.225886 VAL_LOSS: 7.236406 VAL_MAE: 2.058315\n",
      "EPOCH: 387 LOSS: 8.616766 MAE: 2.289893 VAL_LOSS: 7.767341 VAL_MAE: 2.151057\n",
      "EPOCH: 388 LOSS: 8.183999 MAE: 2.196506 VAL_LOSS: 7.013070 VAL_MAE: 2.039797\n",
      "EPOCH: 389 LOSS: 8.891649 MAE: 2.357656 VAL_LOSS: 6.646111 VAL_MAE: 1.924772\n",
      "EPOCH: 390 LOSS: 8.420837 MAE: 2.231024 VAL_LOSS: 8.289860 VAL_MAE: 2.245975\n",
      "EPOCH: 391 LOSS: 7.617195 MAE: 2.075871 VAL_LOSS: 8.281110 VAL_MAE: 2.239278\n",
      "EPOCH: 392 LOSS: 7.750222 MAE: 2.134343 VAL_LOSS: 7.895843 VAL_MAE: 2.179595\n",
      "EPOCH: 393 LOSS: 6.935133 MAE: 1.939502 VAL_LOSS: 8.145279 VAL_MAE: 2.204701\n",
      "EPOCH: 394 LOSS: 8.904605 MAE: 2.335276 VAL_LOSS: 9.519891 VAL_MAE: 2.446833\n",
      "EPOCH: 395 LOSS: 6.982655 MAE: 1.889183 VAL_LOSS: 7.796803 VAL_MAE: 2.178618\n",
      "EPOCH: 396 LOSS: 8.211504 MAE: 2.208800 VAL_LOSS: 7.239685 VAL_MAE: 2.061716\n",
      "EPOCH: 397 LOSS: 8.287162 MAE: 2.201062 VAL_LOSS: 7.940718 VAL_MAE: 2.186825\n",
      "EPOCH: 398 LOSS: 7.487103 MAE: 2.046769 VAL_LOSS: 7.697400 VAL_MAE: 2.156184\n",
      "EPOCH: 399 LOSS: 7.585731 MAE: 2.114956 VAL_LOSS: 6.356425 VAL_MAE: 1.895057\n",
      "EPOCH: 400 LOSS: 7.694447 MAE: 2.105524 VAL_LOSS: 7.566587 VAL_MAE: 2.144079\n",
      "EPOCH: 401 LOSS: 8.604508 MAE: 2.310682 VAL_LOSS: 8.149815 VAL_MAE: 2.225346\n",
      "EPOCH: 402 LOSS: 7.321620 MAE: 2.035953 VAL_LOSS: 8.732116 VAL_MAE: 2.344995\n",
      "EPOCH: 403 LOSS: 7.017596 MAE: 1.980970 VAL_LOSS: 8.449246 VAL_MAE: 2.313426\n",
      "EPOCH: 404 LOSS: 7.736110 MAE: 2.114670 VAL_LOSS: 7.683897 VAL_MAE: 2.155761\n",
      "EPOCH: 405 LOSS: 7.932064 MAE: 2.165070 VAL_LOSS: 8.207225 VAL_MAE: 2.236683\n",
      "EPOCH: 406 LOSS: 7.266282 MAE: 1.999120 VAL_LOSS: 8.811316 VAL_MAE: 2.349994\n",
      "EPOCH: 407 LOSS: 6.071393 MAE: 1.849532 VAL_LOSS: 7.719651 VAL_MAE: 2.121599\n",
      "EPOCH: 408 LOSS: 7.683370 MAE: 2.088174 VAL_LOSS: 8.000006 VAL_MAE: 2.223394\n",
      "EPOCH: 409 LOSS: 7.891032 MAE: 2.170326 VAL_LOSS: 6.067552 VAL_MAE: 1.879995\n",
      "EPOCH: 410 LOSS: 7.102252 MAE: 1.959097 VAL_LOSS: 6.653168 VAL_MAE: 1.967087\n",
      "EPOCH: 411 LOSS: 7.322443 MAE: 2.032857 VAL_LOSS: 8.577083 VAL_MAE: 2.322979\n",
      "EPOCH: 412 LOSS: 7.670084 MAE: 2.134680 VAL_LOSS: 7.554122 VAL_MAE: 2.113519\n",
      "EPOCH: 413 LOSS: 7.042412 MAE: 1.932898 VAL_LOSS: 8.635120 VAL_MAE: 2.347376\n",
      "EPOCH: 414 LOSS: 7.579765 MAE: 2.072554 VAL_LOSS: 9.739096 VAL_MAE: 2.509167\n",
      "EPOCH: 415 LOSS: 7.800655 MAE: 2.123318 VAL_LOSS: 7.347685 VAL_MAE: 2.074126\n",
      "EPOCH: 416 LOSS: 7.712820 MAE: 2.098162 VAL_LOSS: 7.460403 VAL_MAE: 2.106672\n",
      "EPOCH: 417 LOSS: 7.381037 MAE: 2.047907 VAL_LOSS: 7.329878 VAL_MAE: 2.088880\n",
      "EPOCH: 418 LOSS: 7.239652 MAE: 2.055903 VAL_LOSS: 7.914183 VAL_MAE: 2.212062\n",
      "EPOCH: 419 LOSS: 7.724269 MAE: 2.112501 VAL_LOSS: 7.954243 VAL_MAE: 2.211848\n",
      "EPOCH: 420 LOSS: 7.669692 MAE: 2.046726 VAL_LOSS: 8.127224 VAL_MAE: 2.261239\n",
      "EPOCH: 421 LOSS: 7.254672 MAE: 2.019722 VAL_LOSS: 8.046508 VAL_MAE: 2.215173\n",
      "EPOCH: 422 LOSS: 7.981555 MAE: 2.152534 VAL_LOSS: 7.329841 VAL_MAE: 2.070117\n",
      "EPOCH: 423 LOSS: 8.208689 MAE: 2.209138 VAL_LOSS: 7.881830 VAL_MAE: 2.211265\n",
      "EPOCH: 424 LOSS: 7.897012 MAE: 2.140526 VAL_LOSS: 6.397264 VAL_MAE: 1.950943\n",
      "EPOCH: 425 LOSS: 6.596807 MAE: 1.828779 VAL_LOSS: 7.912591 VAL_MAE: 2.201442\n",
      "EPOCH: 426 LOSS: 7.443445 MAE: 2.078572 VAL_LOSS: 7.542631 VAL_MAE: 2.104185\n",
      "EPOCH: 427 LOSS: 8.035364 MAE: 2.184268 VAL_LOSS: 7.134912 VAL_MAE: 2.046470\n",
      "EPOCH: 428 LOSS: 8.204481 MAE: 2.222143 VAL_LOSS: 7.625587 VAL_MAE: 2.124181\n",
      "EPOCH: 429 LOSS: 8.873046 MAE: 2.340316 VAL_LOSS: 7.589405 VAL_MAE: 2.155592\n",
      "EPOCH: 430 LOSS: 7.739498 MAE: 2.127708 VAL_LOSS: 8.153916 VAL_MAE: 2.232621\n",
      "EPOCH: 431 LOSS: 6.370163 MAE: 1.804942 VAL_LOSS: 8.593060 VAL_MAE: 2.325286\n",
      "EPOCH: 432 LOSS: 7.278408 MAE: 1.999144 VAL_LOSS: 8.049757 VAL_MAE: 2.190309\n",
      "EPOCH: 433 LOSS: 7.990639 MAE: 2.157346 VAL_LOSS: 7.697320 VAL_MAE: 2.141532\n",
      "EPOCH: 434 LOSS: 7.941651 MAE: 2.164580 VAL_LOSS: 6.208607 VAL_MAE: 1.877816\n",
      "EPOCH: 435 LOSS: 7.203626 MAE: 1.957154 VAL_LOSS: 7.315291 VAL_MAE: 2.044565\n",
      "EPOCH: 436 LOSS: 7.084804 MAE: 2.040441 VAL_LOSS: 8.468681 VAL_MAE: 2.299347\n",
      "EPOCH: 437 LOSS: 7.565289 MAE: 2.055137 VAL_LOSS: 8.433937 VAL_MAE: 2.290003\n",
      "EPOCH: 438 LOSS: 6.872972 MAE: 1.918338 VAL_LOSS: 7.527306 VAL_MAE: 2.137068\n",
      "EPOCH: 439 LOSS: 7.562325 MAE: 2.076113 VAL_LOSS: 7.817390 VAL_MAE: 2.240724\n",
      "EPOCH: 440 LOSS: 8.188273 MAE: 2.185792 VAL_LOSS: 8.063453 VAL_MAE: 2.246272\n",
      "EPOCH: 441 LOSS: 7.662774 MAE: 2.122540 VAL_LOSS: 7.679512 VAL_MAE: 2.151617\n",
      "EPOCH: 442 LOSS: 7.684191 MAE: 2.092451 VAL_LOSS: 8.096457 VAL_MAE: 2.208393\n",
      "EPOCH: 443 LOSS: 7.653013 MAE: 2.107268 VAL_LOSS: 6.633520 VAL_MAE: 1.944245\n",
      "EPOCH: 444 LOSS: 7.563036 MAE: 2.085182 VAL_LOSS: 9.888908 VAL_MAE: 2.636822\n",
      "EPOCH: 445 LOSS: 7.984951 MAE: 2.175472 VAL_LOSS: 7.539705 VAL_MAE: 2.118129\n",
      "EPOCH: 446 LOSS: 7.109812 MAE: 1.977700 VAL_LOSS: 7.955980 VAL_MAE: 2.214336\n",
      "EPOCH: 447 LOSS: 7.882141 MAE: 2.147417 VAL_LOSS: 8.379277 VAL_MAE: 2.265986\n",
      "EPOCH: 448 LOSS: 7.286375 MAE: 2.024035 VAL_LOSS: 7.084370 VAL_MAE: 2.032115\n",
      "EPOCH: 449 LOSS: 7.934435 MAE: 2.164825 VAL_LOSS: 7.552002 VAL_MAE: 2.098776\n",
      "EPOCH: 450 LOSS: 7.419712 MAE: 2.044140 VAL_LOSS: 7.714482 VAL_MAE: 2.169122\n",
      "EPOCH: 451 LOSS: 7.399462 MAE: 2.085969 VAL_LOSS: 6.951991 VAL_MAE: 1.978000\n",
      "EPOCH: 452 LOSS: 7.905756 MAE: 2.109656 VAL_LOSS: 8.228252 VAL_MAE: 2.218672\n",
      "EPOCH: 453 LOSS: 7.640187 MAE: 2.074372 VAL_LOSS: 7.524990 VAL_MAE: 2.137827\n",
      "EPOCH: 454 LOSS: 7.397268 MAE: 2.048126 VAL_LOSS: 8.063521 VAL_MAE: 2.253207\n",
      "EPOCH: 455 LOSS: 7.925283 MAE: 2.124698 VAL_LOSS: 7.562102 VAL_MAE: 2.133182\n",
      "EPOCH: 456 LOSS: 8.336724 MAE: 2.225698 VAL_LOSS: 7.541069 VAL_MAE: 2.130522\n",
      "EPOCH: 457 LOSS: 7.352229 MAE: 2.071344 VAL_LOSS: 7.916936 VAL_MAE: 2.187619\n",
      "EPOCH: 458 LOSS: 8.054756 MAE: 2.184376 VAL_LOSS: 8.033824 VAL_MAE: 2.199814\n",
      "EPOCH: 459 LOSS: 7.347918 MAE: 2.011405 VAL_LOSS: 10.311139 VAL_MAE: 2.708193\n",
      "EPOCH: 460 LOSS: 7.207982 MAE: 1.940655 VAL_LOSS: 8.320285 VAL_MAE: 2.276271\n",
      "EPOCH: 461 LOSS: 6.782619 MAE: 1.922038 VAL_LOSS: 7.969264 VAL_MAE: 2.198026\n",
      "EPOCH: 462 LOSS: 7.716466 MAE: 2.116145 VAL_LOSS: 7.487279 VAL_MAE: 2.108179\n",
      "EPOCH: 463 LOSS: 7.441219 MAE: 2.027475 VAL_LOSS: 7.703595 VAL_MAE: 2.126120\n",
      "EPOCH: 464 LOSS: 7.173959 MAE: 1.963071 VAL_LOSS: 8.819480 VAL_MAE: 2.414119\n",
      "EPOCH: 465 LOSS: 7.056153 MAE: 1.961277 VAL_LOSS: 8.545612 VAL_MAE: 2.281922\n",
      "EPOCH: 466 LOSS: 7.473735 MAE: 2.007287 VAL_LOSS: 7.818101 VAL_MAE: 2.170474\n",
      "EPOCH: 467 LOSS: 7.616248 MAE: 2.053994 VAL_LOSS: 7.526017 VAL_MAE: 2.097590\n",
      "EPOCH: 468 LOSS: 7.330831 MAE: 2.041702 VAL_LOSS: 7.667415 VAL_MAE: 2.134170\n",
      "EPOCH: 469 LOSS: 7.840114 MAE: 2.103085 VAL_LOSS: 8.150559 VAL_MAE: 2.364856\n",
      "EPOCH: 470 LOSS: 7.931036 MAE: 2.143165 VAL_LOSS: 7.514866 VAL_MAE: 2.096634\n",
      "EPOCH: 471 LOSS: 7.392602 MAE: 2.010231 VAL_LOSS: 7.750736 VAL_MAE: 2.162093\n",
      "EPOCH: 472 LOSS: 7.661590 MAE: 2.054453 VAL_LOSS: 7.898980 VAL_MAE: 2.185606\n",
      "EPOCH: 473 LOSS: 8.258297 MAE: 2.223998 VAL_LOSS: 8.005019 VAL_MAE: 2.220695\n",
      "EPOCH: 474 LOSS: 7.238261 MAE: 2.042860 VAL_LOSS: 7.478625 VAL_MAE: 2.019996\n",
      "EPOCH: 475 LOSS: 6.782574 MAE: 1.936791 VAL_LOSS: 7.226867 VAL_MAE: 2.064014\n",
      "EPOCH: 476 LOSS: 7.615615 MAE: 2.084753 VAL_LOSS: 7.459230 VAL_MAE: 2.090407\n",
      "EPOCH: 477 LOSS: 7.402801 MAE: 2.025089 VAL_LOSS: 7.996843 VAL_MAE: 2.205329\n",
      "EPOCH: 478 LOSS: 7.827271 MAE: 2.118898 VAL_LOSS: 7.486486 VAL_MAE: 2.088886\n",
      "EPOCH: 479 LOSS: 7.830509 MAE: 2.116302 VAL_LOSS: 10.048392 VAL_MAE: 2.554896\n",
      "EPOCH: 480 LOSS: 7.532595 MAE: 2.038676 VAL_LOSS: 6.753307 VAL_MAE: 1.963335\n",
      "EPOCH: 481 LOSS: 6.227914 MAE: 1.796437 VAL_LOSS: 7.300703 VAL_MAE: 2.093132\n",
      "EPOCH: 482 LOSS: 7.240153 MAE: 1.980141 VAL_LOSS: 8.267601 VAL_MAE: 2.231724\n",
      "EPOCH: 483 LOSS: 8.074355 MAE: 2.168758 VAL_LOSS: 6.880951 VAL_MAE: 1.971618\n",
      "EPOCH: 484 LOSS: 8.113987 MAE: 2.199374 VAL_LOSS: 7.139993 VAL_MAE: 2.040088\n",
      "EPOCH: 485 LOSS: 7.676667 MAE: 2.063501 VAL_LOSS: 7.515744 VAL_MAE: 2.116828\n",
      "EPOCH: 486 LOSS: 7.794550 MAE: 2.119355 VAL_LOSS: 7.453442 VAL_MAE: 2.119805\n",
      "EPOCH: 487 LOSS: 7.141861 MAE: 1.980486 VAL_LOSS: 7.570793 VAL_MAE: 2.096221\n",
      "EPOCH: 488 LOSS: 7.907527 MAE: 2.115476 VAL_LOSS: 7.797301 VAL_MAE: 2.161105\n",
      "EPOCH: 489 LOSS: 6.923983 MAE: 1.952848 VAL_LOSS: 9.469981 VAL_MAE: 2.437829\n",
      "EPOCH: 490 LOSS: 7.822907 MAE: 2.120771 VAL_LOSS: 7.412405 VAL_MAE: 2.094701\n",
      "EPOCH: 491 LOSS: 6.341990 MAE: 1.793819 VAL_LOSS: 8.446580 VAL_MAE: 2.303754\n",
      "EPOCH: 492 LOSS: 7.384131 MAE: 2.011831 VAL_LOSS: 7.812979 VAL_MAE: 2.176223\n",
      "EPOCH: 493 LOSS: 8.157625 MAE: 2.173192 VAL_LOSS: 7.766143 VAL_MAE: 2.181556\n",
      "EPOCH: 494 LOSS: 7.400941 MAE: 2.016957 VAL_LOSS: 10.091889 VAL_MAE: 2.587187\n",
      "EPOCH: 495 LOSS: 7.757184 MAE: 2.121371 VAL_LOSS: 7.064696 VAL_MAE: 2.041085\n",
      "EPOCH: 496 LOSS: 7.306204 MAE: 2.049234 VAL_LOSS: 7.370693 VAL_MAE: 2.126425\n",
      "EPOCH: 497 LOSS: 7.326258 MAE: 2.050344 VAL_LOSS: 8.259296 VAL_MAE: 2.246997\n",
      "EPOCH: 498 LOSS: 8.043025 MAE: 2.126944 VAL_LOSS: 7.702727 VAL_MAE: 2.142694\n",
      "EPOCH: 499 LOSS: 7.371414 MAE: 2.055362 VAL_LOSS: 5.022876 VAL_MAE: 1.597376\n",
      "EPOCH: 500 LOSS: 7.478396 MAE: 2.029572 VAL_LOSS: 6.691214 VAL_MAE: 1.938821\n",
      "EPOCH: 501 LOSS: 8.063842 MAE: 2.165215 VAL_LOSS: 7.559993 VAL_MAE: 2.126390\n",
      "EPOCH: 502 LOSS: 6.965089 MAE: 1.984519 VAL_LOSS: 7.694044 VAL_MAE: 2.140669\n",
      "EPOCH: 503 LOSS: 6.521992 MAE: 1.861748 VAL_LOSS: 7.768884 VAL_MAE: 2.171849\n",
      "EPOCH: 504 LOSS: 7.406383 MAE: 2.053689 VAL_LOSS: 8.422347 VAL_MAE: 2.253899\n",
      "EPOCH: 505 LOSS: 7.938701 MAE: 2.100265 VAL_LOSS: 6.792022 VAL_MAE: 1.998567\n",
      "EPOCH: 506 LOSS: 7.479026 MAE: 2.037657 VAL_LOSS: 7.058248 VAL_MAE: 2.018531\n",
      "EPOCH: 507 LOSS: 7.246566 MAE: 2.000034 VAL_LOSS: 7.352126 VAL_MAE: 2.071998\n",
      "EPOCH: 508 LOSS: 7.371604 MAE: 1.978195 VAL_LOSS: 7.763541 VAL_MAE: 2.148648\n",
      "EPOCH: 509 LOSS: 7.463347 MAE: 2.054167 VAL_LOSS: 6.734663 VAL_MAE: 1.917086\n",
      "EPOCH: 510 LOSS: 7.918876 MAE: 2.161856 VAL_LOSS: 6.574737 VAL_MAE: 1.921073\n",
      "EPOCH: 511 LOSS: 7.356175 MAE: 2.017898 VAL_LOSS: 8.223397 VAL_MAE: 2.256576\n",
      "EPOCH: 512 LOSS: 7.293328 MAE: 2.007608 VAL_LOSS: 7.227006 VAL_MAE: 2.064037\n"
     ]
    }
   ],
   "source": [
    "epochs = 512\n",
    "steps_per_epoch = 512 // batch_size + 1  # we usually consider 1 epoch to be\n",
    "                                            # the point where the model has seen\n",
    "                                            # all the training samples at least once\n",
    "\n",
    "historyA = {\"history\":{\"loss\":[],\"mae\":[],\"val_loss\":[],\"val_mae\":[]}}\n",
    "for e in range(epochs):\n",
    "    for i, (images, y_batch) in enumerate(train_generator):\n",
    "       new_y_batch = []\n",
    "       for x,img in enumerate(images):\n",
    "        array = np.ones(image_size+(3,))\n",
    "        array *= img>0\n",
    "        array[array>0] = y_batch[x]\n",
    "        new_y_batch.append(array)\n",
    "       new_y_batch = np.array(new_y_batch)\n",
    "       loss = modelA.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "    #    val = modelB.test_on_batch(images, new_y_batch)\n",
    "       if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "            historyA[\"history\"][\"loss\"].append(loss[0])\n",
    "            historyA[\"history\"][\"mae\"].append(loss[1])\n",
    "            # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "            break  \n",
    "    for i, (images, y_batch) in enumerate(val_generator):\n",
    "       new_y_batch = []\n",
    "       for x,img in enumerate(images):\n",
    "        array = np.ones(image_size+(3,))\n",
    "        array *= img>0\n",
    "        array[array>0] = y_batch[x]\n",
    "        new_y_batch.append(array)\n",
    "       new_y_batch = np.array(new_y_batch)\n",
    "    #    loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "       val = modelA.test_on_batch(images, new_y_batch)\n",
    "       if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "            historyA[\"history\"][\"val_loss\"].append(val[0])\n",
    "            historyA[\"history\"][\"val_mae\"].append(val[1])\n",
    "            # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "            break  \n",
    "    print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}\".format(e+1, loss[0], loss[1],val[0],val[1]))\n",
    "    train_generator.on_epoch_end()  # this shuffles the data at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(historyA[\"history\"][\"loss\"], color='g')\n",
    "plt.plot(historyA[\"history\"][\"val_loss\"], color='r')\n",
    "plt.legend([\"Train\"])\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(historyA[\"history\"][\"mae\"], color='g')\n",
    "plt.plot(historyA[\"history\"][\"val_mae\"], color='r')\n",
    "plt.legend([\"Train\"])\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "plt.suptitle(\"Model A: Nested U-Net\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(train_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[-0.03045534]\n",
      "   [-0.03045534]\n",
      "   [-0.03045534]\n",
      "   ...\n",
      "   [ 4.969138  ]\n",
      "   [ 5.128071  ]\n",
      "   [ 4.8109818 ]]\n",
      "\n",
      "  [[-0.03045534]\n",
      "   [-0.03045534]\n",
      "   [-0.03045534]\n",
      "   ...\n",
      "   [ 4.5028253 ]\n",
      "   [ 4.8109818 ]\n",
      "   [ 5.102302  ]]\n",
      "\n",
      "  [[-0.03045534]\n",
      "   [-0.03045534]\n",
      "   [-0.03045534]\n",
      "   ...\n",
      "   [ 5.5998507 ]\n",
      "   [ 5.5676527 ]\n",
      "   [ 6.293053  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 5.42134   ]\n",
      "   [ 5.532556  ]\n",
      "   [ 5.3520684 ]\n",
      "   ...\n",
      "   [-0.05947632]\n",
      "   [ 2.375415  ]\n",
      "   [-0.0442092 ]]\n",
      "\n",
      "  [[ 5.4562573 ]\n",
      "   [ 5.382819  ]\n",
      "   [ 5.396638  ]\n",
      "   ...\n",
      "   [-0.03952482]\n",
      "   [ 2.1804636 ]\n",
      "   [-0.08588254]]\n",
      "\n",
      "  [[ 5.382819  ]\n",
      "   [ 5.382819  ]\n",
      "   [ 5.4039617 ]\n",
      "   ...\n",
      "   [ 2.3145533 ]\n",
      "   [-0.08588254]\n",
      "   [ 2.1804636 ]]]\n",
      "\n",
      "\n",
      " [[[-0.06207621]\n",
      "   [-0.06207621]\n",
      "   [-0.03164423]\n",
      "   ...\n",
      "   [ 7.6777844 ]\n",
      "   [ 2.394605  ]\n",
      "   [ 2.394557  ]]\n",
      "\n",
      "  [[ 2.4347022 ]\n",
      "   [ 2.3097637 ]\n",
      "   [-0.03164423]\n",
      "   ...\n",
      "   [ 5.392128  ]\n",
      "   [ 2.394557  ]\n",
      "   [ 2.3791034 ]]\n",
      "\n",
      "  [[ 2.1690173 ]\n",
      "   [ 2.390727  ]\n",
      "   [ 2.3097637 ]\n",
      "   ...\n",
      "   [ 2.591487  ]\n",
      "   [ 2.2337599 ]\n",
      "   [ 2.0320773 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.08377291]\n",
      "   [ 2.1295803 ]\n",
      "   [ 2.1295803 ]\n",
      "   ...\n",
      "   [ 2.35727   ]\n",
      "   [ 4.5716553 ]\n",
      "   [ 4.5716553 ]]\n",
      "\n",
      "  [[ 2.1742706 ]\n",
      "   [ 2.1742706 ]\n",
      "   [ 2.3251202 ]\n",
      "   ...\n",
      "   [ 2.2980986 ]\n",
      "   [ 2.35727   ]\n",
      "   [ 4.541474  ]]\n",
      "\n",
      "  [[ 2.1742706 ]\n",
      "   [ 2.1742706 ]\n",
      "   [-0.03816029]\n",
      "   ...\n",
      "   [ 2.3763561 ]\n",
      "   [ 2.35727   ]\n",
      "   [ 2.35727   ]]]\n",
      "\n",
      "\n",
      " [[[-0.14803702]\n",
      "   [-0.14803702]\n",
      "   [-0.09314053]\n",
      "   ...\n",
      "   [-0.04865204]\n",
      "   [-0.049032  ]\n",
      "   [ 2.1173646 ]]\n",
      "\n",
      "  [[ 5.3353424 ]\n",
      "   [ 5.308216  ]\n",
      "   [ 3.7647464 ]\n",
      "   ...\n",
      "   [-0.04865204]\n",
      "   [-0.049032  ]\n",
      "   [ 4.299035  ]]\n",
      "\n",
      "  [[-0.03610433]\n",
      "   [-0.03610433]\n",
      "   [ 3.955246  ]\n",
      "   ...\n",
      "   [-0.04865204]\n",
      "   [ 2.7524037 ]\n",
      "   [ 2.3695965 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 2.0408018 ]\n",
      "   [-0.02077097]\n",
      "   [-0.03938183]\n",
      "   ...\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]]\n",
      "\n",
      "  [[ 2.0408018 ]\n",
      "   [-0.02077097]\n",
      "   [-0.03361152]\n",
      "   ...\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]]\n",
      "\n",
      "  [[-0.02077097]\n",
      "   [-0.02077097]\n",
      "   [-0.03361152]\n",
      "   ...\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 8.38096   ]\n",
      "   [ 8.381093  ]\n",
      "   [ 7.689374  ]\n",
      "   ...\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]]\n",
      "\n",
      "  [[ 8.439999  ]\n",
      "   [ 8.320074  ]\n",
      "   [ 8.17368   ]\n",
      "   ...\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]]\n",
      "\n",
      "  [[ 8.411559  ]\n",
      "   [ 8.200707  ]\n",
      "   [ 8.1988945 ]\n",
      "   ...\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.02568645]\n",
      "   [-0.02568645]\n",
      "   [-0.02568645]\n",
      "   ...\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]]\n",
      "\n",
      "  [[-0.02568645]\n",
      "   [-0.02568645]\n",
      "   [-0.02568645]\n",
      "   ...\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]]\n",
      "\n",
      "  [[-0.02568645]\n",
      "   [-0.02568645]\n",
      "   [-0.02568645]\n",
      "   ...\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]]]\n",
      "\n",
      "\n",
      " [[[-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   ...\n",
      "   [ 2.0210755 ]\n",
      "   [ 2.0626285 ]\n",
      "   [ 2.0626285 ]]\n",
      "\n",
      "  [[-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   ...\n",
      "   [ 2.0210755 ]\n",
      "   [ 2.0626285 ]\n",
      "   [ 2.0626285 ]]\n",
      "\n",
      "  [[-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   ...\n",
      "   [ 2.0210755 ]\n",
      "   [ 2.0626285 ]\n",
      "   [ 2.0626285 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 2.2326005 ]\n",
      "   [ 2.2326005 ]\n",
      "   [ 2.2326005 ]\n",
      "   ...\n",
      "   [ 4.822626  ]\n",
      "   [ 4.702785  ]\n",
      "   [ 4.681659  ]]\n",
      "\n",
      "  [[ 2.3188286 ]\n",
      "   [ 2.3188286 ]\n",
      "   [ 2.3188286 ]\n",
      "   ...\n",
      "   [ 7.177142  ]\n",
      "   [ 7.081558  ]\n",
      "   [ 7.093431  ]]\n",
      "\n",
      "  [[ 2.32253   ]\n",
      "   [ 2.32253   ]\n",
      "   [ 2.32253   ]\n",
      "   ...\n",
      "   [ 4.562008  ]\n",
      "   [ 4.66305   ]\n",
      "   [ 4.634448  ]]]\n",
      "\n",
      "\n",
      " [[[-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   ...\n",
      "   [-0.02582387]\n",
      "   [-0.02582387]\n",
      "   [-0.02582387]]\n",
      "\n",
      "  [[-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   ...\n",
      "   [-0.02582387]\n",
      "   [-0.02582387]\n",
      "   [-0.02582387]]\n",
      "\n",
      "  [[-0.02537522]\n",
      "   [-0.02537522]\n",
      "   [-0.02537522]\n",
      "   ...\n",
      "   [-0.02582387]\n",
      "   [-0.02582387]\n",
      "   [-0.02582387]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.05385276]\n",
      "   [ 2.594901  ]\n",
      "   [ 2.2360954 ]\n",
      "   ...\n",
      "   [ 1.9071655 ]\n",
      "   [ 2.2501075 ]\n",
      "   [ 2.2501075 ]]\n",
      "\n",
      "  [[-0.05385276]\n",
      "   [ 2.594901  ]\n",
      "   [ 4.198589  ]\n",
      "   ...\n",
      "   [ 4.2874327 ]\n",
      "   [ 2.0019271 ]\n",
      "   [ 2.0019271 ]]\n",
      "\n",
      "  [[ 1.9480313 ]\n",
      "   [ 2.594901  ]\n",
      "   [ 4.791328  ]\n",
      "   ...\n",
      "   [ 2.4060009 ]\n",
      "   [ 3.103771  ]\n",
      "   [ 4.667052  ]]]], shape=(32, 128, 128, 1), dtype=float32)\n",
      "[7 6 8 9 9 7 7 6 9 6 7 9 9 8 7 6 6 6 7 9 6 9 6 7 7 7 9 6 7 9 6 9]\n"
     ]
    }
   ],
   "source": [
    "print(modelA(sample[0]))\n",
    "print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.784975 6.0\n",
      "5.8316975 9.0\n",
      "5.8965263 8.0\n",
      "5.8015866 6.0\n",
      "5.8860407 9.0\n",
      "5.980369 7.0\n",
      "5.759862 8.0\n",
      "5.7337956 8.0\n",
      "5.7314296 9.0\n",
      "5.785751 7.0\n",
      "5.5725408 8.0\n",
      "6.016336 8.0\n",
      "5.8472342 8.0\n",
      "5.829247 8.0\n",
      "5.820451 8.0\n",
      "6.048324 8.0\n",
      "5.948191 9.0\n",
      "5.6744432 7.0\n",
      "5.6487474 6.0\n",
      "5.7638903 7.0\n",
      "5.87953 7.0\n",
      "5.7535563 8.0\n",
      "5.874272 9.0\n",
      "6.020834 7.0\n",
      "5.914802 9.0\n",
      "5.758165 8.0\n",
      "5.7863746 6.0\n",
      "5.7537823 6.0\n",
      "5.82201 6.0\n",
      "5.9064016 7.0\n",
      "5.7372937 7.0\n",
      "5.7303786 6.0\n"
     ]
    }
   ],
   "source": [
    "sample = next(train_generator)\n",
    "result = modelA(sample[0])\n",
    "for x,i in enumerate(result):\n",
    "    RGB  = cv2.cvtColor(sample[0][x],cv2.COLOR_HSV2RGB)\n",
    "    MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "    ARRAY = np.array(result[5]).reshape(image_size) * MASK\n",
    "    OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "    print(np.average(OUTPUT), np.average(sample[1][x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA.save('design_models/designA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelA = tf.keras.models.load_model('design_models/designA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'my_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[218], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rmtree\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# removing directory \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mc:\\Users\\miniconda3\\envs\\tf\\lib\\shutil.py:759\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\miniconda3\\envs\\tf\\lib\\shutil.py:610\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    608\u001b[0m         entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m     entries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m entries:\n",
      "File \u001b[1;32mc:\\Users\\miniconda3\\envs\\tf\\lib\\shutil.py:607\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rmtree_unsafe\u001b[39m(path, onerror):\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m scandir_it:\n\u001b[0;32m    608\u001b[0m             entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'my_dir'"
     ]
    }
   ],
   "source": [
    "from shutil import rmtree\n",
    "# removing directory \n",
    "rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segnet(hp):\n",
    "    hp_filters = hp.Choice('filters',values = [16,32,64,128])\n",
    "    # Encoding layer\n",
    "    img_input = Input(shape=image_size+(3,))\n",
    "    x = Conv2D(hp_filters, (3, 3), padding='same', name='conv1',strides= (1,1))(img_input)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters, (3, 3), padding='same', name='conv2')(x)\n",
    "    x = BatchNormalization(name='bn2')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    \n",
    "    x = Conv2D(hp_filters*2 , (3, 3), padding='same', name='conv3')(x)\n",
    "    x = BatchNormalization(name='bn3')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters*2 , (3, 3), padding='same', name='conv4')(x)\n",
    "    x = BatchNormalization(name='bn4')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv2D(hp_filters*4, (3, 3), padding='same', name='conv5')(x)\n",
    "    x = BatchNormalization(name='bn5')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters*4, (3, 3), padding='same', name='conv6')(x)\n",
    "    x = BatchNormalization(name='bn6')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters*4, (3, 3), padding='same', name='conv7')(x)\n",
    "    x = BatchNormalization(name='bn7')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv2D(hp_filters*8, (3, 3), padding='same', name='conv8')(x)\n",
    "    x = BatchNormalization(name='bn8')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters*8, (3, 3), padding='same', name='conv9')(x)\n",
    "    x = BatchNormalization(name='bn9')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters*8, (3, 3), padding='same', name='conv10')(x)\n",
    "    x = BatchNormalization(name='bn10')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    \n",
    "    x = Conv2D(hp_filters*8, (3, 3), padding='same', name='conv11')(x)\n",
    "    x = BatchNormalization(name='bn11')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters*8, (3, 3), padding='same', name='conv12')(x)\n",
    "    x = BatchNormalization(name='bn12')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(hp_filters*8, (3, 3), padding='same', name='conv13')(x)\n",
    "    x = BatchNormalization(name='bn13')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Dense(hp_filters*16, activation = 'relu', name='fc1')(x)\n",
    "    x = Dense(hp_filters*16, activation = 'relu', name='fc2')(x)\n",
    "    # Decoding Layer \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters*8, (3, 3), padding='same', name='deconv1')(x)\n",
    "    x = BatchNormalization(name='bn14')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters*8, (3, 3), padding='same', name='deconv2')(x)\n",
    "    x = BatchNormalization(name='bn15')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters*8, (3, 3), padding='same', name='deconv3')(x)\n",
    "    x = BatchNormalization(name='bn16')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters*8, (3, 3), padding='same', name='deconv4')(x)\n",
    "    x = BatchNormalization(name='bn17')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters*8, (3, 3), padding='same', name='deconv5')(x)\n",
    "    x = BatchNormalization(name='bn18')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters*4, (3, 3), padding='same', name='deconv6')(x)\n",
    "    x = BatchNormalization(name='bn19')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters*4, (3, 3), padding='same', name='deconv7')(x)\n",
    "    x = BatchNormalization(name='bn20')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters*4, (3, 3), padding='same', name='deconv8')(x)\n",
    "    x = BatchNormalization(name='bn21')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters*2, (3, 3), padding='same', name='deconv9')(x)\n",
    "    x = BatchNormalization(name='bn22')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters*2, (3, 3), padding='same', name='deconv10')(x)\n",
    "    x = BatchNormalization(name='bn23')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(hp_filters, (3, 3), padding='same', name='deconv11')(x)\n",
    "    x = BatchNormalization(name='bn24')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(hp_filters, (3, 3), padding='same', name='deconv12')(x)\n",
    "    x = BatchNormalization(name='bn25')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(1, (3, 3), padding='same', name='deconv13')(x)\n",
    "    x = BatchNormalization(name='bn26')(x)\n",
    "    output = Activation('linear')(x)\n",
    "    # output = Conv2D(1,1)(x)\n",
    "    # pred = layers.Reshape(image_size)(x)\n",
    "    # x = Flatten()(x)\n",
    "    # hp_units = hp.Choice('units',values = [512,1024])\n",
    "    # x = Dense(hp_units, activation='relu')(x)\n",
    "    # if hp.Boolean(\"2nd dense\"):\n",
    "    #     x = Dense(hp_units, activation='relu')(x)\n",
    "    # if hp.Boolean(\"3rd dense\"):\n",
    "    #     x = Dense(hp_units, activation='relu')(x)\n",
    "    # if hp.Boolean(\"4th dense\"):\n",
    "    #     x = Dense(hp_units, activation='relu')(x)\n",
    "    # output=Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=img_input, outputs=output)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    model.compile(optimizer= tf.keras.optimizers.Adam(lr=hp_learning_rate), loss= \"mse\"\n",
    "                  , metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "128               |128               |filters\n",
      "0.001             |0.001             |learning_rate\n",
      "\n",
      "Epoch 1/100\n",
      " 6/16 [==========>...................] - ETA: 13s - loss: 57.6956 - mae: 7.4483WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4903s vs `on_train_batch_end` time: 0.6778s). Check your callbacks.\n",
      "16/16 [==============================] - 67s 3s/step - loss: 58.3657 - mae: 7.5130 - val_loss: 77694345216.0000 - val_mae: 141653.8281\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 36s 2s/step - loss: 58.1007 - mae: 7.5091 - val_loss: 88306640.0000 - val_mae: 7328.6211\n",
      "Epoch 3/100\n",
      " 9/16 [===============>..............] - ETA: 9s - loss: 57.8821 - mae: 7.4915 "
     ]
    }
   ],
   "source": [
    "tunerB = kt.BayesianOptimization(segnet,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 1,\n",
    "                     project_name='design_b',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "\n",
    "tunerB.search(train_generator, epochs=100, validation_data=val_generator, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsB=tunerB.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\n",
    "\n",
    "print(best_hpsB.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hpsB.values['filters']=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " bn1 (BatchNormalization)    (None, 32, 32, 64)        256       \n",
      "                                                                 \n",
      " activation_104 (Activation)  (None, 32, 32, 64)       0         \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " bn2 (BatchNormalization)    (None, 32, 32, 64)        256       \n",
      "                                                                 \n",
      " activation_105 (Activation)  (None, 32, 32, 64)       0         \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 16, 16, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " bn3 (BatchNormalization)    (None, 16, 16, 128)       512       \n",
      "                                                                 \n",
      " activation_106 (Activation)  (None, 16, 16, 128)      0         \n",
      "                                                                 \n",
      " conv4 (Conv2D)              (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " bn4 (BatchNormalization)    (None, 16, 16, 128)       512       \n",
      "                                                                 \n",
      " activation_107 (Activation)  (None, 16, 16, 128)      0         \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 8, 8, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv5 (Conv2D)              (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " bn5 (BatchNormalization)    (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_108 (Activation)  (None, 8, 8, 256)        0         \n",
      "                                                                 \n",
      " conv6 (Conv2D)              (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn6 (BatchNormalization)    (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_109 (Activation)  (None, 8, 8, 256)        0         \n",
      "                                                                 \n",
      " conv7 (Conv2D)              (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn7 (BatchNormalization)    (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_110 (Activation)  (None, 8, 8, 256)        0         \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 4, 4, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv8 (Conv2D)              (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " bn8 (BatchNormalization)    (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_111 (Activation)  (None, 4, 4, 512)        0         \n",
      "                                                                 \n",
      " conv9 (Conv2D)              (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn9 (BatchNormalization)    (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_112 (Activation)  (None, 4, 4, 512)        0         \n",
      "                                                                 \n",
      " conv10 (Conv2D)             (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn10 (BatchNormalization)   (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_113 (Activation)  (None, 4, 4, 512)        0         \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 2, 2, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv11 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn11 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_114 (Activation)  (None, 2, 2, 512)        0         \n",
      "                                                                 \n",
      " conv12 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn12 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_115 (Activation)  (None, 2, 2, 512)        0         \n",
      "                                                                 \n",
      " conv13 (Conv2D)             (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn13 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_116 (Activation)  (None, 2, 2, 512)        0         \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPoolin  (None, 1, 1, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 1, 1, 1024)        525312    \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 1, 1, 1024)        1049600   \n",
      "                                                                 \n",
      " up_sampling2d_20 (UpSamplin  (None, 2, 2, 1024)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv1 (Conv2DTranspose)   (None, 2, 2, 512)         4719104   \n",
      "                                                                 \n",
      " bn14 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_117 (Activation)  (None, 2, 2, 512)        0         \n",
      "                                                                 \n",
      " deconv2 (Conv2DTranspose)   (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn15 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_118 (Activation)  (None, 2, 2, 512)        0         \n",
      "                                                                 \n",
      " deconv3 (Conv2DTranspose)   (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " bn16 (BatchNormalization)   (None, 2, 2, 512)         2048      \n",
      "                                                                 \n",
      " activation_119 (Activation)  (None, 2, 2, 512)        0         \n",
      "                                                                 \n",
      " up_sampling2d_21 (UpSamplin  (None, 4, 4, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv4 (Conv2DTranspose)   (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn17 (BatchNormalization)   (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_120 (Activation)  (None, 4, 4, 512)        0         \n",
      "                                                                 \n",
      " deconv5 (Conv2DTranspose)   (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " bn18 (BatchNormalization)   (None, 4, 4, 512)         2048      \n",
      "                                                                 \n",
      " activation_121 (Activation)  (None, 4, 4, 512)        0         \n",
      "                                                                 \n",
      " deconv6 (Conv2DTranspose)   (None, 4, 4, 256)         1179904   \n",
      "                                                                 \n",
      " bn19 (BatchNormalization)   (None, 4, 4, 256)         1024      \n",
      "                                                                 \n",
      " activation_122 (Activation)  (None, 4, 4, 256)        0         \n",
      "                                                                 \n",
      " up_sampling2d_22 (UpSamplin  (None, 8, 8, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv7 (Conv2DTranspose)   (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn20 (BatchNormalization)   (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_123 (Activation)  (None, 8, 8, 256)        0         \n",
      "                                                                 \n",
      " deconv8 (Conv2DTranspose)   (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " bn21 (BatchNormalization)   (None, 8, 8, 256)         1024      \n",
      "                                                                 \n",
      " activation_124 (Activation)  (None, 8, 8, 256)        0         \n",
      "                                                                 \n",
      " deconv9 (Conv2DTranspose)   (None, 8, 8, 128)         295040    \n",
      "                                                                 \n",
      " bn22 (BatchNormalization)   (None, 8, 8, 128)         512       \n",
      "                                                                 \n",
      " activation_125 (Activation)  (None, 8, 8, 128)        0         \n",
      "                                                                 \n",
      " up_sampling2d_23 (UpSamplin  (None, 16, 16, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv10 (Conv2DTranspose)  (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " bn23 (BatchNormalization)   (None, 16, 16, 128)       512       \n",
      "                                                                 \n",
      " activation_126 (Activation)  (None, 16, 16, 128)      0         \n",
      "                                                                 \n",
      " deconv11 (Conv2DTranspose)  (None, 16, 16, 64)        73792     \n",
      "                                                                 \n",
      " bn24 (BatchNormalization)   (None, 16, 16, 64)        256       \n",
      "                                                                 \n",
      " activation_127 (Activation)  (None, 16, 16, 64)       0         \n",
      "                                                                 \n",
      " up_sampling2d_24 (UpSamplin  (None, 32, 32, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " deconv12 (Conv2DTranspose)  (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " bn25 (BatchNormalization)   (None, 32, 32, 64)        256       \n",
      "                                                                 \n",
      " activation_128 (Activation)  (None, 32, 32, 64)       0         \n",
      "                                                                 \n",
      " deconv13 (Conv2DTranspose)  (None, 32, 32, 1)         577       \n",
      "                                                                 \n",
      " bn26 (BatchNormalization)   (None, 32, 32, 1)         4         \n",
      "                                                                 \n",
      " activation_129 (Activation)  (None, 32, 32, 1)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,393,669\n",
      "Trainable params: 33,377,795\n",
      "Non-trainable params: 15,874\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the best hp.\n",
    "modelB = segnet(best_hpsB)\n",
    "modelB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 LOSS: 32.878429 2ND_METRIC: 4.593903\n",
      "EPOCH: 2 LOSS: 29.218323 2ND_METRIC: 4.344123\n",
      "EPOCH: 3 LOSS: 28.650436 2ND_METRIC: 4.317663\n",
      "EPOCH: 4 LOSS: 31.022423 2ND_METRIC: 4.473004\n",
      "EPOCH: 5 LOSS: 27.951824 2ND_METRIC: 4.288621\n",
      "EPOCH: 6 LOSS: 30.994556 2ND_METRIC: 4.512969\n",
      "EPOCH: 7 LOSS: 30.123137 2ND_METRIC: 4.492332\n",
      "EPOCH: 8 LOSS: 29.425310 2ND_METRIC: 4.422371\n",
      "EPOCH: 9 LOSS: 30.588726 2ND_METRIC: 4.525207\n",
      "EPOCH: 10 LOSS: 30.890709 2ND_METRIC: 4.503049\n",
      "EPOCH: 11 LOSS: 31.753479 2ND_METRIC: 4.676004\n",
      "EPOCH: 12 LOSS: 25.688025 2ND_METRIC: 4.199800\n",
      "EPOCH: 13 LOSS: 28.270807 2ND_METRIC: 4.344503\n",
      "EPOCH: 14 LOSS: 28.953213 2ND_METRIC: 4.381193\n",
      "EPOCH: 15 LOSS: 28.851311 2ND_METRIC: 4.422390\n",
      "EPOCH: 16 LOSS: 31.291927 2ND_METRIC: 4.678079\n",
      "EPOCH: 17 LOSS: 27.946194 2ND_METRIC: 4.375125\n",
      "EPOCH: 18 LOSS: 30.171730 2ND_METRIC: 4.544758\n",
      "EPOCH: 19 LOSS: 30.295307 2ND_METRIC: 4.535889\n",
      "EPOCH: 20 LOSS: 27.737835 2ND_METRIC: 4.311453\n",
      "EPOCH: 21 LOSS: 26.552975 2ND_METRIC: 4.227652\n",
      "EPOCH: 22 LOSS: 28.217030 2ND_METRIC: 4.428487\n",
      "EPOCH: 23 LOSS: 29.049110 2ND_METRIC: 4.537617\n",
      "EPOCH: 24 LOSS: 27.837233 2ND_METRIC: 4.359031\n",
      "EPOCH: 25 LOSS: 27.797840 2ND_METRIC: 4.370263\n",
      "EPOCH: 26 LOSS: 30.022465 2ND_METRIC: 4.558482\n",
      "EPOCH: 27 LOSS: 25.039097 2ND_METRIC: 4.140088\n",
      "EPOCH: 28 LOSS: 24.445734 2ND_METRIC: 4.085999\n",
      "EPOCH: 29 LOSS: 28.481831 2ND_METRIC: 4.442103\n",
      "EPOCH: 30 LOSS: 29.016844 2ND_METRIC: 4.509425\n",
      "EPOCH: 31 LOSS: 23.859602 2ND_METRIC: 4.051963\n",
      "EPOCH: 32 LOSS: 23.510818 2ND_METRIC: 4.006460\n",
      "EPOCH: 33 LOSS: 22.788294 2ND_METRIC: 3.949291\n",
      "EPOCH: 34 LOSS: 22.663708 2ND_METRIC: 3.965542\n",
      "EPOCH: 35 LOSS: 28.174614 2ND_METRIC: 4.421782\n",
      "EPOCH: 36 LOSS: 24.568108 2ND_METRIC: 4.102468\n",
      "EPOCH: 37 LOSS: 24.401842 2ND_METRIC: 4.059224\n",
      "EPOCH: 38 LOSS: 26.677525 2ND_METRIC: 4.284163\n",
      "EPOCH: 39 LOSS: 24.567778 2ND_METRIC: 4.120015\n",
      "EPOCH: 40 LOSS: 26.009987 2ND_METRIC: 4.298730\n",
      "EPOCH: 41 LOSS: 22.978779 2ND_METRIC: 4.010402\n",
      "EPOCH: 42 LOSS: 25.897842 2ND_METRIC: 4.223286\n",
      "EPOCH: 43 LOSS: 27.135586 2ND_METRIC: 4.372836\n",
      "EPOCH: 44 LOSS: 23.676716 2ND_METRIC: 4.013371\n",
      "EPOCH: 45 LOSS: 23.594601 2ND_METRIC: 4.071856\n",
      "EPOCH: 46 LOSS: 24.616251 2ND_METRIC: 4.159223\n",
      "EPOCH: 47 LOSS: 24.444508 2ND_METRIC: 4.158412\n",
      "EPOCH: 48 LOSS: 24.627148 2ND_METRIC: 4.121439\n",
      "EPOCH: 49 LOSS: 23.341808 2ND_METRIC: 4.059248\n",
      "EPOCH: 50 LOSS: 26.159779 2ND_METRIC: 4.303356\n",
      "EPOCH: 51 LOSS: 24.507931 2ND_METRIC: 4.144213\n",
      "EPOCH: 52 LOSS: 21.692890 2ND_METRIC: 3.826165\n",
      "EPOCH: 53 LOSS: 23.040855 2ND_METRIC: 4.015673\n",
      "EPOCH: 54 LOSS: 24.923210 2ND_METRIC: 4.162955\n",
      "EPOCH: 55 LOSS: 23.536760 2ND_METRIC: 4.077139\n",
      "EPOCH: 56 LOSS: 25.550417 2ND_METRIC: 4.243203\n",
      "EPOCH: 57 LOSS: 22.581821 2ND_METRIC: 3.964140\n",
      "EPOCH: 58 LOSS: 22.297953 2ND_METRIC: 3.984690\n",
      "EPOCH: 59 LOSS: 24.961401 2ND_METRIC: 4.195657\n",
      "EPOCH: 60 LOSS: 21.687920 2ND_METRIC: 3.913321\n",
      "EPOCH: 61 LOSS: 22.087532 2ND_METRIC: 3.903890\n",
      "EPOCH: 62 LOSS: 19.578775 2ND_METRIC: 3.739492\n",
      "EPOCH: 63 LOSS: 24.456362 2ND_METRIC: 4.173767\n",
      "EPOCH: 64 LOSS: 22.931202 2ND_METRIC: 4.004560\n",
      "EPOCH: 65 LOSS: 22.643394 2ND_METRIC: 3.959390\n",
      "EPOCH: 66 LOSS: 23.570971 2ND_METRIC: 4.108374\n",
      "EPOCH: 67 LOSS: 19.758101 2ND_METRIC: 3.771606\n",
      "EPOCH: 68 LOSS: 22.683029 2ND_METRIC: 4.042072\n",
      "EPOCH: 69 LOSS: 24.041882 2ND_METRIC: 4.071136\n",
      "EPOCH: 70 LOSS: 22.185431 2ND_METRIC: 3.966473\n",
      "EPOCH: 71 LOSS: 23.661331 2ND_METRIC: 4.089074\n",
      "EPOCH: 72 LOSS: 20.208237 2ND_METRIC: 3.798207\n",
      "EPOCH: 73 LOSS: 22.018955 2ND_METRIC: 3.980993\n",
      "EPOCH: 74 LOSS: 21.194998 2ND_METRIC: 3.826782\n",
      "EPOCH: 75 LOSS: 21.826366 2ND_METRIC: 3.927926\n",
      "EPOCH: 76 LOSS: 20.582565 2ND_METRIC: 3.850020\n",
      "EPOCH: 77 LOSS: 21.237040 2ND_METRIC: 3.892901\n",
      "EPOCH: 78 LOSS: 22.074333 2ND_METRIC: 3.959686\n",
      "EPOCH: 79 LOSS: 20.218807 2ND_METRIC: 3.798623\n",
      "EPOCH: 80 LOSS: 20.986996 2ND_METRIC: 3.863306\n",
      "EPOCH: 81 LOSS: 19.068314 2ND_METRIC: 3.645971\n",
      "EPOCH: 82 LOSS: 23.146017 2ND_METRIC: 4.084216\n",
      "EPOCH: 83 LOSS: 20.959660 2ND_METRIC: 3.871144\n",
      "EPOCH: 84 LOSS: 19.453548 2ND_METRIC: 3.680198\n",
      "EPOCH: 85 LOSS: 19.641647 2ND_METRIC: 3.740108\n",
      "EPOCH: 86 LOSS: 20.001144 2ND_METRIC: 3.726012\n",
      "EPOCH: 87 LOSS: 24.056313 2ND_METRIC: 4.124436\n",
      "EPOCH: 88 LOSS: 20.083467 2ND_METRIC: 3.797091\n",
      "EPOCH: 89 LOSS: 19.424976 2ND_METRIC: 3.705524\n",
      "EPOCH: 90 LOSS: 19.359367 2ND_METRIC: 3.690074\n",
      "EPOCH: 91 LOSS: 19.242466 2ND_METRIC: 3.702158\n",
      "EPOCH: 92 LOSS: 18.423912 2ND_METRIC: 3.622533\n",
      "EPOCH: 93 LOSS: 20.518723 2ND_METRIC: 3.803088\n",
      "EPOCH: 94 LOSS: 17.404099 2ND_METRIC: 3.553913\n",
      "EPOCH: 95 LOSS: 17.458637 2ND_METRIC: 3.512799\n",
      "EPOCH: 96 LOSS: 22.699528 2ND_METRIC: 4.069414\n",
      "EPOCH: 97 LOSS: 20.428017 2ND_METRIC: 3.823027\n",
      "EPOCH: 98 LOSS: 17.971546 2ND_METRIC: 3.600407\n",
      "EPOCH: 99 LOSS: 20.105347 2ND_METRIC: 3.826849\n",
      "EPOCH: 100 LOSS: 19.049538 2ND_METRIC: 3.712223\n",
      "EPOCH: 101 LOSS: 19.661102 2ND_METRIC: 3.779508\n",
      "EPOCH: 102 LOSS: 18.154842 2ND_METRIC: 3.616832\n",
      "EPOCH: 103 LOSS: 18.597815 2ND_METRIC: 3.596874\n",
      "EPOCH: 104 LOSS: 19.144917 2ND_METRIC: 3.713196\n",
      "EPOCH: 105 LOSS: 17.935078 2ND_METRIC: 3.593379\n",
      "EPOCH: 106 LOSS: 18.906593 2ND_METRIC: 3.659109\n",
      "EPOCH: 107 LOSS: 18.627951 2ND_METRIC: 3.591402\n",
      "EPOCH: 108 LOSS: 20.015829 2ND_METRIC: 3.848546\n",
      "EPOCH: 109 LOSS: 17.619139 2ND_METRIC: 3.538094\n",
      "EPOCH: 110 LOSS: 18.793884 2ND_METRIC: 3.677223\n",
      "EPOCH: 111 LOSS: 20.345016 2ND_METRIC: 3.827531\n",
      "EPOCH: 112 LOSS: 18.726368 2ND_METRIC: 3.701548\n",
      "EPOCH: 113 LOSS: 17.674923 2ND_METRIC: 3.563959\n",
      "EPOCH: 114 LOSS: 18.627655 2ND_METRIC: 3.643028\n",
      "EPOCH: 115 LOSS: 16.854933 2ND_METRIC: 3.498621\n",
      "EPOCH: 116 LOSS: 19.195599 2ND_METRIC: 3.696412\n",
      "EPOCH: 117 LOSS: 19.294731 2ND_METRIC: 3.703664\n",
      "EPOCH: 118 LOSS: 16.896606 2ND_METRIC: 3.481104\n",
      "EPOCH: 119 LOSS: 17.722725 2ND_METRIC: 3.591202\n",
      "EPOCH: 120 LOSS: 17.282898 2ND_METRIC: 3.556000\n",
      "EPOCH: 121 LOSS: 19.321789 2ND_METRIC: 3.742823\n",
      "EPOCH: 122 LOSS: 16.550581 2ND_METRIC: 3.462421\n",
      "EPOCH: 123 LOSS: 20.466661 2ND_METRIC: 3.869051\n",
      "EPOCH: 124 LOSS: 21.207434 2ND_METRIC: 3.855118\n",
      "EPOCH: 125 LOSS: 18.052692 2ND_METRIC: 3.589718\n",
      "EPOCH: 126 LOSS: 16.497355 2ND_METRIC: 3.462877\n",
      "EPOCH: 127 LOSS: 15.265863 2ND_METRIC: 3.327089\n",
      "EPOCH: 128 LOSS: 15.820956 2ND_METRIC: 3.406637\n",
      "EPOCH: 129 LOSS: 15.903522 2ND_METRIC: 3.386574\n",
      "EPOCH: 130 LOSS: 14.430927 2ND_METRIC: 3.249496\n",
      "EPOCH: 131 LOSS: 16.835392 2ND_METRIC: 3.541136\n",
      "EPOCH: 132 LOSS: 17.126501 2ND_METRIC: 3.531419\n",
      "EPOCH: 133 LOSS: 18.431784 2ND_METRIC: 3.666530\n",
      "EPOCH: 134 LOSS: 15.766806 2ND_METRIC: 3.433505\n",
      "EPOCH: 135 LOSS: 15.593279 2ND_METRIC: 3.390459\n",
      "EPOCH: 136 LOSS: 18.170662 2ND_METRIC: 3.692785\n",
      "EPOCH: 137 LOSS: 16.651974 2ND_METRIC: 3.506520\n",
      "EPOCH: 138 LOSS: 16.658440 2ND_METRIC: 3.500942\n",
      "EPOCH: 139 LOSS: 17.401360 2ND_METRIC: 3.591842\n",
      "EPOCH: 140 LOSS: 18.321592 2ND_METRIC: 3.641982\n",
      "EPOCH: 141 LOSS: 16.479532 2ND_METRIC: 3.476206\n",
      "EPOCH: 142 LOSS: 13.882799 2ND_METRIC: 3.205020\n",
      "EPOCH: 143 LOSS: 15.884212 2ND_METRIC: 3.432360\n",
      "EPOCH: 144 LOSS: 16.158941 2ND_METRIC: 3.414951\n",
      "EPOCH: 145 LOSS: 16.405254 2ND_METRIC: 3.455539\n",
      "EPOCH: 146 LOSS: 18.625206 2ND_METRIC: 3.739579\n",
      "EPOCH: 147 LOSS: 15.498629 2ND_METRIC: 3.378881\n",
      "EPOCH: 148 LOSS: 14.798735 2ND_METRIC: 3.316329\n",
      "EPOCH: 149 LOSS: 13.550568 2ND_METRIC: 3.135232\n",
      "EPOCH: 150 LOSS: 15.293811 2ND_METRIC: 3.325070\n",
      "EPOCH: 151 LOSS: 15.797793 2ND_METRIC: 3.406815\n",
      "EPOCH: 152 LOSS: 16.948933 2ND_METRIC: 3.506934\n",
      "EPOCH: 153 LOSS: 15.734291 2ND_METRIC: 3.402587\n",
      "EPOCH: 154 LOSS: 15.696363 2ND_METRIC: 3.387238\n",
      "EPOCH: 155 LOSS: 15.124388 2ND_METRIC: 3.333523\n",
      "EPOCH: 156 LOSS: 14.023743 2ND_METRIC: 3.174080\n",
      "EPOCH: 157 LOSS: 14.635365 2ND_METRIC: 3.265067\n",
      "EPOCH: 158 LOSS: 13.360496 2ND_METRIC: 3.097881\n",
      "EPOCH: 159 LOSS: 14.644211 2ND_METRIC: 3.234690\n",
      "EPOCH: 160 LOSS: 15.019116 2ND_METRIC: 3.305274\n",
      "EPOCH: 161 LOSS: 14.931193 2ND_METRIC: 3.247710\n",
      "EPOCH: 162 LOSS: 13.139747 2ND_METRIC: 3.089142\n",
      "EPOCH: 163 LOSS: 14.937701 2ND_METRIC: 3.303532\n",
      "EPOCH: 164 LOSS: 16.110298 2ND_METRIC: 3.429565\n",
      "EPOCH: 165 LOSS: 14.852302 2ND_METRIC: 3.310717\n",
      "EPOCH: 166 LOSS: 14.702990 2ND_METRIC: 3.277562\n",
      "EPOCH: 167 LOSS: 13.690676 2ND_METRIC: 3.171266\n",
      "EPOCH: 168 LOSS: 14.632095 2ND_METRIC: 3.258196\n",
      "EPOCH: 169 LOSS: 13.064409 2ND_METRIC: 3.079968\n",
      "EPOCH: 170 LOSS: 14.839764 2ND_METRIC: 3.289992\n",
      "EPOCH: 171 LOSS: 14.520337 2ND_METRIC: 3.262404\n",
      "EPOCH: 172 LOSS: 13.206810 2ND_METRIC: 3.140020\n",
      "EPOCH: 173 LOSS: 13.539917 2ND_METRIC: 3.100047\n",
      "EPOCH: 174 LOSS: 14.632847 2ND_METRIC: 3.254184\n",
      "EPOCH: 175 LOSS: 13.485664 2ND_METRIC: 3.115499\n",
      "EPOCH: 176 LOSS: 13.451313 2ND_METRIC: 3.099530\n",
      "EPOCH: 177 LOSS: 14.017053 2ND_METRIC: 3.182165\n",
      "EPOCH: 178 LOSS: 13.290918 2ND_METRIC: 3.096533\n",
      "EPOCH: 179 LOSS: 14.773788 2ND_METRIC: 3.303492\n",
      "EPOCH: 180 LOSS: 15.299071 2ND_METRIC: 3.289022\n",
      "EPOCH: 181 LOSS: 13.922044 2ND_METRIC: 3.194457\n",
      "EPOCH: 182 LOSS: 14.121756 2ND_METRIC: 3.218867\n",
      "EPOCH: 183 LOSS: 14.378979 2ND_METRIC: 3.225663\n",
      "EPOCH: 184 LOSS: 13.964195 2ND_METRIC: 3.203855\n",
      "EPOCH: 185 LOSS: 14.476877 2ND_METRIC: 3.267748\n",
      "EPOCH: 186 LOSS: 13.468649 2ND_METRIC: 3.107486\n",
      "EPOCH: 187 LOSS: 13.413592 2ND_METRIC: 3.133014\n",
      "EPOCH: 188 LOSS: 12.642447 2ND_METRIC: 3.029081\n",
      "EPOCH: 189 LOSS: 11.837496 2ND_METRIC: 2.913948\n",
      "EPOCH: 190 LOSS: 13.346441 2ND_METRIC: 3.107366\n",
      "EPOCH: 191 LOSS: 14.099957 2ND_METRIC: 3.172044\n",
      "EPOCH: 192 LOSS: 13.142300 2ND_METRIC: 3.117172\n",
      "EPOCH: 193 LOSS: 13.455132 2ND_METRIC: 3.109600\n",
      "EPOCH: 194 LOSS: 14.917534 2ND_METRIC: 3.362404\n",
      "EPOCH: 195 LOSS: 13.438028 2ND_METRIC: 3.096333\n",
      "EPOCH: 196 LOSS: 12.812834 2ND_METRIC: 3.050917\n",
      "EPOCH: 197 LOSS: 13.221898 2ND_METRIC: 3.115205\n",
      "EPOCH: 198 LOSS: 13.406115 2ND_METRIC: 3.118718\n",
      "EPOCH: 199 LOSS: 13.112207 2ND_METRIC: 3.108064\n",
      "EPOCH: 200 LOSS: 12.402740 2ND_METRIC: 3.033021\n",
      "EPOCH: 201 LOSS: 13.520250 2ND_METRIC: 3.106754\n",
      "EPOCH: 202 LOSS: 12.569641 2ND_METRIC: 3.002978\n",
      "EPOCH: 203 LOSS: 12.247625 2ND_METRIC: 2.981433\n",
      "EPOCH: 204 LOSS: 14.470720 2ND_METRIC: 3.298376\n",
      "EPOCH: 205 LOSS: 12.514790 2ND_METRIC: 3.015410\n",
      "EPOCH: 206 LOSS: 12.424757 2ND_METRIC: 2.993870\n",
      "EPOCH: 207 LOSS: 12.186169 2ND_METRIC: 2.985673\n",
      "EPOCH: 208 LOSS: 11.991861 2ND_METRIC: 2.924734\n",
      "EPOCH: 209 LOSS: 12.151941 2ND_METRIC: 2.977228\n",
      "EPOCH: 210 LOSS: 12.659420 2ND_METRIC: 3.032759\n",
      "EPOCH: 211 LOSS: 12.112945 2ND_METRIC: 2.969799\n",
      "EPOCH: 212 LOSS: 12.906136 2ND_METRIC: 3.092584\n",
      "EPOCH: 213 LOSS: 13.891503 2ND_METRIC: 3.177432\n",
      "EPOCH: 214 LOSS: 10.920297 2ND_METRIC: 2.816519\n",
      "EPOCH: 215 LOSS: 12.355599 2ND_METRIC: 2.979561\n",
      "EPOCH: 216 LOSS: 11.747379 2ND_METRIC: 2.917629\n",
      "EPOCH: 217 LOSS: 13.105410 2ND_METRIC: 3.108358\n",
      "EPOCH: 218 LOSS: 11.610899 2ND_METRIC: 2.875815\n",
      "EPOCH: 219 LOSS: 12.137550 2ND_METRIC: 2.946229\n",
      "EPOCH: 220 LOSS: 12.186680 2ND_METRIC: 2.978088\n",
      "EPOCH: 221 LOSS: 13.122017 2ND_METRIC: 3.093884\n",
      "EPOCH: 222 LOSS: 12.057512 2ND_METRIC: 2.975082\n",
      "EPOCH: 223 LOSS: 12.120656 2ND_METRIC: 2.987407\n",
      "EPOCH: 224 LOSS: 11.392378 2ND_METRIC: 2.852597\n",
      "EPOCH: 225 LOSS: 13.594764 2ND_METRIC: 3.199233\n",
      "EPOCH: 226 LOSS: 13.357002 2ND_METRIC: 3.162592\n",
      "EPOCH: 227 LOSS: 12.901897 2ND_METRIC: 3.054799\n",
      "EPOCH: 228 LOSS: 10.694651 2ND_METRIC: 2.743511\n",
      "EPOCH: 229 LOSS: 11.958469 2ND_METRIC: 2.951446\n",
      "EPOCH: 230 LOSS: 12.628983 2ND_METRIC: 3.025302\n",
      "EPOCH: 231 LOSS: 13.454717 2ND_METRIC: 3.129267\n",
      "EPOCH: 232 LOSS: 13.077047 2ND_METRIC: 3.120981\n",
      "EPOCH: 233 LOSS: 11.091419 2ND_METRIC: 2.839488\n",
      "EPOCH: 234 LOSS: 11.651335 2ND_METRIC: 2.930145\n",
      "EPOCH: 235 LOSS: 12.702392 2ND_METRIC: 3.032638\n",
      "EPOCH: 236 LOSS: 11.364988 2ND_METRIC: 2.865781\n",
      "EPOCH: 237 LOSS: 10.925520 2ND_METRIC: 2.817474\n",
      "EPOCH: 238 LOSS: 13.018648 2ND_METRIC: 3.099128\n",
      "EPOCH: 239 LOSS: 11.273210 2ND_METRIC: 2.850457\n",
      "EPOCH: 240 LOSS: 11.999283 2ND_METRIC: 2.974417\n",
      "EPOCH: 241 LOSS: 11.724011 2ND_METRIC: 2.935492\n",
      "EPOCH: 242 LOSS: 12.088071 2ND_METRIC: 2.982170\n",
      "EPOCH: 243 LOSS: 12.518622 2ND_METRIC: 3.040517\n",
      "EPOCH: 244 LOSS: 11.895477 2ND_METRIC: 2.943890\n",
      "EPOCH: 245 LOSS: 12.087305 2ND_METRIC: 3.005662\n",
      "EPOCH: 246 LOSS: 12.831764 2ND_METRIC: 3.075864\n",
      "EPOCH: 247 LOSS: 11.826628 2ND_METRIC: 2.941212\n",
      "EPOCH: 248 LOSS: 11.317687 2ND_METRIC: 2.837869\n",
      "EPOCH: 249 LOSS: 12.406240 2ND_METRIC: 3.030028\n",
      "EPOCH: 250 LOSS: 11.404620 2ND_METRIC: 2.914823\n",
      "EPOCH: 251 LOSS: 12.419484 2ND_METRIC: 3.045722\n",
      "EPOCH: 252 LOSS: 13.516977 2ND_METRIC: 3.183572\n",
      "EPOCH: 253 LOSS: 11.482025 2ND_METRIC: 2.875216\n",
      "EPOCH: 254 LOSS: 11.771497 2ND_METRIC: 2.940008\n",
      "EPOCH: 255 LOSS: 11.881645 2ND_METRIC: 2.950874\n",
      "EPOCH: 256 LOSS: 12.598708 2ND_METRIC: 3.044202\n",
      "EPOCH: 257 LOSS: 11.685434 2ND_METRIC: 2.941112\n",
      "EPOCH: 258 LOSS: 12.412217 2ND_METRIC: 3.055752\n",
      "EPOCH: 259 LOSS: 10.932883 2ND_METRIC: 2.814121\n",
      "EPOCH: 260 LOSS: 12.367548 2ND_METRIC: 3.068565\n",
      "EPOCH: 261 LOSS: 13.611802 2ND_METRIC: 3.218632\n",
      "EPOCH: 262 LOSS: 12.143126 2ND_METRIC: 2.996909\n",
      "EPOCH: 263 LOSS: 11.450493 2ND_METRIC: 2.873628\n",
      "EPOCH: 264 LOSS: 12.273811 2ND_METRIC: 3.002316\n",
      "EPOCH: 265 LOSS: 12.720963 2ND_METRIC: 3.062020\n",
      "EPOCH: 266 LOSS: 12.706766 2ND_METRIC: 3.088761\n",
      "EPOCH: 267 LOSS: 12.874207 2ND_METRIC: 3.102039\n",
      "EPOCH: 268 LOSS: 12.304209 2ND_METRIC: 3.024305\n",
      "EPOCH: 269 LOSS: 12.507496 2ND_METRIC: 3.037862\n",
      "EPOCH: 270 LOSS: 10.850132 2ND_METRIC: 2.788920\n",
      "EPOCH: 271 LOSS: 11.113634 2ND_METRIC: 2.825746\n",
      "EPOCH: 272 LOSS: 12.395732 2ND_METRIC: 3.052977\n",
      "EPOCH: 273 LOSS: 13.311373 2ND_METRIC: 3.197095\n",
      "EPOCH: 274 LOSS: 10.473858 2ND_METRIC: 2.754981\n",
      "EPOCH: 275 LOSS: 11.911144 2ND_METRIC: 2.937934\n",
      "EPOCH: 276 LOSS: 11.323565 2ND_METRIC: 2.877182\n",
      "EPOCH: 277 LOSS: 12.360666 2ND_METRIC: 3.053645\n",
      "EPOCH: 278 LOSS: 12.129974 2ND_METRIC: 2.984076\n",
      "EPOCH: 279 LOSS: 10.389410 2ND_METRIC: 2.717972\n",
      "EPOCH: 280 LOSS: 12.043089 2ND_METRIC: 2.993600\n",
      "EPOCH: 281 LOSS: 11.621937 2ND_METRIC: 2.931607\n",
      "EPOCH: 282 LOSS: 12.030260 2ND_METRIC: 3.000495\n",
      "EPOCH: 283 LOSS: 11.868493 2ND_METRIC: 2.986232\n",
      "EPOCH: 284 LOSS: 11.764126 2ND_METRIC: 2.958115\n",
      "EPOCH: 285 LOSS: 12.393326 2ND_METRIC: 3.053446\n",
      "EPOCH: 286 LOSS: 11.992992 2ND_METRIC: 2.912622\n",
      "EPOCH: 287 LOSS: 12.473907 2ND_METRIC: 3.112925\n",
      "EPOCH: 288 LOSS: 10.932159 2ND_METRIC: 2.817994\n",
      "EPOCH: 289 LOSS: 11.262275 2ND_METRIC: 2.918343\n",
      "EPOCH: 290 LOSS: 11.476718 2ND_METRIC: 2.939981\n",
      "EPOCH: 291 LOSS: 11.221996 2ND_METRIC: 2.852869\n",
      "EPOCH: 292 LOSS: 11.570560 2ND_METRIC: 2.921317\n",
      "EPOCH: 293 LOSS: 11.899794 2ND_METRIC: 2.952912\n",
      "EPOCH: 294 LOSS: 11.472513 2ND_METRIC: 2.878438\n",
      "EPOCH: 295 LOSS: 11.013161 2ND_METRIC: 2.838275\n",
      "EPOCH: 296 LOSS: 11.221124 2ND_METRIC: 2.865880\n",
      "EPOCH: 297 LOSS: 10.521830 2ND_METRIC: 2.717546\n",
      "EPOCH: 298 LOSS: 12.547855 2ND_METRIC: 3.048523\n",
      "EPOCH: 299 LOSS: 11.504631 2ND_METRIC: 2.888834\n",
      "EPOCH: 300 LOSS: 11.649744 2ND_METRIC: 2.891482\n",
      "EPOCH: 301 LOSS: 11.799439 2ND_METRIC: 2.954490\n",
      "EPOCH: 302 LOSS: 11.647110 2ND_METRIC: 2.909509\n",
      "EPOCH: 303 LOSS: 12.240267 2ND_METRIC: 3.005740\n",
      "EPOCH: 304 LOSS: 12.043370 2ND_METRIC: 3.016908\n",
      "EPOCH: 305 LOSS: 10.662291 2ND_METRIC: 2.721917\n",
      "EPOCH: 306 LOSS: 11.015139 2ND_METRIC: 2.790215\n",
      "EPOCH: 307 LOSS: 10.147322 2ND_METRIC: 2.674432\n",
      "EPOCH: 308 LOSS: 10.816176 2ND_METRIC: 2.798322\n",
      "EPOCH: 309 LOSS: 10.839542 2ND_METRIC: 2.809360\n",
      "EPOCH: 310 LOSS: 12.291372 2ND_METRIC: 2.972520\n",
      "EPOCH: 311 LOSS: 10.697525 2ND_METRIC: 2.785964\n",
      "EPOCH: 312 LOSS: 11.021694 2ND_METRIC: 2.804989\n",
      "EPOCH: 313 LOSS: 10.772242 2ND_METRIC: 2.727130\n",
      "EPOCH: 314 LOSS: 9.350229 2ND_METRIC: 2.514508\n",
      "EPOCH: 315 LOSS: 11.355605 2ND_METRIC: 2.863831\n",
      "EPOCH: 316 LOSS: 11.378764 2ND_METRIC: 2.880663\n",
      "EPOCH: 317 LOSS: 11.783997 2ND_METRIC: 2.954313\n",
      "EPOCH: 318 LOSS: 11.830235 2ND_METRIC: 2.958114\n",
      "EPOCH: 319 LOSS: 10.749851 2ND_METRIC: 2.776694\n",
      "EPOCH: 320 LOSS: 10.888624 2ND_METRIC: 2.712387\n",
      "EPOCH: 321 LOSS: 11.287783 2ND_METRIC: 2.804615\n",
      "EPOCH: 322 LOSS: 10.356179 2ND_METRIC: 2.638313\n",
      "EPOCH: 323 LOSS: 9.919691 2ND_METRIC: 2.594700\n",
      "EPOCH: 324 LOSS: 11.300237 2ND_METRIC: 2.808344\n",
      "EPOCH: 325 LOSS: 11.024872 2ND_METRIC: 2.824914\n",
      "EPOCH: 326 LOSS: 12.178091 2ND_METRIC: 2.967815\n",
      "EPOCH: 327 LOSS: 11.948532 2ND_METRIC: 2.995135\n",
      "EPOCH: 328 LOSS: 10.519789 2ND_METRIC: 2.699965\n",
      "EPOCH: 329 LOSS: 10.491205 2ND_METRIC: 2.708064\n",
      "EPOCH: 330 LOSS: 10.092797 2ND_METRIC: 2.575084\n",
      "EPOCH: 331 LOSS: 11.486283 2ND_METRIC: 2.881160\n",
      "EPOCH: 332 LOSS: 10.788721 2ND_METRIC: 2.809689\n",
      "EPOCH: 333 LOSS: 10.240810 2ND_METRIC: 2.647124\n",
      "EPOCH: 334 LOSS: 11.298925 2ND_METRIC: 2.871088\n",
      "EPOCH: 335 LOSS: 11.782123 2ND_METRIC: 2.920883\n",
      "EPOCH: 336 LOSS: 11.417807 2ND_METRIC: 2.852774\n",
      "EPOCH: 337 LOSS: 10.939942 2ND_METRIC: 2.793455\n",
      "EPOCH: 338 LOSS: 10.528502 2ND_METRIC: 2.693138\n",
      "EPOCH: 339 LOSS: 11.826321 2ND_METRIC: 2.928738\n",
      "EPOCH: 340 LOSS: 11.108011 2ND_METRIC: 2.780581\n",
      "EPOCH: 341 LOSS: 11.609271 2ND_METRIC: 2.899371\n",
      "EPOCH: 342 LOSS: 11.858786 2ND_METRIC: 2.945215\n",
      "EPOCH: 343 LOSS: 11.233738 2ND_METRIC: 2.833277\n",
      "EPOCH: 344 LOSS: 10.904605 2ND_METRIC: 2.730342\n",
      "EPOCH: 345 LOSS: 10.920269 2ND_METRIC: 2.762908\n",
      "EPOCH: 346 LOSS: 10.502155 2ND_METRIC: 2.696415\n",
      "EPOCH: 347 LOSS: 11.147985 2ND_METRIC: 2.776260\n",
      "EPOCH: 348 LOSS: 11.688980 2ND_METRIC: 2.910618\n",
      "EPOCH: 349 LOSS: 11.336851 2ND_METRIC: 2.814529\n",
      "EPOCH: 350 LOSS: 10.338144 2ND_METRIC: 2.688714\n",
      "EPOCH: 351 LOSS: 11.515077 2ND_METRIC: 2.824623\n",
      "EPOCH: 352 LOSS: 12.339897 2ND_METRIC: 2.976062\n",
      "EPOCH: 353 LOSS: 10.667428 2ND_METRIC: 2.755473\n",
      "EPOCH: 354 LOSS: 10.411542 2ND_METRIC: 2.644625\n",
      "EPOCH: 355 LOSS: 10.287410 2ND_METRIC: 2.618221\n",
      "EPOCH: 356 LOSS: 10.404468 2ND_METRIC: 2.683175\n",
      "EPOCH: 357 LOSS: 10.326307 2ND_METRIC: 2.655664\n",
      "EPOCH: 358 LOSS: 10.832808 2ND_METRIC: 2.716040\n",
      "EPOCH: 359 LOSS: 10.748413 2ND_METRIC: 2.672329\n",
      "EPOCH: 360 LOSS: 10.092073 2ND_METRIC: 2.575518\n",
      "EPOCH: 361 LOSS: 9.605230 2ND_METRIC: 2.492610\n",
      "EPOCH: 362 LOSS: 10.647094 2ND_METRIC: 2.654006\n",
      "EPOCH: 363 LOSS: 9.948856 2ND_METRIC: 2.558532\n",
      "EPOCH: 364 LOSS: 10.612398 2ND_METRIC: 2.711612\n",
      "EPOCH: 365 LOSS: 10.379879 2ND_METRIC: 2.634823\n",
      "EPOCH: 366 LOSS: 11.085076 2ND_METRIC: 2.755681\n",
      "EPOCH: 367 LOSS: 10.583469 2ND_METRIC: 2.657002\n",
      "EPOCH: 368 LOSS: 11.022353 2ND_METRIC: 2.786070\n",
      "EPOCH: 369 LOSS: 10.518246 2ND_METRIC: 2.657222\n",
      "EPOCH: 370 LOSS: 11.483563 2ND_METRIC: 2.877731\n",
      "EPOCH: 371 LOSS: 11.209751 2ND_METRIC: 2.833050\n",
      "EPOCH: 372 LOSS: 10.388742 2ND_METRIC: 2.670352\n",
      "EPOCH: 373 LOSS: 10.945889 2ND_METRIC: 2.748799\n",
      "EPOCH: 374 LOSS: 11.335164 2ND_METRIC: 2.828436\n",
      "EPOCH: 375 LOSS: 10.504004 2ND_METRIC: 2.651843\n",
      "EPOCH: 376 LOSS: 10.721963 2ND_METRIC: 2.743452\n",
      "EPOCH: 377 LOSS: 10.692475 2ND_METRIC: 2.704921\n",
      "EPOCH: 378 LOSS: 11.129747 2ND_METRIC: 2.784235\n",
      "EPOCH: 379 LOSS: 10.984237 2ND_METRIC: 2.751060\n",
      "EPOCH: 380 LOSS: 10.274979 2ND_METRIC: 2.645163\n",
      "EPOCH: 381 LOSS: 10.399351 2ND_METRIC: 2.652549\n",
      "EPOCH: 382 LOSS: 11.907114 2ND_METRIC: 2.910199\n",
      "EPOCH: 383 LOSS: 11.412035 2ND_METRIC: 2.861773\n",
      "EPOCH: 384 LOSS: 11.253471 2ND_METRIC: 2.771889\n",
      "EPOCH: 385 LOSS: 11.135591 2ND_METRIC: 2.799990\n",
      "EPOCH: 386 LOSS: 10.106346 2ND_METRIC: 2.563941\n",
      "EPOCH: 387 LOSS: 12.009090 2ND_METRIC: 2.937287\n",
      "EPOCH: 388 LOSS: 11.555641 2ND_METRIC: 2.850002\n",
      "EPOCH: 389 LOSS: 11.643264 2ND_METRIC: 2.856452\n",
      "EPOCH: 390 LOSS: 10.479582 2ND_METRIC: 2.708283\n",
      "EPOCH: 391 LOSS: 11.347196 2ND_METRIC: 2.795712\n",
      "EPOCH: 392 LOSS: 10.663408 2ND_METRIC: 2.746196\n",
      "EPOCH: 393 LOSS: 11.289355 2ND_METRIC: 2.803086\n",
      "EPOCH: 394 LOSS: 11.322102 2ND_METRIC: 2.857081\n",
      "EPOCH: 395 LOSS: 10.981472 2ND_METRIC: 2.718244\n",
      "EPOCH: 396 LOSS: 11.620283 2ND_METRIC: 2.916499\n",
      "EPOCH: 397 LOSS: 10.728901 2ND_METRIC: 2.703255\n",
      "EPOCH: 398 LOSS: 11.717146 2ND_METRIC: 2.947328\n",
      "EPOCH: 399 LOSS: 10.392766 2ND_METRIC: 2.651137\n",
      "EPOCH: 400 LOSS: 10.591906 2ND_METRIC: 2.695175\n",
      "EPOCH: 401 LOSS: 10.884774 2ND_METRIC: 2.751143\n",
      "EPOCH: 402 LOSS: 12.151844 2ND_METRIC: 3.001347\n",
      "EPOCH: 403 LOSS: 10.494682 2ND_METRIC: 2.664146\n",
      "EPOCH: 404 LOSS: 9.617446 2ND_METRIC: 2.486435\n",
      "EPOCH: 405 LOSS: 10.384804 2ND_METRIC: 2.629788\n",
      "EPOCH: 406 LOSS: 10.698975 2ND_METRIC: 2.741401\n",
      "EPOCH: 407 LOSS: 11.508315 2ND_METRIC: 2.872752\n",
      "EPOCH: 408 LOSS: 11.275413 2ND_METRIC: 2.796795\n",
      "EPOCH: 409 LOSS: 11.367239 2ND_METRIC: 2.779346\n",
      "EPOCH: 410 LOSS: 10.345762 2ND_METRIC: 2.642249\n",
      "EPOCH: 411 LOSS: 10.273554 2ND_METRIC: 2.585159\n",
      "EPOCH: 412 LOSS: 10.786427 2ND_METRIC: 2.728374\n",
      "EPOCH: 413 LOSS: 11.220403 2ND_METRIC: 2.778930\n",
      "EPOCH: 414 LOSS: 11.444038 2ND_METRIC: 2.885625\n",
      "EPOCH: 415 LOSS: 10.669792 2ND_METRIC: 2.684001\n",
      "EPOCH: 416 LOSS: 11.109709 2ND_METRIC: 2.772254\n",
      "EPOCH: 417 LOSS: 10.975386 2ND_METRIC: 2.719703\n",
      "EPOCH: 418 LOSS: 10.510867 2ND_METRIC: 2.603032\n",
      "EPOCH: 419 LOSS: 11.350063 2ND_METRIC: 2.781311\n",
      "EPOCH: 420 LOSS: 10.999540 2ND_METRIC: 2.725562\n",
      "EPOCH: 421 LOSS: 11.221830 2ND_METRIC: 2.805032\n",
      "EPOCH: 422 LOSS: 11.117759 2ND_METRIC: 2.794113\n",
      "EPOCH: 423 LOSS: 11.706974 2ND_METRIC: 2.852494\n",
      "EPOCH: 424 LOSS: 11.362995 2ND_METRIC: 2.829884\n",
      "EPOCH: 425 LOSS: 11.664747 2ND_METRIC: 2.868177\n",
      "EPOCH: 426 LOSS: 10.680683 2ND_METRIC: 2.726815\n",
      "EPOCH: 427 LOSS: 10.405415 2ND_METRIC: 2.655481\n",
      "EPOCH: 428 LOSS: 11.295914 2ND_METRIC: 2.792846\n",
      "EPOCH: 429 LOSS: 11.408453 2ND_METRIC: 2.795497\n",
      "EPOCH: 430 LOSS: 9.859953 2ND_METRIC: 2.585353\n",
      "EPOCH: 431 LOSS: 10.851290 2ND_METRIC: 2.727942\n",
      "EPOCH: 432 LOSS: 10.129606 2ND_METRIC: 2.579769\n",
      "EPOCH: 433 LOSS: 11.157228 2ND_METRIC: 2.809086\n",
      "EPOCH: 434 LOSS: 10.515179 2ND_METRIC: 2.708450\n",
      "EPOCH: 435 LOSS: 10.209793 2ND_METRIC: 2.549703\n",
      "EPOCH: 436 LOSS: 10.842529 2ND_METRIC: 2.713290\n",
      "EPOCH: 437 LOSS: 9.836912 2ND_METRIC: 2.512371\n",
      "EPOCH: 438 LOSS: 11.657186 2ND_METRIC: 2.876029\n",
      "EPOCH: 439 LOSS: 10.199856 2ND_METRIC: 2.549592\n",
      "EPOCH: 440 LOSS: 10.210173 2ND_METRIC: 2.548786\n",
      "EPOCH: 441 LOSS: 11.036678 2ND_METRIC: 2.788940\n",
      "EPOCH: 442 LOSS: 10.881126 2ND_METRIC: 2.718154\n",
      "EPOCH: 443 LOSS: 10.253742 2ND_METRIC: 2.582232\n",
      "EPOCH: 444 LOSS: 10.493128 2ND_METRIC: 2.692653\n",
      "EPOCH: 445 LOSS: 10.175489 2ND_METRIC: 2.607126\n",
      "EPOCH: 446 LOSS: 10.896927 2ND_METRIC: 2.749135\n",
      "EPOCH: 447 LOSS: 10.074383 2ND_METRIC: 2.604431\n",
      "EPOCH: 448 LOSS: 11.191204 2ND_METRIC: 2.771881\n",
      "EPOCH: 449 LOSS: 12.509787 2ND_METRIC: 2.969816\n",
      "EPOCH: 450 LOSS: 10.643246 2ND_METRIC: 2.640440\n",
      "EPOCH: 451 LOSS: 10.251013 2ND_METRIC: 2.558342\n",
      "EPOCH: 452 LOSS: 11.546858 2ND_METRIC: 2.837372\n",
      "EPOCH: 453 LOSS: 10.456164 2ND_METRIC: 2.653163\n",
      "EPOCH: 454 LOSS: 10.436556 2ND_METRIC: 2.624263\n",
      "EPOCH: 455 LOSS: 11.488264 2ND_METRIC: 2.897360\n",
      "EPOCH: 456 LOSS: 10.402733 2ND_METRIC: 2.618852\n",
      "EPOCH: 457 LOSS: 11.031025 2ND_METRIC: 2.708601\n",
      "EPOCH: 458 LOSS: 10.722969 2ND_METRIC: 2.708696\n",
      "EPOCH: 459 LOSS: 11.677441 2ND_METRIC: 2.862297\n",
      "EPOCH: 460 LOSS: 10.502521 2ND_METRIC: 2.640713\n",
      "EPOCH: 461 LOSS: 10.766867 2ND_METRIC: 2.724120\n",
      "EPOCH: 462 LOSS: 11.478039 2ND_METRIC: 2.828469\n",
      "EPOCH: 463 LOSS: 12.039730 2ND_METRIC: 2.960354\n",
      "EPOCH: 464 LOSS: 10.550619 2ND_METRIC: 2.612536\n",
      "EPOCH: 465 LOSS: 11.269234 2ND_METRIC: 2.788872\n",
      "EPOCH: 466 LOSS: 10.830275 2ND_METRIC: 2.773140\n",
      "EPOCH: 467 LOSS: 11.160390 2ND_METRIC: 2.781994\n",
      "EPOCH: 468 LOSS: 10.875597 2ND_METRIC: 2.715082\n",
      "EPOCH: 469 LOSS: 10.916698 2ND_METRIC: 2.721832\n",
      "EPOCH: 470 LOSS: 10.596488 2ND_METRIC: 2.654234\n",
      "EPOCH: 471 LOSS: 10.161152 2ND_METRIC: 2.587527\n",
      "EPOCH: 472 LOSS: 11.034191 2ND_METRIC: 2.769962\n",
      "EPOCH: 473 LOSS: 11.010924 2ND_METRIC: 2.772850\n",
      "EPOCH: 474 LOSS: 10.272902 2ND_METRIC: 2.650503\n",
      "EPOCH: 475 LOSS: 11.811611 2ND_METRIC: 2.876765\n",
      "EPOCH: 476 LOSS: 11.139732 2ND_METRIC: 2.769225\n",
      "EPOCH: 477 LOSS: 11.864422 2ND_METRIC: 2.916034\n",
      "EPOCH: 478 LOSS: 10.175479 2ND_METRIC: 2.556407\n",
      "EPOCH: 479 LOSS: 11.083046 2ND_METRIC: 2.770503\n",
      "EPOCH: 480 LOSS: 10.677421 2ND_METRIC: 2.650002\n",
      "EPOCH: 481 LOSS: 10.156089 2ND_METRIC: 2.575575\n",
      "EPOCH: 482 LOSS: 10.449717 2ND_METRIC: 2.631477\n",
      "EPOCH: 483 LOSS: 10.563559 2ND_METRIC: 2.657228\n",
      "EPOCH: 484 LOSS: 10.925697 2ND_METRIC: 2.732858\n",
      "EPOCH: 485 LOSS: 11.069097 2ND_METRIC: 2.782979\n",
      "EPOCH: 486 LOSS: 11.154236 2ND_METRIC: 2.754247\n",
      "EPOCH: 487 LOSS: 10.880138 2ND_METRIC: 2.687889\n",
      "EPOCH: 488 LOSS: 10.854951 2ND_METRIC: 2.722945\n",
      "EPOCH: 489 LOSS: 11.904358 2ND_METRIC: 2.913734\n",
      "EPOCH: 490 LOSS: 9.774128 2ND_METRIC: 2.521513\n",
      "EPOCH: 491 LOSS: 10.349579 2ND_METRIC: 2.631694\n",
      "EPOCH: 492 LOSS: 11.147049 2ND_METRIC: 2.771837\n",
      "EPOCH: 493 LOSS: 10.723078 2ND_METRIC: 2.693491\n",
      "EPOCH: 494 LOSS: 9.667198 2ND_METRIC: 2.475477\n",
      "EPOCH: 495 LOSS: 10.950140 2ND_METRIC: 2.702573\n",
      "EPOCH: 496 LOSS: 11.533932 2ND_METRIC: 2.839499\n",
      "EPOCH: 497 LOSS: 10.943428 2ND_METRIC: 2.733481\n",
      "EPOCH: 498 LOSS: 10.033108 2ND_METRIC: 2.536107\n",
      "EPOCH: 499 LOSS: 10.282230 2ND_METRIC: 2.562038\n",
      "EPOCH: 500 LOSS: 10.563897 2ND_METRIC: 2.639017\n",
      "EPOCH: 501 LOSS: 10.853401 2ND_METRIC: 2.709017\n",
      "EPOCH: 502 LOSS: 11.196159 2ND_METRIC: 2.783224\n",
      "EPOCH: 503 LOSS: 10.266365 2ND_METRIC: 2.619545\n",
      "EPOCH: 504 LOSS: 9.941629 2ND_METRIC: 2.534832\n",
      "EPOCH: 505 LOSS: 10.298494 2ND_METRIC: 2.607595\n",
      "EPOCH: 506 LOSS: 10.546309 2ND_METRIC: 2.653120\n",
      "EPOCH: 507 LOSS: 10.800560 2ND_METRIC: 2.701144\n",
      "EPOCH: 508 LOSS: 10.962029 2ND_METRIC: 2.739407\n",
      "EPOCH: 509 LOSS: 10.460533 2ND_METRIC: 2.649590\n",
      "EPOCH: 510 LOSS: 10.415472 2ND_METRIC: 2.609139\n",
      "EPOCH: 511 LOSS: 10.408294 2ND_METRIC: 2.594193\n",
      "EPOCH: 512 LOSS: 10.654100 2ND_METRIC: 2.634995\n"
     ]
    }
   ],
   "source": [
    "epochs = 512\n",
    "steps_per_epoch = 512 // batch_size + 1  # we usually consider 1 epoch to be\n",
    "                                            # the point where the model has seen\n",
    "                                            # all the training samples at least once\n",
    "\n",
    "historyB = {\"history\":{\"loss\":[],\"mae\":[]}}\n",
    "for e in range(epochs):\n",
    "    for i, (images, y_batch) in enumerate(train_generator):\n",
    "       new_y_batch = []\n",
    "       for x,img in enumerate(images):\n",
    "        array = np.ones(image_size+(3,))\n",
    "        array *= img>0\n",
    "        array[array>0] = y_batch[x]\n",
    "        new_y_batch.append(array)\n",
    "       new_y_batch = np.array(new_y_batch)\n",
    "       loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "    #    val = modelB.test_on_batch(images, new_y_batch)\n",
    "       if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "            historyB[\"history\"][\"loss\"].append(loss[0])\n",
    "            historyB[\"history\"][\"mae\"].append(loss[1])\n",
    "            # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "            break  \n",
    "    for i, (images, y_batch) in enumerate(val_generator):\n",
    "       new_y_batch = []\n",
    "       for x,img in enumerate(images):\n",
    "        array = np.ones(image_size+(3,))\n",
    "        array *= img>0\n",
    "        array[array>0] = y_batch[x]\n",
    "        new_y_batch.append(array)\n",
    "       new_y_batch = np.array(new_y_batch)\n",
    "    #    loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "       val = modelB.test_on_batch(images, new_y_batch)\n",
    "       if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "            historyB[\"history\"][\"val_loss\"].append(val[0])\n",
    "            historyB[\"history\"][\"val_mae\"].append(val[1])\n",
    "            # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "            break  \n",
    "    print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}\".format(e+1, loss[0], loss[1],val[0],val[1]))\n",
    "    train_generator.on_epoch_end()  # this shuffles the data at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(historyB[\"history\"][\"loss\"], color='g')\n",
    "plt.legend([\"Train\"])\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(historyB[\"history\"][\"mae\"], color='g')\n",
    "plt.legend([\"Train\"])\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "plt.suptitle(\"Model B: SegNet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9216,)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(historyB[\"history\"][\"loss\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "#                               patience=10, min_lr=1e-7)\n",
    "# # stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "# history = modelB.fit(train_generator, epochs=2,# validation_data=val_generator,\n",
    "#                      callbacks=[red_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(train_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.13106 6.0\n",
      "3.5999355 9.0\n",
      "3.0707192 6.0\n",
      "3.859416 9.0\n",
      "5.1304493 6.0\n",
      "4.1446676 8.0\n",
      "4.389975 6.0\n",
      "4.2854056 9.0\n",
      "3.9184299 7.0\n",
      "4.396599 9.0\n",
      "5.1493983 6.0\n",
      "2.8802035 7.0\n",
      "4.008257 6.0\n",
      "3.3107173 7.0\n",
      "3.901464 7.0\n",
      "4.393871 8.0\n",
      "4.2648 8.0\n",
      "4.3108788 8.0\n",
      "4.0289116 7.0\n",
      "4.0725636 6.0\n",
      "3.87923 9.0\n",
      "4.100874 7.0\n",
      "4.896346 6.0\n",
      "4.201166 8.0\n",
      "4.1489835 6.0\n",
      "4.7526345 9.0\n",
      "5.589805 8.0\n",
      "3.661984 6.0\n",
      "4.1210756 9.0\n",
      "4.683068 8.0\n",
      "4.0432644 7.0\n",
      "3.4685657 8.0\n"
     ]
    }
   ],
   "source": [
    "sample = next(train_generator)\n",
    "result = modelB(sample[0])\n",
    "for x,i in enumerate(result):\n",
    "    RGB  = cv2.cvtColor(sample[0][x],cv2.COLOR_HSV2RGB)\n",
    "    MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "    ARRAY = np.array(result[5]).reshape(image_size) * MASK\n",
    "    OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "    print(np.average(OUTPUT), np.average(sample[1][x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21df916fb80>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(result[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21df80ec340>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "RGB  = cv2.cvtColor(sample[0][5],cv2.COLOR_HSV2RGB)\n",
    "MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "ARRAY = np.array(result[5]).reshape(image_size) * MASK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0256219\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "AVE = np.average(ARRAY)\n",
    "TRUTH = sample[1][5]\n",
    "print(AVE)\n",
    "print(TRUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ARRAY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA.save('design_models/designA.h5')\n",
    "modelB.save('design_models/designB.h5')\n",
    "modelC.save('design_models/designC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "# removing directory \n",
    "rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_block(\n",
    "    block_input,\n",
    "    num_filters=256,\n",
    "    kernel_size=3,\n",
    "    dilation_rate=1,\n",
    "    padding=\"same\",\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding=\"same\",\n",
    "        use_bias=use_bias,\n",
    "        kernel_initializer=keras.initializers.HeNormal(),\n",
    "    )(block_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def DilatedSpatialPyramidPooling(dspp_input, num_filters):\n",
    "    dims = dspp_input.shape\n",
    "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "    out_pool = layers.UpSampling2D(\n",
    "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "\n",
    "    out_1 = convolution_block(dspp_input, num_filters=num_filters, kernel_size=1, dilation_rate=1)\n",
    "    out_6 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=6)\n",
    "    out_12 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=12)\n",
    "    out_18 = convolution_block(dspp_input, num_filters=num_filters,  kernel_size=3, dilation_rate=18)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "    output = convolution_block(x, kernel_size=1)\n",
    "    return output\n",
    "\n",
    "def DeeplabV3Plus(hp):\n",
    "    model_input = keras.Input(shape=image_size+(3,))\n",
    "    resnet50 = keras.applications.ResNet50(\n",
    "        weights=None, include_top=False, input_tensor=model_input\n",
    "    )\n",
    "    for layer in resnet50.layers:\n",
    "        layer.trainable=True\n",
    "    \n",
    "    hp_filters = hp.Choice('filters',values = [16,32,64,128])\n",
    "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
    "    x = DilatedSpatialPyramidPooling(x, hp_filters*4)\n",
    "    image_size=32\n",
    "    input_a = layers.UpSampling2D(\n",
    "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    input_b = convolution_block(input_b, num_filters=hp_filters, kernel_size=1)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "    x = convolution_block(x, num_filters=hp_filters*4)\n",
    "    x = convolution_block(x, num_filters=hp_filters*4)\n",
    "    x = layers.UpSampling2D(\n",
    "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    model_output = layers.Conv2D(1, 1, padding=\"same\", activation='linear')(x)\n",
    "    # x = layers.GlobalAveragePooling2D()(x)\n",
    "    # hp_units = hp.Choice('units',values = [512,1024])\n",
    "    # x = Dense(hp_units, activation='relu')(x)\n",
    "    # if hp.Boolean(\"2nd dense\"):\n",
    "    #     x = Dense(hp_units, activation='relu')(x)\n",
    "    # model_output = layers.Dense(1, activation='linear')(x)\n",
    "    model  = keras.Model(inputs=model_input, outputs=model_output)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(hp_learning_rate), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# modelC = DeeplabV3Plus(image_size=image_size[0])\n",
    "# modelC.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\design_a\\tuner0.json\n",
      "{'filters': 16, 'learning_rate': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "tunerC = kt.BayesianOptimization(DeeplabV3Plus,\n",
    "                     objective='val_loss',\n",
    "                     directory='my_dir',\n",
    "                     max_trials= 1,\n",
    "                     project_name='design_a',\n",
    "                    #  seed=42,\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tunerC.search(train_generator, epochs=100, validation_data=val_generator, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hpsC=tunerC.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(best_hpsC.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'image_size' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m best_hpsC\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Build the model with the best hp.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m modelC \u001b[38;5;241m=\u001b[39m \u001b[43mDeeplabV3Plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_hpsC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m modelC\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[40], line 39\u001b[0m, in \u001b[0;36mDeeplabV3Plus\u001b[1;34m(hp)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mDeeplabV3Plus\u001b[39m(hp):\n\u001b[1;32m---> 39\u001b[0m     model_input \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m\u001b[43mimage_size\u001b[49m\u001b[38;5;241m+\u001b[39m(\u001b[38;5;241m3\u001b[39m,))\n\u001b[0;32m     40\u001b[0m     resnet50 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mapplications\u001b[38;5;241m.\u001b[39mResNet50(\n\u001b[0;32m     41\u001b[0m         weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_tensor\u001b[38;5;241m=\u001b[39mmodel_input\n\u001b[0;32m     42\u001b[0m     )\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m resnet50\u001b[38;5;241m.\u001b[39mlayers:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'image_size' referenced before assignment"
     ]
    }
   ],
   "source": [
    "best_hpsC.values['filters']=64\n",
    "# Build the model with the best hp.\n",
    "modelC = DeeplabV3Plus(best_hpsC)\n",
    "modelC.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 512\n",
    "steps_per_epoch = 512 // batch_size + 1  # we usually consider 1 epoch to be\n",
    "                                            # the point where the model has seen\n",
    "                                            # all the training samples at least once\n",
    "\n",
    "historyC = {\"history\":{\"loss\":[],\"mae\":[],\"val_loss\":[],\"val_mae\":[]}}\n",
    "for e in range(epochs):\n",
    "    for i, (images, y_batch) in enumerate(train_generator):\n",
    "       new_y_batch = []\n",
    "       for x,img in enumerate(images):\n",
    "        array = np.ones(image_size+(3,))\n",
    "        array *= img>0\n",
    "        array[array>0] = y_batch[x]\n",
    "        new_y_batch.append(array)\n",
    "       new_y_batch = np.array(new_y_batch)\n",
    "       loss = modelC.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "    #    val = modelB.test_on_batch(images, new_y_batch)\n",
    "       if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "            historyC[\"history\"][\"loss\"].append(loss[0])\n",
    "            historyC[\"history\"][\"mae\"].append(loss[1])\n",
    "            # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "            break  \n",
    "    for i, (images, y_batch) in enumerate(val_generator):\n",
    "       new_y_batch = []\n",
    "       for x,img in enumerate(images):\n",
    "        array = np.ones(image_size+(3,))\n",
    "        array *= img>0\n",
    "        array[array>0] = y_batch[x]\n",
    "        new_y_batch.append(array)\n",
    "       new_y_batch = np.array(new_y_batch)\n",
    "    #    loss = modelB.train_on_batch(images, new_y_batch)  # train model for a single iteration\n",
    "       val = modelC.test_on_batch(images, new_y_batch)\n",
    "       if i >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "            historyC[\"history\"][\"val_loss\"].append(val[0])\n",
    "            historyC[\"history\"][\"val_mae\"].append(val[1])\n",
    "            # print(\"EPOCH: {} LOSS: {:.6f} 2ND_METRIC: {:.6f}\".format(e+1, loss[0], loss[1]))\n",
    "            break  \n",
    "    print(\"EPOCH: {} LOSS: {:.6f} MAE: {:.6f} VAL_LOSS: {:.6f} VAL_MAE: {:.6f}\".format(e+1, loss[0], loss[1],val[0],val[1]))\n",
    "    train_generator.on_epoch_end()  # this shuffles the data at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(historyC[\"history\"][\"loss\"], color='g')\n",
    "plt.plot(historyC[\"history\"][\"val_loss\"], color='r')\n",
    "plt.legend([\"Train\"])\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(historyC[\"history\"][\"mae\"], color='g')\n",
    "plt.plot(historyC[\"history\"][\"val_mae\"], color='r')\n",
    "plt.legend([\"Train\"])\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "\n",
    "plt.suptitle(\"Model C: DeepLabV3+\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(train_generator)\n",
    "result = modelC(sample[0])\n",
    "for x,i in enumerate(result):\n",
    "    RGB  = cv2.cvtColor(sample[0][x],cv2.COLOR_HSV2RGB)\n",
    "    MASK = cv2.cvtColor(RGB, cv2.COLOR_RGB2GRAY) > 0\n",
    "    ARRAY = np.array(result[5]).reshape(image_size) * MASK\n",
    "    OUTPUT = np.array([x for x in ARRAY.flatten() if x > 0])\n",
    "    print(np.average(OUTPUT), np.average(sample[1][x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelC.save('design_models/designC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "# removing directory \n",
    "rmtree('my_dir') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANUFACTURABILITY: TRAINING TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 128\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=4, min_lr=1e-9)\n",
    "# es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=16, mode=\"min\", restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 - 5s - loss: 99.4323 - mae: 5.0791 - val_loss: 51.0406 - val_mae: 7.0570 - lr: 1.0000e-04 - 5s/epoch - 321ms/step\n",
      "Epoch 2/128\n",
      "16/16 - 1s - loss: 2.6698 - mae: 1.3242 - val_loss: 39.7382 - val_mae: 6.2047 - lr: 1.0000e-04 - 1s/epoch - 84ms/step\n",
      "Epoch 3/128\n",
      "16/16 - 1s - loss: 1.8973 - mae: 1.1454 - val_loss: 33.6737 - val_mae: 5.6948 - lr: 1.0000e-04 - 1s/epoch - 84ms/step\n",
      "Epoch 4/128\n",
      "16/16 - 1s - loss: 1.3084 - mae: 0.9492 - val_loss: 27.3942 - val_mae: 5.1136 - lr: 1.0000e-04 - 1s/epoch - 84ms/step\n",
      "Epoch 5/128\n",
      "16/16 - 1s - loss: 1.1753 - mae: 0.9087 - val_loss: 20.7657 - val_mae: 4.4181 - lr: 1.0000e-04 - 1s/epoch - 84ms/step\n",
      "Epoch 6/128\n",
      "16/16 - 1s - loss: 1.0557 - mae: 0.8732 - val_loss: 17.1287 - val_mae: 3.9855 - lr: 1.0000e-04 - 1s/epoch - 84ms/step\n",
      "Epoch 7/128\n",
      "16/16 - 1s - loss: 1.0540 - mae: 0.8573 - val_loss: 14.8750 - val_mae: 3.6921 - lr: 1.0000e-04 - 1s/epoch - 85ms/step\n",
      "Epoch 8/128\n",
      "16/16 - 1s - loss: 1.2161 - mae: 0.9112 - val_loss: 10.1716 - val_mae: 2.9879 - lr: 1.0000e-04 - 1s/epoch - 84ms/step\n",
      "Epoch 9/128\n",
      "16/16 - 1s - loss: 1.0015 - mae: 0.8466 - val_loss: 7.1844 - val_mae: 2.4382 - lr: 1.0000e-04 - 1s/epoch - 85ms/step\n",
      "Epoch 10/128\n",
      "16/16 - 1s - loss: 0.9467 - mae: 0.7948 - val_loss: 6.4319 - val_mae: 2.2797 - lr: 1.0000e-04 - 1s/epoch - 85ms/step\n",
      "Epoch 11/128\n",
      "16/16 - 1s - loss: 0.9408 - mae: 0.8077 - val_loss: 5.2776 - val_mae: 2.0103 - lr: 1.0000e-04 - 1s/epoch - 84ms/step\n",
      "Epoch 12/128\n",
      "16/16 - 1s - loss: 0.9807 - mae: 0.8234 - val_loss: 3.1708 - val_mae: 1.4405 - lr: 1.0000e-04 - 1s/epoch - 84ms/step\n",
      "Epoch 13/128\n",
      "16/16 - 1s - loss: 0.8241 - mae: 0.7544 - val_loss: 1.5645 - val_mae: 1.0350 - lr: 1.0000e-04 - 1s/epoch - 86ms/step\n",
      "Epoch 14/128\n",
      "16/16 - 1s - loss: 1.1139 - mae: 0.8606 - val_loss: 1.3652 - val_mae: 0.9906 - lr: 1.0000e-04 - 1s/epoch - 87ms/step\n",
      "Epoch 15/128\n",
      "16/16 - 1s - loss: 0.8854 - mae: 0.7825 - val_loss: 1.4030 - val_mae: 1.0097 - lr: 1.0000e-04 - 1s/epoch - 85ms/step\n",
      "Epoch 16/128\n",
      "16/16 - 1s - loss: 0.9524 - mae: 0.7967 - val_loss: 1.4118 - val_mae: 1.0165 - lr: 1.0000e-04 - 1s/epoch - 84ms/step\n",
      "Epoch 17/128\n",
      "16/16 - 1s - loss: 0.8894 - mae: 0.7586 - val_loss: 2.4532 - val_mae: 1.3051 - lr: 1.0000e-04 - 1s/epoch - 85ms/step\n",
      "Epoch 18/128\n",
      "16/16 - 1s - loss: 0.7509 - mae: 0.7061 - val_loss: 1.9506 - val_mae: 1.1713 - lr: 1.0000e-04 - 1s/epoch - 85ms/step\n",
      "Epoch 19/128\n",
      "16/16 - 1s - loss: 0.6690 - mae: 0.6669 - val_loss: 2.0319 - val_mae: 1.1944 - lr: 1.0000e-05 - 1s/epoch - 85ms/step\n",
      "Epoch 20/128\n",
      "16/16 - 1s - loss: 0.6513 - mae: 0.6760 - val_loss: 1.8269 - val_mae: 1.1288 - lr: 1.0000e-05 - 1s/epoch - 84ms/step\n",
      "Epoch 21/128\n",
      "16/16 - 1s - loss: 0.6301 - mae: 0.6500 - val_loss: 2.3483 - val_mae: 1.2638 - lr: 1.0000e-05 - 1s/epoch - 85ms/step\n",
      "Epoch 22/128\n",
      "16/16 - 1s - loss: 0.7123 - mae: 0.6961 - val_loss: 1.9874 - val_mae: 1.1671 - lr: 1.0000e-05 - 1s/epoch - 85ms/step\n",
      "Epoch 23/128\n",
      "16/16 - 1s - loss: 0.6476 - mae: 0.6637 - val_loss: 2.2341 - val_mae: 1.2322 - lr: 1.0000e-06 - 1s/epoch - 86ms/step\n",
      "Epoch 24/128\n",
      "16/16 - 1s - loss: 0.6036 - mae: 0.6310 - val_loss: 2.3824 - val_mae: 1.2713 - lr: 1.0000e-06 - 1s/epoch - 85ms/step\n",
      "Epoch 25/128\n",
      "16/16 - 1s - loss: 0.6399 - mae: 0.6585 - val_loss: 2.5040 - val_mae: 1.3027 - lr: 1.0000e-06 - 1s/epoch - 85ms/step\n",
      "Epoch 26/128\n",
      "16/16 - 1s - loss: 0.6713 - mae: 0.6795 - val_loss: 2.4769 - val_mae: 1.2972 - lr: 1.0000e-06 - 1s/epoch - 85ms/step\n",
      "Epoch 27/128\n",
      "16/16 - 1s - loss: 0.6690 - mae: 0.6886 - val_loss: 2.4670 - val_mae: 1.2909 - lr: 1.0000e-07 - 1s/epoch - 85ms/step\n",
      "Epoch 28/128\n",
      "16/16 - 1s - loss: 0.6451 - mae: 0.6654 - val_loss: 2.3054 - val_mae: 1.2514 - lr: 1.0000e-07 - 1s/epoch - 85ms/step\n",
      "Epoch 29/128\n",
      "16/16 - 1s - loss: 0.5973 - mae: 0.6374 - val_loss: 1.9692 - val_mae: 1.1654 - lr: 1.0000e-07 - 1s/epoch - 84ms/step\n",
      "Epoch 30/128\n",
      "16/16 - 1s - loss: 0.6342 - mae: 0.6490 - val_loss: 1.7517 - val_mae: 1.1072 - lr: 1.0000e-07 - 1s/epoch - 85ms/step\n",
      "Epoch 31/128\n",
      "16/16 - 1s - loss: 0.6637 - mae: 0.6774 - val_loss: 1.6541 - val_mae: 1.0714 - lr: 1.0000e-08 - 1s/epoch - 85ms/step\n",
      "Epoch 32/128\n",
      "16/16 - 1s - loss: 0.6559 - mae: 0.6645 - val_loss: 1.8170 - val_mae: 1.1104 - lr: 1.0000e-08 - 1s/epoch - 84ms/step\n",
      "Epoch 33/128\n",
      "16/16 - 1s - loss: 0.6460 - mae: 0.6595 - val_loss: 2.0184 - val_mae: 1.1565 - lr: 1.0000e-08 - 1s/epoch - 85ms/step\n",
      "Epoch 34/128\n",
      "16/16 - 1s - loss: 0.6350 - mae: 0.6511 - val_loss: 2.2859 - val_mae: 1.2392 - lr: 1.0000e-08 - 1s/epoch - 84ms/step\n",
      "Epoch 35/128\n",
      "16/16 - 1s - loss: 0.5879 - mae: 0.6327 - val_loss: 2.3497 - val_mae: 1.2614 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 36/128\n",
      "16/16 - 1s - loss: 0.6385 - mae: 0.6508 - val_loss: 2.4817 - val_mae: 1.3051 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 37/128\n",
      "16/16 - 1s - loss: 0.5895 - mae: 0.6359 - val_loss: 2.5138 - val_mae: 1.3019 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 38/128\n",
      "16/16 - 1s - loss: 0.6539 - mae: 0.6572 - val_loss: 2.6054 - val_mae: 1.3183 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 39/128\n",
      "16/16 - 1s - loss: 0.6500 - mae: 0.6719 - val_loss: 2.6808 - val_mae: 1.3418 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 40/128\n",
      "16/16 - 1s - loss: 0.6409 - mae: 0.6637 - val_loss: 2.6190 - val_mae: 1.3373 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 41/128\n",
      "16/16 - 1s - loss: 0.6627 - mae: 0.6706 - val_loss: 2.4709 - val_mae: 1.3124 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 42/128\n",
      "16/16 - 1s - loss: 0.5479 - mae: 0.6059 - val_loss: 2.2060 - val_mae: 1.2430 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 43/128\n",
      "16/16 - 1s - loss: 0.5855 - mae: 0.6258 - val_loss: 1.9895 - val_mae: 1.1801 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 44/128\n",
      "16/16 - 1s - loss: 0.6104 - mae: 0.6433 - val_loss: 1.7269 - val_mae: 1.0999 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 45/128\n",
      "16/16 - 1s - loss: 0.6208 - mae: 0.6533 - val_loss: 1.5232 - val_mae: 1.0312 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 46/128\n",
      "16/16 - 1s - loss: 0.6194 - mae: 0.6520 - val_loss: 1.3875 - val_mae: 0.9888 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 47/128\n",
      "16/16 - 1s - loss: 0.6134 - mae: 0.6419 - val_loss: 1.2541 - val_mae: 0.9420 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 48/128\n",
      "16/16 - 1s - loss: 0.5751 - mae: 0.6308 - val_loss: 1.1528 - val_mae: 0.9044 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 49/128\n",
      "16/16 - 1s - loss: 0.6367 - mae: 0.6569 - val_loss: 1.0708 - val_mae: 0.8704 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 50/128\n",
      "16/16 - 1s - loss: 0.6015 - mae: 0.6459 - val_loss: 0.9960 - val_mae: 0.8318 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 51/128\n",
      "16/16 - 1s - loss: 0.6284 - mae: 0.6544 - val_loss: 0.9521 - val_mae: 0.8059 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 52/128\n",
      "16/16 - 1s - loss: 0.6352 - mae: 0.6563 - val_loss: 0.9209 - val_mae: 0.7878 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 53/128\n",
      "16/16 - 1s - loss: 0.6462 - mae: 0.6654 - val_loss: 0.9124 - val_mae: 0.7787 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 54/128\n",
      "16/16 - 1s - loss: 0.6378 - mae: 0.6670 - val_loss: 0.9077 - val_mae: 0.7717 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 55/128\n",
      "16/16 - 1s - loss: 0.6138 - mae: 0.6493 - val_loss: 0.8970 - val_mae: 0.7638 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 56/128\n",
      "16/16 - 1s - loss: 0.6465 - mae: 0.6571 - val_loss: 0.8911 - val_mae: 0.7578 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 57/128\n",
      "16/16 - 1s - loss: 0.6367 - mae: 0.6658 - val_loss: 0.8860 - val_mae: 0.7517 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 58/128\n",
      "16/16 - 1s - loss: 0.6319 - mae: 0.6584 - val_loss: 0.8822 - val_mae: 0.7491 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 59/128\n",
      "16/16 - 1s - loss: 0.6168 - mae: 0.6411 - val_loss: 0.8803 - val_mae: 0.7468 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 60/128\n",
      "16/16 - 1s - loss: 0.6239 - mae: 0.6543 - val_loss: 0.8830 - val_mae: 0.7449 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 61/128\n",
      "16/16 - 1s - loss: 0.6645 - mae: 0.6677 - val_loss: 0.8839 - val_mae: 0.7459 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 62/128\n",
      "16/16 - 1s - loss: 0.6162 - mae: 0.6547 - val_loss: 0.8873 - val_mae: 0.7457 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 63/128\n",
      "16/16 - 1s - loss: 0.6364 - mae: 0.6545 - val_loss: 0.8867 - val_mae: 0.7454 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 64/128\n",
      "16/16 - 1s - loss: 0.5859 - mae: 0.6232 - val_loss: 0.8895 - val_mae: 0.7455 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 65/128\n",
      "16/16 - 1s - loss: 0.6611 - mae: 0.6709 - val_loss: 0.8928 - val_mae: 0.7461 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 66/128\n",
      "16/16 - 1s - loss: 0.6108 - mae: 0.6431 - val_loss: 0.8910 - val_mae: 0.7437 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 67/128\n",
      "16/16 - 1s - loss: 0.6276 - mae: 0.6512 - val_loss: 0.8933 - val_mae: 0.7446 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 68/128\n",
      "16/16 - 1s - loss: 0.5932 - mae: 0.6405 - val_loss: 0.8899 - val_mae: 0.7412 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 69/128\n",
      "16/16 - 1s - loss: 0.6588 - mae: 0.6714 - val_loss: 0.8869 - val_mae: 0.7398 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 70/128\n",
      "16/16 - 1s - loss: 0.6364 - mae: 0.6725 - val_loss: 0.8886 - val_mae: 0.7400 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 71/128\n",
      "16/16 - 1s - loss: 0.6019 - mae: 0.6331 - val_loss: 0.8911 - val_mae: 0.7402 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 72/128\n",
      "16/16 - 1s - loss: 0.6841 - mae: 0.6819 - val_loss: 0.8881 - val_mae: 0.7392 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 73/128\n",
      "16/16 - 1s - loss: 0.6408 - mae: 0.6488 - val_loss: 0.8889 - val_mae: 0.7393 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 74/128\n",
      "16/16 - 1s - loss: 0.6070 - mae: 0.6434 - val_loss: 0.8877 - val_mae: 0.7393 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 75/128\n",
      "16/16 - 1s - loss: 0.6015 - mae: 0.6433 - val_loss: 0.8907 - val_mae: 0.7405 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 76/128\n",
      "16/16 - 1s - loss: 0.5907 - mae: 0.6243 - val_loss: 0.8895 - val_mae: 0.7401 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 77/128\n",
      "16/16 - 1s - loss: 0.6041 - mae: 0.6521 - val_loss: 0.8889 - val_mae: 0.7412 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 78/128\n",
      "16/16 - 1s - loss: 0.6122 - mae: 0.6466 - val_loss: 0.8877 - val_mae: 0.7415 - lr: 1.0000e-09 - 1s/epoch - 87ms/step\n",
      "Epoch 79/128\n",
      "16/16 - 1s - loss: 0.6439 - mae: 0.6553 - val_loss: 0.8862 - val_mae: 0.7402 - lr: 1.0000e-09 - 1s/epoch - 90ms/step\n",
      "Epoch 80/128\n",
      "16/16 - 1s - loss: 0.6432 - mae: 0.6594 - val_loss: 0.8902 - val_mae: 0.7421 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 81/128\n",
      "16/16 - 1s - loss: 0.6329 - mae: 0.6553 - val_loss: 0.8907 - val_mae: 0.7419 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 82/128\n",
      "16/16 - 1s - loss: 0.6770 - mae: 0.6859 - val_loss: 0.8902 - val_mae: 0.7409 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 83/128\n",
      "16/16 - 1s - loss: 0.6311 - mae: 0.6590 - val_loss: 0.8912 - val_mae: 0.7419 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 84/128\n",
      "16/16 - 1s - loss: 0.6321 - mae: 0.6559 - val_loss: 0.8916 - val_mae: 0.7423 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 85/128\n",
      "16/16 - 1s - loss: 0.6139 - mae: 0.6488 - val_loss: 0.8922 - val_mae: 0.7425 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 86/128\n",
      "16/16 - 1s - loss: 0.6400 - mae: 0.6546 - val_loss: 0.8911 - val_mae: 0.7423 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 87/128\n",
      "16/16 - 1s - loss: 0.6002 - mae: 0.6323 - val_loss: 0.8923 - val_mae: 0.7437 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 88/128\n",
      "16/16 - 1s - loss: 0.6035 - mae: 0.6428 - val_loss: 0.9040 - val_mae: 0.7488 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 89/128\n",
      "16/16 - 1s - loss: 0.7188 - mae: 0.6951 - val_loss: 0.8993 - val_mae: 0.7475 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 90/128\n",
      "16/16 - 1s - loss: 0.6060 - mae: 0.6480 - val_loss: 0.8909 - val_mae: 0.7441 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 91/128\n",
      "16/16 - 1s - loss: 0.6260 - mae: 0.6603 - val_loss: 0.8900 - val_mae: 0.7430 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 92/128\n",
      "16/16 - 1s - loss: 0.6116 - mae: 0.6454 - val_loss: 0.8882 - val_mae: 0.7422 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 93/128\n",
      "16/16 - 1s - loss: 0.5824 - mae: 0.6334 - val_loss: 0.8927 - val_mae: 0.7443 - lr: 1.0000e-09 - 1s/epoch - 84ms/step\n",
      "Epoch 94/128\n",
      "16/16 - 1s - loss: 0.6025 - mae: 0.6292 - val_loss: 0.8906 - val_mae: 0.7432 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 95/128\n",
      "16/16 - 1s - loss: 0.6489 - mae: 0.6702 - val_loss: 0.8895 - val_mae: 0.7425 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 96/128\n",
      "16/16 - 1s - loss: 0.6396 - mae: 0.6576 - val_loss: 0.8959 - val_mae: 0.7462 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 97/128\n",
      "16/16 - 1s - loss: 0.6116 - mae: 0.6491 - val_loss: 0.8937 - val_mae: 0.7456 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 98/128\n",
      "16/16 - 1s - loss: 0.6328 - mae: 0.6612 - val_loss: 0.8899 - val_mae: 0.7431 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 99/128\n",
      "16/16 - 1s - loss: 0.6520 - mae: 0.6642 - val_loss: 0.8912 - val_mae: 0.7435 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 100/128\n",
      "16/16 - 1s - loss: 0.6305 - mae: 0.6581 - val_loss: 0.8886 - val_mae: 0.7421 - lr: 1.0000e-09 - 1s/epoch - 87ms/step\n",
      "Epoch 101/128\n",
      "16/16 - 1s - loss: 0.6184 - mae: 0.6478 - val_loss: 0.8903 - val_mae: 0.7425 - lr: 1.0000e-09 - 1s/epoch - 91ms/step\n",
      "Epoch 102/128\n",
      "16/16 - 1s - loss: 0.6160 - mae: 0.6653 - val_loss: 0.8932 - val_mae: 0.7426 - lr: 1.0000e-09 - 1s/epoch - 88ms/step\n",
      "Epoch 103/128\n",
      "16/16 - 1s - loss: 0.5990 - mae: 0.6377 - val_loss: 0.8894 - val_mae: 0.7403 - lr: 1.0000e-09 - 1s/epoch - 87ms/step\n",
      "Epoch 104/128\n",
      "16/16 - 1s - loss: 0.5903 - mae: 0.6431 - val_loss: 0.8893 - val_mae: 0.7399 - lr: 1.0000e-09 - 1s/epoch - 87ms/step\n",
      "Epoch 105/128\n",
      "16/16 - 1s - loss: 0.6265 - mae: 0.6570 - val_loss: 0.8902 - val_mae: 0.7406 - lr: 1.0000e-09 - 1s/epoch - 87ms/step\n",
      "Epoch 106/128\n",
      "16/16 - 1s - loss: 0.6724 - mae: 0.6730 - val_loss: 0.8883 - val_mae: 0.7399 - lr: 1.0000e-09 - 1s/epoch - 87ms/step\n",
      "Epoch 107/128\n",
      "16/16 - 1s - loss: 0.6602 - mae: 0.6711 - val_loss: 0.8864 - val_mae: 0.7395 - lr: 1.0000e-09 - 1s/epoch - 87ms/step\n",
      "Epoch 108/128\n",
      "16/16 - 1s - loss: 0.6197 - mae: 0.6478 - val_loss: 0.8897 - val_mae: 0.7413 - lr: 1.0000e-09 - 1s/epoch - 87ms/step\n",
      "Epoch 109/128\n",
      "16/16 - 1s - loss: 0.6086 - mae: 0.6454 - val_loss: 0.8880 - val_mae: 0.7414 - lr: 1.0000e-09 - 1s/epoch - 88ms/step\n",
      "Epoch 110/128\n",
      "16/16 - 1s - loss: 0.6463 - mae: 0.6659 - val_loss: 0.8923 - val_mae: 0.7429 - lr: 1.0000e-09 - 1s/epoch - 91ms/step\n",
      "Epoch 111/128\n",
      "16/16 - 1s - loss: 0.6072 - mae: 0.6466 - val_loss: 0.8997 - val_mae: 0.7455 - lr: 1.0000e-09 - 1s/epoch - 87ms/step\n",
      "Epoch 112/128\n",
      "16/16 - 1s - loss: 0.6362 - mae: 0.6510 - val_loss: 0.8957 - val_mae: 0.7441 - lr: 1.0000e-09 - 1s/epoch - 87ms/step\n",
      "Epoch 113/128\n",
      "16/16 - 1s - loss: 0.5952 - mae: 0.6484 - val_loss: 0.8990 - val_mae: 0.7456 - lr: 1.0000e-09 - 1s/epoch - 87ms/step\n",
      "Epoch 114/128\n",
      "16/16 - 1s - loss: 0.6029 - mae: 0.6399 - val_loss: 0.8983 - val_mae: 0.7456 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 115/128\n",
      "16/16 - 1s - loss: 0.6117 - mae: 0.6398 - val_loss: 0.8939 - val_mae: 0.7442 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 116/128\n",
      "16/16 - 1s - loss: 0.6198 - mae: 0.6477 - val_loss: 0.8923 - val_mae: 0.7427 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 117/128\n",
      "16/16 - 1s - loss: 0.6105 - mae: 0.6396 - val_loss: 0.8901 - val_mae: 0.7422 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 118/128\n",
      "16/16 - 1s - loss: 0.6099 - mae: 0.6345 - val_loss: 0.8951 - val_mae: 0.7451 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 119/128\n",
      "16/16 - 1s - loss: 0.6250 - mae: 0.6570 - val_loss: 0.8947 - val_mae: 0.7445 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 120/128\n",
      "16/16 - 1s - loss: 0.6184 - mae: 0.6443 - val_loss: 0.8927 - val_mae: 0.7429 - lr: 1.0000e-09 - 1s/epoch - 86ms/step\n",
      "Epoch 121/128\n",
      "16/16 - 1s - loss: 0.5768 - mae: 0.6298 - val_loss: 0.8958 - val_mae: 0.7441 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 122/128\n",
      "16/16 - 1s - loss: 0.6313 - mae: 0.6515 - val_loss: 0.8940 - val_mae: 0.7421 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 123/128\n",
      "16/16 - 1s - loss: 0.6291 - mae: 0.6570 - val_loss: 0.8908 - val_mae: 0.7419 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 124/128\n",
      "16/16 - 1s - loss: 0.6914 - mae: 0.6879 - val_loss: 0.8892 - val_mae: 0.7416 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 125/128\n",
      "16/16 - 1s - loss: 0.6141 - mae: 0.6449 - val_loss: 0.8871 - val_mae: 0.7415 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 126/128\n",
      "16/16 - 1s - loss: 0.6651 - mae: 0.6767 - val_loss: 0.8884 - val_mae: 0.7412 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 127/128\n",
      "16/16 - 1s - loss: 0.6446 - mae: 0.6630 - val_loss: 0.8889 - val_mae: 0.7414 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Epoch 128/128\n",
      "16/16 - 1s - loss: 0.6562 - mae: 0.6635 - val_loss: 0.8894 - val_mae: 0.7423 - lr: 1.0000e-09 - 1s/epoch - 85ms/step\n",
      "Training time: 179.95399808883667s\n"
     ]
    }
   ],
   "source": [
    "## DESIGN A\n",
    "import time \n",
    "start = time.time()\n",
    "historyA = modelA.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [reduce_lr])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")\n",
    "\n",
    "# ## FUNCTIONALITY: INFERENCE TIME\n",
    "# modelC.evaluate(train_generator[1][0][0].reshape(1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \n\u001b[0;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m historyB \u001b[38;5;241m=\u001b[39m \u001b[43mmodelB\u001b[49m\u001b[38;5;241m.\u001b[39mfit(train_generator, epochs\u001b[38;5;241m=\u001b[39mepochs, validation_data\u001b[38;5;241m=\u001b[39mval_generator, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, callbacks \u001b[38;5;241m=\u001b[39m [reduce_lr])\n\u001b[0;32m      5\u001b[0m stop \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstop\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'modelB' is not defined"
     ]
    }
   ],
   "source": [
    "## DESIGN B\n",
    "import time \n",
    "start = time.time()\n",
    "historyB = modelB.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [reduce_lr])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "16/16 - 7s - loss: 52.1323 - mae: 7.1257 - val_loss: 55.8993 - val_mae: 7.3930 - lr: 1.0000e-05 - 7s/epoch - 446ms/step\n",
      "Epoch 2/128\n",
      "16/16 - 1s - loss: 40.1086 - mae: 6.2171 - val_loss: 52.3317 - val_mae: 7.1476 - lr: 1.0000e-05 - 935ms/epoch - 58ms/step\n",
      "Epoch 3/128\n",
      "16/16 - 1s - loss: 29.2437 - mae: 5.2535 - val_loss: 46.4533 - val_mae: 6.7238 - lr: 1.0000e-05 - 955ms/epoch - 60ms/step\n",
      "Epoch 4/128\n",
      "16/16 - 1s - loss: 19.9652 - mae: 4.2573 - val_loss: 37.7043 - val_mae: 6.0384 - lr: 1.0000e-05 - 943ms/epoch - 59ms/step\n",
      "Epoch 5/128\n",
      "16/16 - 1s - loss: 12.6390 - mae: 3.3172 - val_loss: 26.9307 - val_mae: 5.0683 - lr: 1.0000e-05 - 957ms/epoch - 60ms/step\n",
      "Epoch 6/128\n",
      "16/16 - 1s - loss: 7.0398 - mae: 2.3238 - val_loss: 16.8671 - val_mae: 3.9527 - lr: 1.0000e-05 - 939ms/epoch - 59ms/step\n",
      "Epoch 7/128\n",
      "16/16 - 1s - loss: 3.7456 - mae: 1.6009 - val_loss: 10.1775 - val_mae: 2.9894 - lr: 1.0000e-05 - 955ms/epoch - 60ms/step\n",
      "Epoch 8/128\n",
      "16/16 - 1s - loss: 2.3669 - mae: 1.2491 - val_loss: 6.9272 - val_mae: 2.3852 - lr: 1.0000e-05 - 966ms/epoch - 60ms/step\n",
      "Epoch 9/128\n",
      "16/16 - 1s - loss: 1.6777 - mae: 1.0845 - val_loss: 5.1180 - val_mae: 1.9703 - lr: 1.0000e-05 - 944ms/epoch - 59ms/step\n",
      "Epoch 10/128\n",
      "16/16 - 1s - loss: 1.4125 - mae: 1.0150 - val_loss: 3.8748 - val_mae: 1.6240 - lr: 1.0000e-05 - 943ms/epoch - 59ms/step\n",
      "Epoch 11/128\n",
      "16/16 - 1s - loss: 1.3294 - mae: 0.9989 - val_loss: 3.2819 - val_mae: 1.4579 - lr: 1.0000e-05 - 953ms/epoch - 60ms/step\n",
      "Epoch 12/128\n",
      "16/16 - 1s - loss: 1.2844 - mae: 0.9836 - val_loss: 2.4949 - val_mae: 1.3059 - lr: 1.0000e-05 - 953ms/epoch - 60ms/step\n",
      "Epoch 13/128\n",
      "16/16 - 1s - loss: 1.2600 - mae: 0.9645 - val_loss: 2.0498 - val_mae: 1.1977 - lr: 1.0000e-05 - 933ms/epoch - 58ms/step\n",
      "Epoch 14/128\n",
      "16/16 - 1s - loss: 1.2615 - mae: 0.9701 - val_loss: 1.8285 - val_mae: 1.1306 - lr: 1.0000e-05 - 945ms/epoch - 59ms/step\n",
      "Epoch 15/128\n",
      "16/16 - 1s - loss: 1.2029 - mae: 0.9512 - val_loss: 1.5072 - val_mae: 1.0125 - lr: 1.0000e-05 - 960ms/epoch - 60ms/step\n",
      "Epoch 16/128\n",
      "16/16 - 1s - loss: 1.2316 - mae: 0.9673 - val_loss: 1.4963 - val_mae: 1.0123 - lr: 1.0000e-05 - 959ms/epoch - 60ms/step\n",
      "Epoch 17/128\n",
      "16/16 - 1s - loss: 1.1697 - mae: 0.9335 - val_loss: 1.4045 - val_mae: 0.9999 - lr: 1.0000e-05 - 942ms/epoch - 59ms/step\n",
      "Epoch 18/128\n",
      "16/16 - 1s - loss: 1.2397 - mae: 0.9737 - val_loss: 1.3333 - val_mae: 1.0061 - lr: 1.0000e-05 - 953ms/epoch - 60ms/step\n",
      "Epoch 19/128\n",
      "16/16 - 1s - loss: 1.2021 - mae: 0.9409 - val_loss: 1.3336 - val_mae: 1.0084 - lr: 1.0000e-05 - 958ms/epoch - 60ms/step\n",
      "Epoch 20/128\n",
      "16/16 - 1s - loss: 1.2317 - mae: 0.9613 - val_loss: 1.3809 - val_mae: 1.0115 - lr: 1.0000e-05 - 949ms/epoch - 59ms/step\n",
      "Epoch 21/128\n",
      "16/16 - 1s - loss: 1.1747 - mae: 0.9375 - val_loss: 1.4042 - val_mae: 1.0191 - lr: 1.0000e-05 - 953ms/epoch - 60ms/step\n",
      "Epoch 22/128\n",
      "16/16 - 1s - loss: 1.1593 - mae: 0.9411 - val_loss: 1.4704 - val_mae: 1.0222 - lr: 1.0000e-05 - 929ms/epoch - 58ms/step\n",
      "Epoch 23/128\n",
      "16/16 - 1s - loss: 1.2148 - mae: 0.9471 - val_loss: 1.4537 - val_mae: 1.0080 - lr: 1.0000e-06 - 961ms/epoch - 60ms/step\n",
      "Epoch 24/128\n",
      "16/16 - 1s - loss: 1.1941 - mae: 0.9393 - val_loss: 1.4865 - val_mae: 1.0097 - lr: 1.0000e-06 - 962ms/epoch - 60ms/step\n",
      "Epoch 25/128\n",
      "16/16 - 1s - loss: 1.1639 - mae: 0.9328 - val_loss: 1.6135 - val_mae: 1.0601 - lr: 1.0000e-06 - 937ms/epoch - 59ms/step\n",
      "Epoch 26/128\n",
      "16/16 - 1s - loss: 1.1673 - mae: 0.9309 - val_loss: 1.7161 - val_mae: 1.0981 - lr: 1.0000e-06 - 945ms/epoch - 59ms/step\n",
      "Epoch 27/128\n",
      "16/16 - 1s - loss: 1.1461 - mae: 0.9266 - val_loss: 1.7278 - val_mae: 1.1003 - lr: 1.0000e-07 - 961ms/epoch - 60ms/step\n",
      "Epoch 28/128\n",
      "16/16 - 1s - loss: 1.1839 - mae: 0.9336 - val_loss: 1.8329 - val_mae: 1.1230 - lr: 1.0000e-07 - 953ms/epoch - 60ms/step\n",
      "Epoch 29/128\n",
      "16/16 - 1s - loss: 1.2063 - mae: 0.9530 - val_loss: 1.9207 - val_mae: 1.1622 - lr: 1.0000e-07 - 939ms/epoch - 59ms/step\n",
      "Epoch 30/128\n",
      "16/16 - 1s - loss: 1.1829 - mae: 0.9433 - val_loss: 1.9219 - val_mae: 1.1617 - lr: 1.0000e-07 - 979ms/epoch - 61ms/step\n",
      "Epoch 31/128\n",
      "16/16 - 1s - loss: 1.1955 - mae: 0.9408 - val_loss: 1.9114 - val_mae: 1.1699 - lr: 1.0000e-08 - 968ms/epoch - 61ms/step\n",
      "Epoch 32/128\n",
      "16/16 - 1s - loss: 1.2072 - mae: 0.9585 - val_loss: 1.9347 - val_mae: 1.1658 - lr: 1.0000e-08 - 959ms/epoch - 60ms/step\n",
      "Epoch 33/128\n",
      "16/16 - 1s - loss: 1.2558 - mae: 0.9670 - val_loss: 1.9241 - val_mae: 1.1678 - lr: 1.0000e-08 - 947ms/epoch - 59ms/step\n",
      "Epoch 34/128\n",
      "16/16 - 1s - loss: 1.1522 - mae: 0.9332 - val_loss: 1.8537 - val_mae: 1.1374 - lr: 1.0000e-08 - 944ms/epoch - 59ms/step\n",
      "Epoch 35/128\n",
      "16/16 - 1s - loss: 1.2100 - mae: 0.9591 - val_loss: 1.7451 - val_mae: 1.1084 - lr: 1.0000e-09 - 951ms/epoch - 59ms/step\n",
      "Epoch 36/128\n",
      "16/16 - 1s - loss: 1.1821 - mae: 0.9382 - val_loss: 1.6851 - val_mae: 1.0930 - lr: 1.0000e-09 - 954ms/epoch - 60ms/step\n",
      "Epoch 37/128\n",
      "16/16 - 1s - loss: 1.2049 - mae: 0.9440 - val_loss: 1.5447 - val_mae: 1.0481 - lr: 1.0000e-09 - 956ms/epoch - 60ms/step\n",
      "Epoch 38/128\n",
      "16/16 - 1s - loss: 1.1636 - mae: 0.9390 - val_loss: 1.5164 - val_mae: 1.0357 - lr: 1.0000e-09 - 942ms/epoch - 59ms/step\n",
      "Epoch 39/128\n",
      "16/16 - 1s - loss: 1.2210 - mae: 0.9663 - val_loss: 1.4333 - val_mae: 1.0100 - lr: 1.0000e-09 - 958ms/epoch - 60ms/step\n",
      "Epoch 40/128\n",
      "16/16 - 1s - loss: 1.1635 - mae: 0.9391 - val_loss: 1.4397 - val_mae: 1.0193 - lr: 1.0000e-09 - 966ms/epoch - 60ms/step\n",
      "Epoch 41/128\n",
      "16/16 - 1s - loss: 1.1961 - mae: 0.9442 - val_loss: 1.4360 - val_mae: 1.0081 - lr: 1.0000e-09 - 955ms/epoch - 60ms/step\n",
      "Epoch 42/128\n",
      "16/16 - 1s - loss: 1.1967 - mae: 0.9544 - val_loss: 1.3711 - val_mae: 0.9966 - lr: 1.0000e-09 - 960ms/epoch - 60ms/step\n",
      "Epoch 43/128\n",
      "16/16 - 1s - loss: 1.2368 - mae: 0.9646 - val_loss: 1.2982 - val_mae: 0.9728 - lr: 1.0000e-09 - 979ms/epoch - 61ms/step\n",
      "Epoch 44/128\n",
      "16/16 - 1s - loss: 1.2043 - mae: 0.9607 - val_loss: 1.2622 - val_mae: 0.9732 - lr: 1.0000e-09 - 972ms/epoch - 61ms/step\n",
      "Epoch 45/128\n",
      "16/16 - 1s - loss: 1.1814 - mae: 0.9379 - val_loss: 1.3036 - val_mae: 0.9958 - lr: 1.0000e-09 - 959ms/epoch - 60ms/step\n",
      "Epoch 46/128\n",
      "16/16 - 1s - loss: 1.2029 - mae: 0.9550 - val_loss: 1.2446 - val_mae: 0.9750 - lr: 1.0000e-09 - 950ms/epoch - 59ms/step\n",
      "Epoch 47/128\n",
      "16/16 - 1s - loss: 1.2178 - mae: 0.9522 - val_loss: 1.2365 - val_mae: 0.9620 - lr: 1.0000e-09 - 952ms/epoch - 60ms/step\n",
      "Epoch 48/128\n",
      "16/16 - 1s - loss: 1.1587 - mae: 0.9315 - val_loss: 1.2102 - val_mae: 0.9722 - lr: 1.0000e-09 - 957ms/epoch - 60ms/step\n",
      "Epoch 49/128\n",
      "16/16 - 1s - loss: 1.1468 - mae: 0.9178 - val_loss: 1.1923 - val_mae: 0.9662 - lr: 1.0000e-09 - 952ms/epoch - 60ms/step\n",
      "Epoch 50/128\n",
      "16/16 - 1s - loss: 1.1944 - mae: 0.9519 - val_loss: 1.2104 - val_mae: 0.9718 - lr: 1.0000e-09 - 939ms/epoch - 59ms/step\n",
      "Epoch 51/128\n",
      "16/16 - 1s - loss: 1.1208 - mae: 0.9118 - val_loss: 1.1794 - val_mae: 0.9501 - lr: 1.0000e-09 - 957ms/epoch - 60ms/step\n",
      "Epoch 52/128\n",
      "16/16 - 1s - loss: 1.2269 - mae: 0.9658 - val_loss: 1.2198 - val_mae: 0.9579 - lr: 1.0000e-09 - 937ms/epoch - 59ms/step\n",
      "Epoch 53/128\n",
      "16/16 - 1s - loss: 1.2219 - mae: 0.9610 - val_loss: 1.1937 - val_mae: 0.9574 - lr: 1.0000e-09 - 957ms/epoch - 60ms/step\n",
      "Epoch 54/128\n",
      "16/16 - 1s - loss: 1.2170 - mae: 0.9556 - val_loss: 1.2238 - val_mae: 0.9631 - lr: 1.0000e-09 - 954ms/epoch - 60ms/step\n",
      "Epoch 55/128\n",
      "16/16 - 1s - loss: 1.1545 - mae: 0.9270 - val_loss: 1.2226 - val_mae: 0.9566 - lr: 1.0000e-09 - 943ms/epoch - 59ms/step\n",
      "Epoch 56/128\n",
      "16/16 - 1s - loss: 1.1947 - mae: 0.9587 - val_loss: 1.2668 - val_mae: 0.9711 - lr: 1.0000e-09 - 950ms/epoch - 59ms/step\n",
      "Epoch 57/128\n",
      "16/16 - 1s - loss: 1.1962 - mae: 0.9393 - val_loss: 1.2422 - val_mae: 0.9664 - lr: 1.0000e-09 - 944ms/epoch - 59ms/step\n",
      "Epoch 58/128\n",
      "16/16 - 1s - loss: 1.2795 - mae: 0.9859 - val_loss: 1.2498 - val_mae: 0.9716 - lr: 1.0000e-09 - 959ms/epoch - 60ms/step\n",
      "Epoch 59/128\n",
      "16/16 - 1s - loss: 1.1923 - mae: 0.9499 - val_loss: 1.2376 - val_mae: 0.9646 - lr: 1.0000e-09 - 966ms/epoch - 60ms/step\n",
      "Epoch 60/128\n",
      "16/16 - 1s - loss: 1.2107 - mae: 0.9417 - val_loss: 1.2664 - val_mae: 0.9726 - lr: 1.0000e-09 - 957ms/epoch - 60ms/step\n",
      "Epoch 61/128\n",
      "16/16 - 1s - loss: 1.2357 - mae: 0.9564 - val_loss: 1.2747 - val_mae: 0.9785 - lr: 1.0000e-09 - 972ms/epoch - 61ms/step\n",
      "Epoch 62/128\n",
      "16/16 - 1s - loss: 1.2082 - mae: 0.9520 - val_loss: 1.3108 - val_mae: 0.9934 - lr: 1.0000e-09 - 943ms/epoch - 59ms/step\n",
      "Epoch 63/128\n",
      "16/16 - 1s - loss: 1.1798 - mae: 0.9388 - val_loss: 1.3068 - val_mae: 0.9897 - lr: 1.0000e-09 - 952ms/epoch - 60ms/step\n",
      "Epoch 64/128\n",
      "16/16 - 1s - loss: 1.2153 - mae: 0.9523 - val_loss: 1.3130 - val_mae: 0.9874 - lr: 1.0000e-09 - 952ms/epoch - 59ms/step\n",
      "Epoch 65/128\n",
      "16/16 - 1s - loss: 1.2006 - mae: 0.9506 - val_loss: 1.3263 - val_mae: 0.9866 - lr: 1.0000e-09 - 947ms/epoch - 59ms/step\n",
      "Epoch 66/128\n",
      "16/16 - 1s - loss: 1.1917 - mae: 0.9481 - val_loss: 1.3141 - val_mae: 0.9886 - lr: 1.0000e-09 - 941ms/epoch - 59ms/step\n",
      "Epoch 67/128\n",
      "16/16 - 1s - loss: 1.1715 - mae: 0.9435 - val_loss: 1.3340 - val_mae: 0.9974 - lr: 1.0000e-09 - 970ms/epoch - 61ms/step\n",
      "Epoch 68/128\n",
      "16/16 - 1s - loss: 1.2337 - mae: 0.9637 - val_loss: 1.3277 - val_mae: 0.9983 - lr: 1.0000e-09 - 950ms/epoch - 59ms/step\n",
      "Epoch 69/128\n",
      "16/16 - 1s - loss: 1.1901 - mae: 0.9432 - val_loss: 1.3383 - val_mae: 1.0051 - lr: 1.0000e-09 - 953ms/epoch - 60ms/step\n",
      "Epoch 70/128\n",
      "16/16 - 1s - loss: 1.1957 - mae: 0.9496 - val_loss: 1.3446 - val_mae: 1.0080 - lr: 1.0000e-09 - 954ms/epoch - 60ms/step\n",
      "Epoch 71/128\n",
      "16/16 - 1s - loss: 1.1767 - mae: 0.9424 - val_loss: 1.3509 - val_mae: 1.0111 - lr: 1.0000e-09 - 964ms/epoch - 60ms/step\n",
      "Epoch 72/128\n",
      "16/16 - 1s - loss: 1.1979 - mae: 0.9452 - val_loss: 1.3345 - val_mae: 1.0066 - lr: 1.0000e-09 - 957ms/epoch - 60ms/step\n",
      "Epoch 73/128\n",
      "16/16 - 1s - loss: 1.2291 - mae: 0.9644 - val_loss: 1.3374 - val_mae: 1.0107 - lr: 1.0000e-09 - 948ms/epoch - 59ms/step\n",
      "Epoch 74/128\n",
      "16/16 - 1s - loss: 1.2062 - mae: 0.9454 - val_loss: 1.3514 - val_mae: 1.0191 - lr: 1.0000e-09 - 958ms/epoch - 60ms/step\n",
      "Epoch 75/128\n",
      "16/16 - 1s - loss: 1.1623 - mae: 0.9289 - val_loss: 1.3618 - val_mae: 1.0233 - lr: 1.0000e-09 - 971ms/epoch - 61ms/step\n",
      "Epoch 76/128\n",
      "16/16 - 1s - loss: 1.1628 - mae: 0.9372 - val_loss: 1.3654 - val_mae: 1.0221 - lr: 1.0000e-09 - 957ms/epoch - 60ms/step\n",
      "Epoch 77/128\n",
      "16/16 - 1s - loss: 1.1658 - mae: 0.9359 - val_loss: 1.3504 - val_mae: 1.0125 - lr: 1.0000e-09 - 948ms/epoch - 59ms/step\n",
      "Epoch 78/128\n",
      "16/16 - 1s - loss: 1.2100 - mae: 0.9474 - val_loss: 1.3553 - val_mae: 1.0147 - lr: 1.0000e-09 - 946ms/epoch - 59ms/step\n",
      "Epoch 79/128\n",
      "16/16 - 1s - loss: 1.1504 - mae: 0.9281 - val_loss: 1.3757 - val_mae: 1.0259 - lr: 1.0000e-09 - 981ms/epoch - 61ms/step\n",
      "Epoch 80/128\n",
      "16/16 - 1s - loss: 1.2087 - mae: 0.9539 - val_loss: 1.3472 - val_mae: 1.0106 - lr: 1.0000e-09 - 961ms/epoch - 60ms/step\n",
      "Epoch 81/128\n",
      "16/16 - 1s - loss: 1.1659 - mae: 0.9414 - val_loss: 1.3626 - val_mae: 1.0184 - lr: 1.0000e-09 - 939ms/epoch - 59ms/step\n",
      "Epoch 82/128\n",
      "16/16 - 1s - loss: 1.1718 - mae: 0.9323 - val_loss: 1.3598 - val_mae: 1.0207 - lr: 1.0000e-09 - 952ms/epoch - 60ms/step\n",
      "Epoch 83/128\n",
      "16/16 - 1s - loss: 1.2200 - mae: 0.9547 - val_loss: 1.3531 - val_mae: 1.0175 - lr: 1.0000e-09 - 955ms/epoch - 60ms/step\n",
      "Epoch 84/128\n",
      "16/16 - 1s - loss: 1.2205 - mae: 0.9539 - val_loss: 1.3591 - val_mae: 1.0239 - lr: 1.0000e-09 - 949ms/epoch - 59ms/step\n",
      "Epoch 85/128\n",
      "16/16 - 1s - loss: 1.1684 - mae: 0.9363 - val_loss: 1.3584 - val_mae: 1.0225 - lr: 1.0000e-09 - 951ms/epoch - 59ms/step\n",
      "Epoch 86/128\n",
      "16/16 - 1s - loss: 1.1982 - mae: 0.9369 - val_loss: 1.3387 - val_mae: 1.0121 - lr: 1.0000e-09 - 960ms/epoch - 60ms/step\n",
      "Epoch 87/128\n",
      "16/16 - 1s - loss: 1.1797 - mae: 0.9383 - val_loss: 1.3424 - val_mae: 1.0152 - lr: 1.0000e-09 - 949ms/epoch - 59ms/step\n",
      "Epoch 88/128\n",
      "16/16 - 1s - loss: 1.1468 - mae: 0.9356 - val_loss: 1.3536 - val_mae: 1.0214 - lr: 1.0000e-09 - 955ms/epoch - 60ms/step\n",
      "Epoch 89/128\n",
      "16/16 - 1s - loss: 1.2315 - mae: 0.9514 - val_loss: 1.3338 - val_mae: 1.0075 - lr: 1.0000e-09 - 952ms/epoch - 60ms/step\n",
      "Epoch 90/128\n",
      "16/16 - 1s - loss: 1.1992 - mae: 0.9565 - val_loss: 1.3468 - val_mae: 1.0118 - lr: 1.0000e-09 - 947ms/epoch - 59ms/step\n",
      "Epoch 91/128\n",
      "16/16 - 1s - loss: 1.1733 - mae: 0.9302 - val_loss: 1.3515 - val_mae: 1.0139 - lr: 1.0000e-09 - 952ms/epoch - 60ms/step\n",
      "Epoch 92/128\n",
      "16/16 - 1s - loss: 1.2326 - mae: 0.9695 - val_loss: 1.3645 - val_mae: 1.0222 - lr: 1.0000e-09 - 961ms/epoch - 60ms/step\n",
      "Epoch 93/128\n",
      "16/16 - 1s - loss: 1.2624 - mae: 0.9823 - val_loss: 1.3331 - val_mae: 1.0073 - lr: 1.0000e-09 - 942ms/epoch - 59ms/step\n",
      "Epoch 94/128\n",
      "16/16 - 1s - loss: 1.1375 - mae: 0.9228 - val_loss: 1.3175 - val_mae: 0.9994 - lr: 1.0000e-09 - 970ms/epoch - 61ms/step\n",
      "Epoch 95/128\n",
      "16/16 - 1s - loss: 1.1943 - mae: 0.9471 - val_loss: 1.3136 - val_mae: 0.9976 - lr: 1.0000e-09 - 963ms/epoch - 60ms/step\n",
      "Epoch 96/128\n",
      "16/16 - 1s - loss: 1.2085 - mae: 0.9480 - val_loss: 1.3187 - val_mae: 0.9966 - lr: 1.0000e-09 - 946ms/epoch - 59ms/step\n",
      "Epoch 97/128\n",
      "16/16 - 1s - loss: 1.2369 - mae: 0.9702 - val_loss: 1.3581 - val_mae: 1.0190 - lr: 1.0000e-09 - 974ms/epoch - 61ms/step\n",
      "Epoch 98/128\n",
      "16/16 - 1s - loss: 1.1978 - mae: 0.9370 - val_loss: 1.3750 - val_mae: 1.0285 - lr: 1.0000e-09 - 950ms/epoch - 59ms/step\n",
      "Epoch 99/128\n",
      "16/16 - 1s - loss: 1.1928 - mae: 0.9556 - val_loss: 1.3740 - val_mae: 1.0271 - lr: 1.0000e-09 - 948ms/epoch - 59ms/step\n",
      "Epoch 100/128\n",
      "16/16 - 1s - loss: 1.2108 - mae: 0.9494 - val_loss: 1.3667 - val_mae: 1.0244 - lr: 1.0000e-09 - 960ms/epoch - 60ms/step\n",
      "Epoch 101/128\n",
      "16/16 - 1s - loss: 1.2155 - mae: 0.9612 - val_loss: 1.3583 - val_mae: 1.0205 - lr: 1.0000e-09 - 941ms/epoch - 59ms/step\n",
      "Epoch 102/128\n",
      "16/16 - 1s - loss: 1.2096 - mae: 0.9544 - val_loss: 1.3566 - val_mae: 1.0209 - lr: 1.0000e-09 - 948ms/epoch - 59ms/step\n",
      "Epoch 103/128\n",
      "16/16 - 1s - loss: 1.2136 - mae: 0.9502 - val_loss: 1.3533 - val_mae: 1.0189 - lr: 1.0000e-09 - 954ms/epoch - 60ms/step\n",
      "Epoch 104/128\n",
      "16/16 - 1s - loss: 1.1995 - mae: 0.9404 - val_loss: 1.3521 - val_mae: 1.0171 - lr: 1.0000e-09 - 939ms/epoch - 59ms/step\n",
      "Epoch 105/128\n",
      "16/16 - 1s - loss: 1.2329 - mae: 0.9647 - val_loss: 1.3597 - val_mae: 1.0166 - lr: 1.0000e-09 - 941ms/epoch - 59ms/step\n",
      "Epoch 106/128\n",
      "16/16 - 1s - loss: 1.2436 - mae: 0.9716 - val_loss: 1.3745 - val_mae: 1.0253 - lr: 1.0000e-09 - 947ms/epoch - 59ms/step\n",
      "Epoch 107/128\n",
      "16/16 - 1s - loss: 1.1520 - mae: 0.9330 - val_loss: 1.3652 - val_mae: 1.0239 - lr: 1.0000e-09 - 946ms/epoch - 59ms/step\n",
      "Epoch 108/128\n",
      "16/16 - 1s - loss: 1.2192 - mae: 0.9615 - val_loss: 1.3655 - val_mae: 1.0238 - lr: 1.0000e-09 - 949ms/epoch - 59ms/step\n",
      "Epoch 109/128\n",
      "16/16 - 1s - loss: 1.1883 - mae: 0.9508 - val_loss: 1.3753 - val_mae: 1.0290 - lr: 1.0000e-09 - 959ms/epoch - 60ms/step\n",
      "Epoch 110/128\n",
      "16/16 - 1s - loss: 1.2027 - mae: 0.9461 - val_loss: 1.3580 - val_mae: 1.0209 - lr: 1.0000e-09 - 947ms/epoch - 59ms/step\n",
      "Epoch 111/128\n",
      "16/16 - 1s - loss: 1.1742 - mae: 0.9413 - val_loss: 1.3680 - val_mae: 1.0226 - lr: 1.0000e-09 - 970ms/epoch - 61ms/step\n",
      "Epoch 112/128\n",
      "16/16 - 1s - loss: 1.1933 - mae: 0.9457 - val_loss: 1.3601 - val_mae: 1.0247 - lr: 1.0000e-09 - 971ms/epoch - 61ms/step\n",
      "Epoch 113/128\n",
      "16/16 - 1s - loss: 1.2398 - mae: 0.9547 - val_loss: 1.3664 - val_mae: 1.0238 - lr: 1.0000e-09 - 955ms/epoch - 60ms/step\n",
      "Epoch 114/128\n",
      "16/16 - 1s - loss: 1.2058 - mae: 0.9560 - val_loss: 1.3712 - val_mae: 1.0269 - lr: 1.0000e-09 - 972ms/epoch - 61ms/step\n",
      "Epoch 115/128\n",
      "16/16 - 1s - loss: 1.2183 - mae: 0.9612 - val_loss: 1.3609 - val_mae: 1.0231 - lr: 1.0000e-09 - 971ms/epoch - 61ms/step\n",
      "Epoch 116/128\n",
      "16/16 - 1s - loss: 1.1691 - mae: 0.9351 - val_loss: 1.3679 - val_mae: 1.0250 - lr: 1.0000e-09 - 954ms/epoch - 60ms/step\n",
      "Epoch 117/128\n",
      "16/16 - 1s - loss: 1.1985 - mae: 0.9501 - val_loss: 1.3726 - val_mae: 1.0260 - lr: 1.0000e-09 - 970ms/epoch - 61ms/step\n",
      "Epoch 118/128\n",
      "16/16 - 1s - loss: 1.1532 - mae: 0.9276 - val_loss: 1.3689 - val_mae: 1.0255 - lr: 1.0000e-09 - 968ms/epoch - 61ms/step\n",
      "Epoch 119/128\n",
      "16/16 - 1s - loss: 1.2245 - mae: 0.9582 - val_loss: 1.3673 - val_mae: 1.0256 - lr: 1.0000e-09 - 949ms/epoch - 59ms/step\n",
      "Epoch 120/128\n",
      "16/16 - 1s - loss: 1.1454 - mae: 0.9263 - val_loss: 1.3681 - val_mae: 1.0246 - lr: 1.0000e-09 - 962ms/epoch - 60ms/step\n",
      "Epoch 121/128\n",
      "16/16 - 1s - loss: 1.1972 - mae: 0.9376 - val_loss: 1.3700 - val_mae: 1.0255 - lr: 1.0000e-09 - 963ms/epoch - 60ms/step\n",
      "Epoch 122/128\n",
      "16/16 - 1s - loss: 1.2176 - mae: 0.9567 - val_loss: 1.3606 - val_mae: 1.0233 - lr: 1.0000e-09 - 944ms/epoch - 59ms/step\n",
      "Epoch 123/128\n",
      "16/16 - 1s - loss: 1.1824 - mae: 0.9396 - val_loss: 1.3557 - val_mae: 1.0169 - lr: 1.0000e-09 - 968ms/epoch - 61ms/step\n",
      "Epoch 124/128\n",
      "16/16 - 1s - loss: 1.2151 - mae: 0.9553 - val_loss: 1.3599 - val_mae: 1.0205 - lr: 1.0000e-09 - 961ms/epoch - 60ms/step\n",
      "Epoch 125/128\n",
      "16/16 - 1s - loss: 1.1930 - mae: 0.9370 - val_loss: 1.3633 - val_mae: 1.0221 - lr: 1.0000e-09 - 949ms/epoch - 59ms/step\n",
      "Epoch 126/128\n",
      "16/16 - 1s - loss: 1.1806 - mae: 0.9428 - val_loss: 1.3599 - val_mae: 1.0192 - lr: 1.0000e-09 - 987ms/epoch - 62ms/step\n",
      "Epoch 127/128\n",
      "16/16 - 1s - loss: 1.1886 - mae: 0.9493 - val_loss: 1.3348 - val_mae: 1.0032 - lr: 1.0000e-09 - 962ms/epoch - 60ms/step\n",
      "Epoch 128/128\n",
      "16/16 - 1s - loss: 1.1486 - mae: 0.9251 - val_loss: 1.3494 - val_mae: 1.0147 - lr: 1.0000e-09 - 957ms/epoch - 60ms/step\n",
      "Training time: 130.0349953174591s\n"
     ]
    }
   ],
   "source": [
    "## DESIGN C\n",
    "import time \n",
    "start = time.time()\n",
    "historyC = modelC.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=2, callbacks = [reduce_lr])\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPS: 2054807061\n"
     ]
    }
   ],
   "source": [
    "## Design A\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculae FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsA = get_flops(modelA, batch_size=1)\n",
    "print(f\"FLOPS: {flopsA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPS: 1339928587\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculae FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsB = get_flops(modelB, batch_size=1)\n",
    "print(f\"FLOPS: {flopsB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPS: 1854608519\n"
     ]
    }
   ],
   "source": [
    "## Design C\n",
    "## ECONOMIC: FLOATING-POINT OPERATIONS PER SECOND\n",
    "# Calculae FLOPS\n",
    "from keras_flops import get_flops\n",
    "flopsC = get_flops(modelC, batch_size=1)\n",
    "print(f\"FLOPS: {flopsC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONALITY: INFERENCE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sample = train_generator[1][0][0].reshape(1,32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 300ms/step\n",
      "Inference time: 570.9851ms\n"
     ]
    }
   ],
   "source": [
    "## Design A\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelA.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 156ms/step\n",
      "Inference time: 304.4195ms\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelB.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "Inference time: 102.9987ms\n"
     ]
    }
   ],
   "source": [
    "## Design AC\n",
    "## FUNCTIONALITY: INFERENCE TIME\n",
    "start = time.time()\n",
    "modelC.predict(inference_sample)\n",
    "stop = time.time()\n",
    "print(f\"Inference time: {(stop - start)*1e3:.4f}ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORMANCE: COEFFICIENT OF DETERMINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare set of x values and y values for performance constraint\n",
    "X_values,y_values = [],[]\n",
    "for i in range(100):\n",
    "    values = next(train_generator)\n",
    "    # for j in range(values[0].shape[0]):\n",
    "    X_values.append(values[0])\n",
    "    y_values.append(values[1])\n",
    "\n",
    "## create X_values generator\n",
    "gen_X_values_1 = (x for x in X_values)\n",
    "gen_X_values_2 = (x for x in X_values)\n",
    "gen_X_values_3 = (x for x in X_values)\n",
    "y_values = [y  for y_set in y_values for y in y_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s 23ms/step\n",
      "0.5706\n"
     ]
    }
   ],
   "source": [
    "## Design A\n",
    "## PERFORMANCE: COEFFICIENT OF DETERMINATION\n",
    "from sklearn.metrics import r2_score\n",
    "predictionsA = modelA.predict(gen_X_values_1).reshape(3200)\n",
    "codA = r2_score(y_values,predictionsA)\n",
    "print(f'{codA:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s 16ms/step\n",
      "0.4405\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## PERFORMANCE: COEFFICIENT OF DETERMINATION\n",
    "from sklearn.metrics import r2_score\n",
    "predictionsB = modelB.predict(gen_X_values_2).reshape(3200)\n",
    "codB = r2_score(y_values,predictionsB)\n",
    "print(f'{codB:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s 18ms/step\n",
      "0.0367\n"
     ]
    }
   ],
   "source": [
    "## Design B\n",
    "## PERFORMANCE: COEFFICIENT OF DETERMINATION\n",
    "from sklearn.metrics import r2_score\n",
    "predictionsC = modelC.predict(gen_X_values_3).reshape(3200)\n",
    "codC = r2_score(y_values,predictionsC)\n",
    "print(f'{codC:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFFICIENCY: STORAGE CONSUMPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weight size in megabytes: 669.7216\n"
     ]
    }
   ],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsA = modelA.get_weights()\n",
    "total_sizeA = 0\n",
    "for weight in weightsA:\n",
    "    total_sizeA += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeA*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weight size in megabytes: 535.5884\n"
     ]
    }
   ],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsB = modelB.get_weights()\n",
    "total_sizeB = 0\n",
    "for weight in weightsB:\n",
    "    total_sizeB += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeB*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.368568125"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sizeB/(8e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weight size in megabytes: 147.002376\n"
     ]
    }
   ],
   "source": [
    "## EFFICIENCY: STORAGE CONSUMPTION\n",
    "weightsC = modelC.get_weights()\n",
    "total_sizeC = 0\n",
    "for weight in weightsC:\n",
    "    total_sizeC += tf.size(weight).numpy()\n",
    "\n",
    "print(f\"Total model weight size in megabytes: {total_sizeC*8e-6:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving final trained and constrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA.save('design_models/designA_v2.h5')\n",
    "modelB.save('design_models/designB_v2.h5')\n",
    "modelC.save('design_models/designC_v2.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
